{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd05ee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2906, 1, 25)\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "r_test_x = []\n",
    "r_test_y = []\n",
    "posit_1 = 1;\n",
    "negat_0 = 0;\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'OARNDCQEGHILKMFPSTWYV'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "i = 0\n",
    "#-------------------------TEST DATASET----------------------------------------\n",
    "#for positive sequence\n",
    "def innertest1():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #rint(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            print(data, i)\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded)\n",
    "    r_test_y.append(posit_1)\n",
    "for seq_record in SeqIO.parse(\"./Datasets/training_data/ecoli_train.fasta\", \"fasta\"):\n",
    "\n",
    "    innertest1()\n",
    "    i += 1\n",
    "    \n",
    "#print(len(r_test_x))\n",
    "\n",
    "#for negative sequence\n",
    "def innertest2():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #print(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded) \n",
    "    r_test_y.append(negat_0)\n",
    "\n",
    "for seq_record in SeqIO.parse(\"./Datasets/training_data/ecoli_train_neg.fasta\", \"fasta\"):\n",
    "    innertest2()\n",
    "# Changing to array (matrix)    \n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "\n",
    "# Balancing test dataset\n",
    "# Testing Data Balancing by undersampling####################################\n",
    "# trộn dữ liệu\n",
    "rus = RandomUnderSampler(random_state=7)\n",
    "x_res3, y_res3 = rus.fit_resample(r_test_x, r_test_y)\n",
    "#Shuffling\n",
    "r_test_x, r_test_y = shuffle(x_res3, y_res3, random_state=7)\n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "#print(r_test_y.shape)\n",
    "r_test_x = np.expand_dims(r_test_x, 1)\n",
    "print(r_test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a040e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1554, -0.0391]], grad_fn=<AddmmBackward>)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DLMal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLMal, self).__init__()\n",
    "        self.embedding = nn.Embedding(25,21)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (15, 3))\n",
    "        self.dropout1 = nn.Dropout(0.6)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(4096, 768)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(768, 256)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.maxpool(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout4(x)\n",
    "        return self.fc3(x)\n",
    "net = DLMal()\n",
    "y = net(torch.from_numpy(np.array([r_test_x[0]])))#convert array numpy to tensor\n",
    "print(y)\n",
    "print(r_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3189f2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] train loss: 0.085 train acc: 0.973\n",
      "[1,     2] train loss: 0.075 train acc: 0.969\n",
      "[1,     3] train loss: 0.093 train acc: 0.965\n",
      "[1,     4] train loss: 0.060 train acc: 0.980\n",
      "[1,     5] train loss: 0.082 train acc: 0.965\n",
      "[1,     6] train loss: 0.067 train acc: 0.980\n",
      "[1,     7] train loss: 0.089 train acc: 0.969\n",
      "[1,     8] train loss: 0.104 train acc: 0.959\n",
      "[1] val loss: 0.006 val acc: 1.000\n",
      "[2,     1] train loss: 0.061 train acc: 0.977\n",
      "[2,     2] train loss: 0.053 train acc: 0.980\n",
      "[2,     3] train loss: 0.135 train acc: 0.969\n",
      "[2,     4] train loss: 0.137 train acc: 0.957\n",
      "[2,     5] train loss: 0.172 train acc: 0.953\n",
      "[2,     6] train loss: 0.090 train acc: 0.965\n",
      "[2,     7] train loss: 0.060 train acc: 0.984\n",
      "[2,     8] train loss: 0.070 train acc: 0.967\n",
      "[2] val loss: 0.014 val acc: 1.000\n",
      "[3,     1] train loss: 0.080 train acc: 0.961\n",
      "[3,     2] train loss: 0.073 train acc: 0.973\n",
      "[3,     3] train loss: 0.108 train acc: 0.969\n",
      "[3,     4] train loss: 0.156 train acc: 0.965\n",
      "[3,     5] train loss: 0.080 train acc: 0.973\n",
      "[3,     6] train loss: 0.049 train acc: 0.980\n",
      "[3,     7] train loss: 0.077 train acc: 0.965\n",
      "[3,     8] train loss: 0.053 train acc: 0.983\n",
      "[3] val loss: 0.023 val acc: 1.000\n",
      "[4,     1] train loss: 0.090 train acc: 0.969\n",
      "[4,     2] train loss: 0.066 train acc: 0.973\n",
      "[4,     3] train loss: 0.093 train acc: 0.965\n",
      "[4,     4] train loss: 0.069 train acc: 0.973\n",
      "[4,     5] train loss: 0.095 train acc: 0.957\n",
      "[4,     6] train loss: 0.062 train acc: 0.973\n",
      "[4,     7] train loss: 0.072 train acc: 0.980\n",
      "[4,     8] train loss: 0.073 train acc: 0.971\n",
      "[4] val loss: 0.023 val acc: 1.000\n",
      "[5,     1] train loss: 0.072 train acc: 0.984\n",
      "[5,     2] train loss: 0.094 train acc: 0.957\n",
      "[5,     3] train loss: 0.082 train acc: 0.973\n",
      "[5,     4] train loss: 0.072 train acc: 0.973\n",
      "[5,     5] train loss: 0.149 train acc: 0.949\n",
      "[5,     6] train loss: 0.119 train acc: 0.949\n",
      "[5,     7] train loss: 0.146 train acc: 0.957\n",
      "[5,     8] train loss: 0.085 train acc: 0.959\n",
      "[5] val loss: 0.021 val acc: 1.000\n",
      "[6,     1] train loss: 0.107 train acc: 0.965\n",
      "[6,     2] train loss: 0.077 train acc: 0.965\n",
      "[6,     3] train loss: 0.088 train acc: 0.969\n",
      "[6,     4] train loss: 0.054 train acc: 0.980\n",
      "[6,     5] train loss: 0.092 train acc: 0.965\n",
      "[6,     6] train loss: 0.058 train acc: 0.984\n",
      "[6,     7] train loss: 0.103 train acc: 0.965\n",
      "[6,     8] train loss: 0.125 train acc: 0.955\n",
      "[6] val loss: 0.032 val acc: 1.000\n",
      "[7,     1] train loss: 0.079 train acc: 0.969\n",
      "[7,     2] train loss: 0.092 train acc: 0.980\n",
      "[7,     3] train loss: 0.099 train acc: 0.961\n",
      "[7,     4] train loss: 0.105 train acc: 0.945\n",
      "[7,     5] train loss: 0.059 train acc: 0.988\n",
      "[7,     6] train loss: 0.096 train acc: 0.965\n",
      "[7,     7] train loss: 0.112 train acc: 0.965\n",
      "[7,     8] train loss: 0.142 train acc: 0.955\n",
      "[7] val loss: 0.031 val acc: 1.000\n",
      "[8,     1] train loss: 0.055 train acc: 0.988\n",
      "[8,     2] train loss: 0.067 train acc: 0.977\n",
      "[8,     3] train loss: 0.077 train acc: 0.973\n",
      "[8,     4] train loss: 0.125 train acc: 0.941\n",
      "[8,     5] train loss: 0.060 train acc: 0.980\n",
      "[8,     6] train loss: 0.094 train acc: 0.961\n",
      "[8,     7] train loss: 0.116 train acc: 0.969\n",
      "[8,     8] train loss: 0.088 train acc: 0.959\n",
      "[8] val loss: 0.025 val acc: 1.000\n",
      "[9,     1] train loss: 0.071 train acc: 0.969\n",
      "[9,     2] train loss: 0.095 train acc: 0.961\n",
      "[9,     3] train loss: 0.109 train acc: 0.957\n",
      "[9,     4] train loss: 0.047 train acc: 0.980\n",
      "[9,     5] train loss: 0.094 train acc: 0.961\n",
      "[9,     6] train loss: 0.075 train acc: 0.980\n",
      "[9,     7] train loss: 0.065 train acc: 0.980\n",
      "[9,     8] train loss: 0.090 train acc: 0.959\n",
      "[9] val loss: 0.021 val acc: 1.000\n",
      "[10,     1] train loss: 0.083 train acc: 0.965\n",
      "[10,     2] train loss: 0.101 train acc: 0.969\n",
      "[10,     3] train loss: 0.049 train acc: 0.984\n",
      "[10,     4] train loss: 0.073 train acc: 0.969\n",
      "[10,     5] train loss: 0.073 train acc: 0.969\n",
      "[10,     6] train loss: 0.052 train acc: 0.977\n",
      "[10,     7] train loss: 0.093 train acc: 0.977\n",
      "[10,     8] train loss: 0.092 train acc: 0.955\n",
      "[10] val loss: 0.023 val acc: 1.000\n",
      "[11,     1] train loss: 0.112 train acc: 0.961\n",
      "[11,     2] train loss: 0.068 train acc: 0.980\n",
      "[11,     3] train loss: 0.082 train acc: 0.965\n",
      "[11,     4] train loss: 0.071 train acc: 0.965\n",
      "[11,     5] train loss: 0.093 train acc: 0.969\n",
      "[11,     6] train loss: 0.051 train acc: 0.969\n",
      "[11,     7] train loss: 0.083 train acc: 0.973\n",
      "[11,     8] train loss: 0.113 train acc: 0.959\n",
      "[11] val loss: 0.021 val acc: 1.000\n",
      "[12,     1] train loss: 0.098 train acc: 0.965\n",
      "[12,     2] train loss: 0.058 train acc: 0.980\n",
      "[12,     3] train loss: 0.071 train acc: 0.977\n",
      "[12,     4] train loss: 0.094 train acc: 0.965\n",
      "[12,     5] train loss: 0.081 train acc: 0.973\n",
      "[12,     6] train loss: 0.068 train acc: 0.980\n",
      "[12,     7] train loss: 0.060 train acc: 0.980\n",
      "[12,     8] train loss: 0.046 train acc: 0.983\n",
      "[12] val loss: 0.023 val acc: 1.000\n",
      "[13,     1] train loss: 0.074 train acc: 0.961\n",
      "[13,     2] train loss: 0.055 train acc: 0.980\n",
      "[13,     3] train loss: 0.083 train acc: 0.969\n",
      "[13,     4] train loss: 0.055 train acc: 0.977\n",
      "[13,     5] train loss: 0.062 train acc: 0.977\n",
      "[13,     6] train loss: 0.059 train acc: 0.977\n",
      "[13,     7] train loss: 0.073 train acc: 0.965\n",
      "[13,     8] train loss: 0.070 train acc: 0.971\n",
      "[13] val loss: 0.018 val acc: 1.000\n",
      "[14,     1] train loss: 0.089 train acc: 0.973\n",
      "[14,     2] train loss: 0.097 train acc: 0.961\n",
      "[14,     3] train loss: 0.045 train acc: 0.980\n",
      "[14,     4] train loss: 0.090 train acc: 0.961\n",
      "[14,     5] train loss: 0.066 train acc: 0.969\n",
      "[14,     6] train loss: 0.047 train acc: 0.980\n",
      "[14,     7] train loss: 0.060 train acc: 0.984\n",
      "[14,     8] train loss: 0.097 train acc: 0.959\n",
      "[14] val loss: 0.019 val acc: 1.000\n",
      "[15,     1] train loss: 0.088 train acc: 0.969\n",
      "[15,     2] train loss: 0.078 train acc: 0.977\n",
      "[15,     3] train loss: 0.091 train acc: 0.965\n",
      "[15,     4] train loss: 0.078 train acc: 0.969\n",
      "[15,     5] train loss: 0.084 train acc: 0.969\n",
      "[15,     6] train loss: 0.111 train acc: 0.957\n",
      "[15,     7] train loss: 0.076 train acc: 0.965\n",
      "[15,     8] train loss: 0.062 train acc: 0.975\n",
      "[15] val loss: 0.018 val acc: 1.000\n",
      "[16,     1] train loss: 0.093 train acc: 0.973\n",
      "[16,     2] train loss: 0.086 train acc: 0.984\n",
      "[16,     3] train loss: 0.103 train acc: 0.961\n",
      "[16,     4] train loss: 0.079 train acc: 0.961\n",
      "[16,     5] train loss: 0.115 train acc: 0.969\n",
      "[16,     6] train loss: 0.083 train acc: 0.980\n",
      "[16,     7] train loss: 0.055 train acc: 0.984\n",
      "[16,     8] train loss: 0.094 train acc: 0.975\n",
      "[16] val loss: 0.019 val acc: 1.000\n",
      "[17,     1] train loss: 0.063 train acc: 0.980\n",
      "[17,     2] train loss: 0.109 train acc: 0.969\n",
      "[17,     3] train loss: 0.089 train acc: 0.969\n",
      "[17,     4] train loss: 0.075 train acc: 0.965\n",
      "[17,     5] train loss: 0.076 train acc: 0.961\n",
      "[17,     6] train loss: 0.101 train acc: 0.961\n",
      "[17,     7] train loss: 0.082 train acc: 0.969\n",
      "[17,     8] train loss: 0.065 train acc: 0.988\n",
      "[17] val loss: 0.031 val acc: 1.000\n",
      "[18,     1] train loss: 0.105 train acc: 0.961\n",
      "[18,     2] train loss: 0.087 train acc: 0.965\n",
      "[18,     3] train loss: 0.112 train acc: 0.957\n",
      "[18,     4] train loss: 0.089 train acc: 0.973\n",
      "[18,     5] train loss: 0.100 train acc: 0.953\n",
      "[18,     6] train loss: 0.080 train acc: 0.977\n",
      "[18,     7] train loss: 0.080 train acc: 0.973\n",
      "[18,     8] train loss: 0.061 train acc: 0.988\n",
      "[18] val loss: 0.027 val acc: 1.000\n",
      "[19,     1] train loss: 0.060 train acc: 0.984\n",
      "[19,     2] train loss: 0.056 train acc: 0.980\n",
      "[19,     3] train loss: 0.079 train acc: 0.969\n",
      "[19,     4] train loss: 0.055 train acc: 0.984\n",
      "[19,     5] train loss: 0.092 train acc: 0.969\n",
      "[19,     6] train loss: 0.099 train acc: 0.965\n",
      "[19,     7] train loss: 0.050 train acc: 0.984\n",
      "[19,     8] train loss: 0.105 train acc: 0.975\n",
      "[19] val loss: 0.016 val acc: 1.000\n",
      "[20,     1] train loss: 0.073 train acc: 0.973\n",
      "[20,     2] train loss: 0.049 train acc: 0.988\n",
      "[20,     3] train loss: 0.116 train acc: 0.969\n",
      "[20,     4] train loss: 0.036 train acc: 0.984\n",
      "[20,     5] train loss: 0.096 train acc: 0.969\n",
      "[20,     6] train loss: 0.107 train acc: 0.965\n",
      "[20,     7] train loss: 0.064 train acc: 0.973\n",
      "[20,     8] train loss: 0.068 train acc: 0.975\n",
      "[20] val loss: 0.015 val acc: 1.000\n",
      "[21,     1] train loss: 0.108 train acc: 0.965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21,     2] train loss: 0.049 train acc: 0.988\n",
      "[21,     3] train loss: 0.083 train acc: 0.977\n",
      "[21,     4] train loss: 0.070 train acc: 0.977\n",
      "[21,     5] train loss: 0.073 train acc: 0.969\n",
      "[21,     6] train loss: 0.111 train acc: 0.973\n",
      "[21,     7] train loss: 0.095 train acc: 0.965\n",
      "[21,     8] train loss: 0.107 train acc: 0.950\n",
      "[21] val loss: 0.022 val acc: 1.000\n",
      "[22,     1] train loss: 0.080 train acc: 0.961\n",
      "[22,     2] train loss: 0.073 train acc: 0.973\n",
      "[22,     3] train loss: 0.056 train acc: 0.984\n",
      "[22,     4] train loss: 0.057 train acc: 0.984\n",
      "[22,     5] train loss: 0.080 train acc: 0.973\n",
      "[22,     6] train loss: 0.080 train acc: 0.973\n",
      "[22,     7] train loss: 0.117 train acc: 0.945\n",
      "[22,     8] train loss: 0.058 train acc: 0.983\n",
      "[22] val loss: 0.032 val acc: 1.000\n",
      "[23,     1] train loss: 0.091 train acc: 0.965\n",
      "[23,     2] train loss: 0.118 train acc: 0.965\n",
      "[23,     3] train loss: 0.065 train acc: 0.984\n",
      "[23,     4] train loss: 0.045 train acc: 0.992\n",
      "[23,     5] train loss: 0.084 train acc: 0.961\n",
      "[23,     6] train loss: 0.053 train acc: 0.988\n",
      "[23,     7] train loss: 0.078 train acc: 0.965\n",
      "[23,     8] train loss: 0.117 train acc: 0.950\n",
      "[23] val loss: 0.026 val acc: 1.000\n",
      "[24,     1] train loss: 0.070 train acc: 0.977\n",
      "[24,     2] train loss: 0.070 train acc: 0.980\n",
      "[24,     3] train loss: 0.081 train acc: 0.977\n",
      "[24,     4] train loss: 0.065 train acc: 0.977\n",
      "[24,     5] train loss: 0.105 train acc: 0.973\n",
      "[24,     6] train loss: 0.088 train acc: 0.969\n",
      "[24,     7] train loss: 0.094 train acc: 0.969\n",
      "[24,     8] train loss: 0.049 train acc: 0.983\n",
      "[24] val loss: 0.023 val acc: 1.000\n",
      "[25,     1] train loss: 0.117 train acc: 0.953\n",
      "[25,     2] train loss: 0.073 train acc: 0.961\n",
      "[25,     3] train loss: 0.103 train acc: 0.957\n",
      "[25,     4] train loss: 0.071 train acc: 0.977\n",
      "[25,     5] train loss: 0.056 train acc: 0.977\n",
      "[25,     6] train loss: 0.079 train acc: 0.977\n",
      "[25,     7] train loss: 0.045 train acc: 0.984\n",
      "[25,     8] train loss: 0.073 train acc: 0.979\n",
      "[25] val loss: 0.018 val acc: 1.000\n",
      "[26,     1] train loss: 0.085 train acc: 0.980\n",
      "[26,     2] train loss: 0.072 train acc: 0.977\n",
      "[26,     3] train loss: 0.063 train acc: 0.977\n",
      "[26,     4] train loss: 0.066 train acc: 0.969\n",
      "[26,     5] train loss: 0.070 train acc: 0.977\n",
      "[26,     6] train loss: 0.079 train acc: 0.965\n",
      "[26,     7] train loss: 0.048 train acc: 0.988\n",
      "[26,     8] train loss: 0.081 train acc: 0.967\n",
      "[26] val loss: 0.021 val acc: 1.000\n",
      "[27,     1] train loss: 0.052 train acc: 0.980\n",
      "[27,     2] train loss: 0.070 train acc: 0.980\n",
      "[27,     3] train loss: 0.069 train acc: 0.973\n",
      "[27,     4] train loss: 0.095 train acc: 0.977\n",
      "[27,     5] train loss: 0.053 train acc: 0.984\n",
      "[27,     6] train loss: 0.092 train acc: 0.957\n",
      "[27,     7] train loss: 0.059 train acc: 0.977\n",
      "[27,     8] train loss: 0.064 train acc: 0.975\n",
      "[27] val loss: 0.019 val acc: 1.000\n",
      "[28,     1] train loss: 0.101 train acc: 0.973\n",
      "[28,     2] train loss: 0.082 train acc: 0.965\n",
      "[28,     3] train loss: 0.089 train acc: 0.965\n",
      "[28,     4] train loss: 0.056 train acc: 0.973\n",
      "[28,     5] train loss: 0.059 train acc: 0.977\n",
      "[28,     6] train loss: 0.080 train acc: 0.969\n",
      "[28,     7] train loss: 0.042 train acc: 0.984\n",
      "[28,     8] train loss: 0.019 train acc: 0.996\n",
      "[28] val loss: 0.012 val acc: 1.000\n",
      "[29,     1] train loss: 0.086 train acc: 0.969\n",
      "[29,     2] train loss: 0.021 train acc: 0.992\n",
      "[29,     3] train loss: 0.063 train acc: 0.977\n",
      "[29,     4] train loss: 0.045 train acc: 0.984\n",
      "[29,     5] train loss: 0.051 train acc: 0.984\n",
      "[29,     6] train loss: 0.056 train acc: 0.980\n",
      "[29,     7] train loss: 0.032 train acc: 0.992\n",
      "[29,     8] train loss: 0.119 train acc: 0.959\n",
      "[29] val loss: 0.010 val acc: 1.000\n",
      "[30,     1] train loss: 0.095 train acc: 0.965\n",
      "[30,     2] train loss: 0.059 train acc: 0.988\n",
      "[30,     3] train loss: 0.065 train acc: 0.973\n",
      "[30,     4] train loss: 0.088 train acc: 0.973\n",
      "[30,     5] train loss: 0.068 train acc: 0.969\n",
      "[30,     6] train loss: 0.051 train acc: 0.980\n",
      "[30,     7] train loss: 0.093 train acc: 0.957\n",
      "[30,     8] train loss: 0.087 train acc: 0.967\n",
      "[30] val loss: 0.017 val acc: 1.000\n",
      "[31,     1] train loss: 0.054 train acc: 0.980\n",
      "[31,     2] train loss: 0.036 train acc: 0.988\n",
      "[31,     3] train loss: 0.098 train acc: 0.977\n",
      "[31,     4] train loss: 0.128 train acc: 0.980\n",
      "[31,     5] train loss: 0.089 train acc: 0.969\n",
      "[31,     6] train loss: 0.043 train acc: 0.992\n",
      "[31,     7] train loss: 0.073 train acc: 0.973\n",
      "[31,     8] train loss: 0.074 train acc: 0.975\n",
      "[31] val loss: 0.022 val acc: 1.000\n",
      "[32,     1] train loss: 0.033 train acc: 0.992\n",
      "[32,     2] train loss: 0.052 train acc: 0.977\n",
      "[32,     3] train loss: 0.048 train acc: 0.984\n",
      "[32,     4] train loss: 0.080 train acc: 0.977\n",
      "[32,     5] train loss: 0.055 train acc: 0.980\n",
      "[32,     6] train loss: 0.067 train acc: 0.965\n",
      "[32,     7] train loss: 0.090 train acc: 0.973\n",
      "[32,     8] train loss: 0.085 train acc: 0.979\n",
      "[32] val loss: 0.016 val acc: 1.000\n",
      "[33,     1] train loss: 0.046 train acc: 0.980\n",
      "[33,     2] train loss: 0.046 train acc: 0.984\n",
      "[33,     3] train loss: 0.064 train acc: 0.977\n",
      "[33,     4] train loss: 0.081 train acc: 0.969\n",
      "[33,     5] train loss: 0.048 train acc: 0.984\n",
      "[33,     6] train loss: 0.088 train acc: 0.965\n",
      "[33,     7] train loss: 0.027 train acc: 0.992\n",
      "[33,     8] train loss: 0.070 train acc: 0.975\n",
      "[33] val loss: 0.015 val acc: 1.000\n",
      "[34,     1] train loss: 0.086 train acc: 0.973\n",
      "[34,     2] train loss: 0.069 train acc: 0.977\n",
      "[34,     3] train loss: 0.126 train acc: 0.965\n",
      "[34,     4] train loss: 0.052 train acc: 0.980\n",
      "[34,     5] train loss: 0.064 train acc: 0.973\n",
      "[34,     6] train loss: 0.058 train acc: 0.984\n",
      "[34,     7] train loss: 0.049 train acc: 0.984\n",
      "[34,     8] train loss: 0.028 train acc: 0.988\n",
      "[34] val loss: 0.016 val acc: 1.000\n",
      "[35,     1] train loss: 0.061 train acc: 0.969\n",
      "[35,     2] train loss: 0.093 train acc: 0.961\n",
      "[35,     3] train loss: 0.070 train acc: 0.969\n",
      "[35,     4] train loss: 0.075 train acc: 0.977\n",
      "[35,     5] train loss: 0.037 train acc: 0.984\n",
      "[35,     6] train loss: 0.075 train acc: 0.969\n",
      "[35,     7] train loss: 0.092 train acc: 0.961\n",
      "[35,     8] train loss: 0.060 train acc: 0.988\n",
      "[35] val loss: 0.013 val acc: 1.000\n",
      "[36,     1] train loss: 0.070 train acc: 0.973\n",
      "[36,     2] train loss: 0.065 train acc: 0.980\n",
      "[36,     3] train loss: 0.055 train acc: 0.980\n",
      "[36,     4] train loss: 0.083 train acc: 0.961\n",
      "[36,     5] train loss: 0.071 train acc: 0.973\n",
      "[36,     6] train loss: 0.083 train acc: 0.957\n",
      "[36,     7] train loss: 0.061 train acc: 0.973\n",
      "[36,     8] train loss: 0.048 train acc: 0.988\n",
      "[36] val loss: 0.014 val acc: 1.000\n",
      "[37,     1] train loss: 0.069 train acc: 0.980\n",
      "[37,     2] train loss: 0.086 train acc: 0.961\n",
      "[37,     3] train loss: 0.064 train acc: 0.977\n",
      "[37,     4] train loss: 0.092 train acc: 0.957\n",
      "[37,     5] train loss: 0.088 train acc: 0.973\n",
      "[37,     6] train loss: 0.073 train acc: 0.973\n",
      "[37,     7] train loss: 0.097 train acc: 0.973\n",
      "[37,     8] train loss: 0.090 train acc: 0.975\n",
      "[37] val loss: 0.019 val acc: 1.000\n",
      "[38,     1] train loss: 0.092 train acc: 0.977\n",
      "[38,     2] train loss: 0.066 train acc: 0.984\n",
      "[38,     3] train loss: 0.082 train acc: 0.965\n",
      "[38,     4] train loss: 0.066 train acc: 0.977\n",
      "[38,     5] train loss: 0.077 train acc: 0.977\n",
      "[38,     6] train loss: 0.085 train acc: 0.953\n",
      "[38,     7] train loss: 0.046 train acc: 0.988\n",
      "[38,     8] train loss: 0.115 train acc: 0.959\n",
      "[38] val loss: 0.021 val acc: 1.000\n",
      "[39,     1] train loss: 0.054 train acc: 0.980\n",
      "[39,     2] train loss: 0.044 train acc: 0.984\n",
      "[39,     3] train loss: 0.068 train acc: 0.977\n",
      "[39,     4] train loss: 0.090 train acc: 0.969\n",
      "[39,     5] train loss: 0.062 train acc: 0.973\n",
      "[39,     6] train loss: 0.051 train acc: 0.984\n",
      "[39,     7] train loss: 0.049 train acc: 0.980\n",
      "[39,     8] train loss: 0.046 train acc: 0.979\n",
      "[39] val loss: 0.017 val acc: 1.000\n",
      "[40,     1] train loss: 0.072 train acc: 0.984\n",
      "[40,     2] train loss: 0.083 train acc: 0.973\n",
      "[40,     3] train loss: 0.060 train acc: 0.984\n",
      "[40,     4] train loss: 0.072 train acc: 0.965\n",
      "[40,     5] train loss: 0.052 train acc: 0.984\n",
      "[40,     6] train loss: 0.059 train acc: 0.980\n",
      "[40,     7] train loss: 0.055 train acc: 0.984\n",
      "[40,     8] train loss: 0.099 train acc: 0.963\n",
      "[40] val loss: 0.012 val acc: 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41,     1] train loss: 0.040 train acc: 0.980\n",
      "[41,     2] train loss: 0.076 train acc: 0.977\n",
      "[41,     3] train loss: 0.075 train acc: 0.980\n",
      "[41,     4] train loss: 0.060 train acc: 0.980\n",
      "[41,     5] train loss: 0.042 train acc: 0.980\n",
      "[41,     6] train loss: 0.127 train acc: 0.965\n",
      "[41,     7] train loss: 0.090 train acc: 0.961\n",
      "[41,     8] train loss: 0.116 train acc: 0.955\n",
      "[41] val loss: 0.016 val acc: 1.000\n",
      "[42,     1] train loss: 0.067 train acc: 0.980\n",
      "[42,     2] train loss: 0.105 train acc: 0.953\n",
      "[42,     3] train loss: 0.113 train acc: 0.969\n",
      "[42,     4] train loss: 0.050 train acc: 0.980\n",
      "[42,     5] train loss: 0.099 train acc: 0.969\n",
      "[42,     6] train loss: 0.047 train acc: 0.984\n",
      "[42,     7] train loss: 0.045 train acc: 0.984\n",
      "[42,     8] train loss: 0.069 train acc: 0.975\n",
      "[42] val loss: 0.021 val acc: 1.000\n",
      "[43,     1] train loss: 0.080 train acc: 0.961\n",
      "[43,     2] train loss: 0.057 train acc: 0.980\n",
      "[43,     3] train loss: 0.076 train acc: 0.977\n",
      "[43,     4] train loss: 0.071 train acc: 0.980\n",
      "[43,     5] train loss: 0.056 train acc: 0.984\n",
      "[43,     6] train loss: 0.055 train acc: 0.984\n",
      "[43,     7] train loss: 0.048 train acc: 0.984\n",
      "[43,     8] train loss: 0.055 train acc: 0.979\n",
      "[43] val loss: 0.017 val acc: 1.000\n",
      "[44,     1] train loss: 0.086 train acc: 0.969\n",
      "[44,     2] train loss: 0.108 train acc: 0.965\n",
      "[44,     3] train loss: 0.090 train acc: 0.965\n",
      "[44,     4] train loss: 0.059 train acc: 0.984\n",
      "[44,     5] train loss: 0.057 train acc: 0.988\n",
      "[44,     6] train loss: 0.052 train acc: 0.988\n",
      "[44,     7] train loss: 0.092 train acc: 0.965\n",
      "[44,     8] train loss: 0.071 train acc: 0.963\n",
      "[44] val loss: 0.018 val acc: 1.000\n",
      "[45,     1] train loss: 0.029 train acc: 0.992\n",
      "[45,     2] train loss: 0.068 train acc: 0.969\n",
      "[45,     3] train loss: 0.077 train acc: 0.980\n",
      "[45,     4] train loss: 0.043 train acc: 0.988\n",
      "[45,     5] train loss: 0.067 train acc: 0.980\n",
      "[45,     6] train loss: 0.086 train acc: 0.969\n",
      "[45,     7] train loss: 0.098 train acc: 0.973\n",
      "[45,     8] train loss: 0.043 train acc: 0.988\n",
      "[45] val loss: 0.019 val acc: 1.000\n",
      "[46,     1] train loss: 0.060 train acc: 0.980\n",
      "[46,     2] train loss: 0.082 train acc: 0.969\n",
      "[46,     3] train loss: 0.063 train acc: 0.965\n",
      "[46,     4] train loss: 0.062 train acc: 0.980\n",
      "[46,     5] train loss: 0.060 train acc: 0.984\n",
      "[46,     6] train loss: 0.025 train acc: 0.996\n",
      "[46,     7] train loss: 0.057 train acc: 0.973\n",
      "[46,     8] train loss: 0.055 train acc: 0.979\n",
      "[46] val loss: 0.017 val acc: 1.000\n",
      "[47,     1] train loss: 0.061 train acc: 0.973\n",
      "[47,     2] train loss: 0.045 train acc: 0.984\n",
      "[47,     3] train loss: 0.105 train acc: 0.965\n",
      "[47,     4] train loss: 0.063 train acc: 0.984\n",
      "[47,     5] train loss: 0.079 train acc: 0.973\n",
      "[47,     6] train loss: 0.058 train acc: 0.977\n",
      "[47,     7] train loss: 0.038 train acc: 0.992\n",
      "[47,     8] train loss: 0.028 train acc: 0.988\n",
      "[47] val loss: 0.013 val acc: 1.000\n",
      "[48,     1] train loss: 0.047 train acc: 0.984\n",
      "[48,     2] train loss: 0.078 train acc: 0.965\n",
      "[48,     3] train loss: 0.089 train acc: 0.973\n",
      "[48,     4] train loss: 0.052 train acc: 0.980\n",
      "[48,     5] train loss: 0.049 train acc: 0.973\n",
      "[48,     6] train loss: 0.067 train acc: 0.984\n",
      "[48,     7] train loss: 0.037 train acc: 0.984\n",
      "[48,     8] train loss: 0.028 train acc: 0.988\n",
      "[48] val loss: 0.010 val acc: 1.000\n",
      "[49,     1] train loss: 0.030 train acc: 0.984\n",
      "[49,     2] train loss: 0.079 train acc: 0.969\n",
      "[49,     3] train loss: 0.055 train acc: 0.984\n",
      "[49,     4] train loss: 0.063 train acc: 0.969\n",
      "[49,     5] train loss: 0.057 train acc: 0.973\n",
      "[49,     6] train loss: 0.048 train acc: 0.977\n",
      "[49,     7] train loss: 0.041 train acc: 0.984\n",
      "[49,     8] train loss: 0.164 train acc: 0.950\n",
      "[49] val loss: 0.014 val acc: 1.000\n",
      "[50,     1] train loss: 0.073 train acc: 0.965\n",
      "[50,     2] train loss: 0.038 train acc: 0.984\n",
      "[50,     3] train loss: 0.038 train acc: 0.984\n",
      "[50,     4] train loss: 0.066 train acc: 0.973\n",
      "[50,     5] train loss: 0.043 train acc: 0.980\n",
      "[50,     6] train loss: 0.075 train acc: 0.984\n",
      "[50,     7] train loss: 0.060 train acc: 0.980\n",
      "[50,     8] train loss: 0.073 train acc: 0.963\n",
      "[50] val loss: 0.016 val acc: 1.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MALDataset(Dataset):\n",
    "    def __init__(self, r_test_x, r_test_y):\n",
    "        self.r_test_x = r_test_x\n",
    "        self.r_test_y = r_test_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.r_test_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.from_numpy(np.array(r_test_x[idx]))\n",
    "        label = [[1,0],[0,1]]\n",
    "        label = torch.from_numpy(np.array(label[r_test_y[idx]], dtype='float32'))\n",
    "        return data, label\n",
    "\n",
    "ratio = 0.7\n",
    "\n",
    "trainset = MALDataset(r_test_x[:int(len(r_test_x)*ratio)], r_test_y[:int(len(r_test_y)*ratio)])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "valset = MALDataset(r_test_x[int(len(r_test_x)*ratio):], r_test_y[int(len(r_test_y)*ratio):])\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "# Define a Loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "#Train the network\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_tot_acc = 0\n",
    "    train_tot_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        outputs = outputs[:,1] > outputs[:,0]\n",
    "        labels = labels[:,1] > labels[:,0]\n",
    "        acc = np.sum(outputs.astype(\"int32\") == labels.astype(\"int32\"))/len(labels)\n",
    "#         print(outputs.astype('int32'), labels.astype(\"int32\"))\n",
    "        train_acc += acc\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        train_tot_loss += loss.item()\n",
    "        train_tot_acc += acc\n",
    "        if i % 1 == 0: \n",
    "            print('[%d, %5d] train loss: %.3f train acc: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1, train_acc/ 1))\n",
    "            running_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "    \n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    for i, data in enumerate(valloader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        outputs = outputs[:,1] > outputs[:,0]\n",
    "        labels = labels[:,1] > labels[:,0]\n",
    "        acc = np.sum(outputs.astype(\"int32\") == labels.astype(\"int32\"))/len(labels)\n",
    "        val_acc += acc\n",
    "    print('[%d] val loss: %.3f val acc: %.3f' %\n",
    "              (epoch + 1, val_loss / len(valloader), val_acc/len(valloader)))\n",
    "    train_accs.append(train_tot_acc/len(trainloader))\n",
    "    train_losses.append(train_tot_loss/len(trainloader))\n",
    "    val_accs.append(val_acc / len(valloader))\n",
    "    val_losses.append(val_loss / len(valloader))\n",
    "\n",
    "PATH = './Path/dlmal_e_net.pth'\n",
    "torch.save(net, PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e040aa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(val_accs)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05ad20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LYTKYEKDDSIRKQRVKAVELFSLM\n",
      "GIGVINFAYYLAKHGKRYSDGSANN\n",
      "HMQIINEINTRFKTLVEKTWPGDEK\n",
      "ILEANLLPPPEPKESWRRIMDELSV\n",
      "TYFGELSRMTQFKDKSARYAENINA\n",
      "KEILGDEADQYVKVPDTLDVWFDSG\n",
      "AIQFAWELLTSEKWFALPKERLWVT\n",
      "AALASVKGWVSAKLQOOOOOOOOOO\n",
      "PIQETITFDDFAKVDLRVALIENAE\n",
      "ERGKVIADYEERKAKIKADAEEAAR\n",
      "RAALGVLRIIVEKNLNLDLQTLTEE\n",
      "IRQIIDEDLASGKHTTVHTRFPPEP\n",
      "KVISVQEMHAQIKOOOOOOOOOOOO\n",
      "OOOOOOOOOMKIKTRFAPSPTGYLH\n",
      "WDAAQSLLATYIKLNVARHQQGQPL\n",
      "CGTAMDSYLIDPKRKLHVCGNNPTC\n",
      "DAREAFLNITVTKDSRTRYSEAGHP\n",
      "MATGSYPWIPPIKGSDTQDCFVYRT\n",
      "EIYKRLIVSEDNKTLLGAVLVGDTS\n",
      "TFIATSPMHIATKLRSTLDEVIERA\n",
      "ISGLYERVPNIDKAIISVHTHDDLG\n",
      "RNGLRPARYVITKDKLITCASEVGI\n",
      "ADVIQIKVAQGAKPGEGGQLPGDKV\n",
      "PAPCDYAITAIQKFLETDIPVFGIC\n",
      "LPEQVRTNADLEKMVDTSDEWIVTR\n",
      "LVDDERWARFNEKLENIERERQRLK\n",
      "AAGDEVTVVRDEKKAREVALYRQGK\n",
      "QLGIVSLREALEKAEEAGVDLVEIS\n",
      "QLGIVSLREALEKAEEAGVDLVEIS Q 127\n",
      "VGEDWISLDMHGKRPKAVNVRTAPH\n",
      "OOOMSLLNVPAGKDLPEDIYVVIEI\n",
      "OOOOOOOOOOOMKPYQRQFIEFALS\n",
      "FGRPQRVAQEMQKEIALILQREIKD\n",
      "AAALGQIEKQFGKGSIMRLGEDRSM\n",
      "HTTIGKVDFDADKLKENLEALLVAL\n",
      "KQYDINEAIALLKELATAKFVESVD\n",
      "VRQHVIYKEAKIKOOOOOOOOOOOO\n",
      "RAKYQRQLARAIKRARYLSLLPYTD\n",
      "AFAVIVKAAEAAKQAOOOOOOOOOO\n",
      "FQVNQLLDILRAKLLKRGIEGSSLD\n",
      "TIFRELEVFVRSKLKEYQYQEVKGP\n",
      "LKAMGEMKNGEAKOOOOOOOOOOOO\n",
      "YLAVKRRIQPGDKMAGRHGNKGVIS\n",
      "OOMKTLGEFIVEKQHEFSHATGELT\n",
      "LEKKLQYVNEALKDEHWICGQRFTI\n",
      "IAELGRQITERYKDSGSDMVLVGLL\n",
      "KILKDLDEDIRGKDVLIVEDIIDSG\n",
      "SNGKSASAKSLFKLQTLGLTQGTVV\n",
      "APSQEEAVIAFGKFKLNLGTREMFR\n",
      "ANNDMQELEARLKEAREAGARHVLI\n",
      "AERGYLADVELSKIGSFEAALLAYV\n",
      "MAVAHFSPVNDLKHLNIMITAGPTR\n",
      "NNALHLFWQDGDKVLPLERKELLGQ\n",
      "QIDLYAVDGDEYKFLCIAKGGGSAN\n",
      "QIDLYAVDGDEYKFLCIAKGGGSAN Q 152\n",
      "VGDLQRSIDFYTKVLGMKLLRTSEN\n",
      "MEEVKQSNRLVIKTROOOOOOOOOO\n",
      "NDRRCLHLQLTEKGHEFLREVLPPQ\n",
      "KTLLPVLDTMLYKFDDNEIITSAID\n",
      "RWVNALVSELNDKEQHGSQWKFDVH\n",
      "FKDKVKGEWDKIKKDMOOOOOOOOO\n",
      "TIAIAINLFNPQKIVIAGEITEADK\n",
      "LPQELRQAIEHIKAHVTAETPKGKH\n",
      "EEVVEIRGGQRRKSERKFFPGYVLV\n",
      "FLGDGEMDEPESKGAITIATREKLD\n",
      "MQSMQFPAELIEKVCGTIOOOOOOO\n",
      "PGDPIIAHVSPGKGLVIHHESCRNI\n",
      "DTLHLEGKELEFKVIKLDQKRNNVV\n",
      "GGSAEEEAAAYIKEHVTKPVVGYIA\n",
      "VTEIAKKLNRSIKTISSQKKSAMMK\n",
      "TVALIAGGHTLGKTHGAGPTSNVGP\n",
      "NADWVIDGEQQPKSLFKMIKNTFET\n",
      "GDPETQPIMLRMKSDLVELCLAACE\n",
      "EAIGVLEQQSDLKGLLLRSNKAAFI\n",
      "VLFDMAREVNRLKAEDMAAANAMAS\n",
      "GGLDSSIISAITKKYAARRVEDQER\n",
      "SGSRDKGLHGKLKAGVCYSMLDTIN\n",
      "EILQWQRERLVAKLEDAQVQLENNR\n",
      "TTWRKLDETTRNKITDAASAAALMT\n",
      "DTALVHIAAAYHKPTLAFYPNSRTP\n",
      "QFTDDLIARNLLKDVTRVVVDVYGS\n",
      "QFTDDLIARNLLKDVTRVVVDVYGS Q 178\n",
      "SQIEIALDQGEVKAGEFAEPICELE\n",
      "EAMAMAKRVSKLKNANRFFVASDVH\n",
      "TEMMRYMHSLERKDLALNQAMIPLG\n",
      "SHLNTGRPYNADKPNKYTSRYFDEA\n",
      "PLGEEEVALARQKLGWHHPPFEIPK\n",
      "TPSHNPPEDGGIKYNPPNGGPADTN\n",
      "IGPDWYGTDVHHKTLGIVGMGRIGM\n",
      "QLETILNYIDIGKKEGADVLTGGRR\n",
      "QLETILNYIDIGKKEGADVLTGGRR Q 186\n",
      "QEIVDSMTIETYKQISENTKIISQK\n",
      "QEIVDSMTIETYKQISENTKIISQK Q 187\n",
      "LAQEEVWIRQGIKARRTRNEGRVRA\n",
      "GYDPIFFVPSEGKTAAELTREEKSA\n",
      "OOOOOOOOOOMQKVVLATGNVGKVR\n",
      "RSACTYVGASRLKELTKRTTFIRVQ\n",
      "KVRFFKSNSETIKOOOOOOOOOOOO\n",
      "MKVITLTGKDGGKMAGTADIEIRVP\n",
      "MAGVVIHAAFVYKLGDWFARDTRNF\n",
      "PHKSPEVFNLIMKRRAIAGSMIGGI\n",
      "GFGARHNASNSLKDIAELVPFAHRY\n",
      "NIFQSDHPVAMMKAVQAVVHHNETA\n",
      "KVALAQAQGQLAKDKATLANARRDL\n",
      "FEIVNNESDPRFKEYWTEYFQIMKR\n",
      "(200, 1, 25)\n",
      "[[[11  7 13 ...  9 10 13]]\n",
      "\n",
      " [[10  4 11 ... 12 16 11]]\n",
      "\n",
      " [[10  1  8 ...  6  6 20]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3 10 14 ...  7 17  1]]\n",
      "\n",
      " [[12 20  1 ...  2  4 11]]\n",
      "\n",
      " [[14  7 10 ... 13 12  2]]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "r_test_x = []\n",
    "r_test_y = []\n",
    "posit_1 = 1;\n",
    "negat_0 = 0; #dinh nghĩa label\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'OARNDCQEGHILKMFPSTWYV'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "i = 0\n",
    "#-------------------------TEST DATASET----------------------------------------\n",
    "#for positive sequence\n",
    "def innertest1():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #rint(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded)\n",
    "    r_test_y.append(posit_1)\n",
    "for seq_record in SeqIO.parse(\"./Datasets/independent_data/ecoli_test.fasta\", \"fasta\"):\n",
    "\n",
    "    innertest1()\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#for negative sequence\n",
    "def innertest2():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    print(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "#             print(data, i)\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    if integer_encoded[0] == 6: print(data, int_to_char[6], len(r_test_x))\n",
    "    r_test_x.append(integer_encoded) \n",
    "    r_test_y.append(negat_0)\n",
    "\n",
    "for seq_record in SeqIO.parse(\"./Datasets/independent_data/ecoli_test_neg.fasta\", \"fasta\"):\n",
    "    innertest2()\n",
    "# Changing to array (matrix)    \n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "\n",
    "# Balancing test dataset\n",
    "# Testing Data Balancing by undersampling####################################\n",
    "# rus = RandomUnderSampler(random_state=7)\n",
    "# x_res3, y_res3 = rus.fit_resample(r_test_x, r_test_y)\n",
    "# #Shuffling\n",
    "# r_test_x, r_test_y = shuffle(x_res3, y_res3, random_state=7)\n",
    "# r_test_x = np.array(r_test_x)\n",
    "# r_test_y = np.array(r_test_y)\n",
    "\n",
    "r_test_x = np.expand_dims(r_test_x, 1)\n",
    "print(r_test_x.shape)\n",
    "print(r_test_x)\n",
    "print(r_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f316a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  7 13  1  6  3  1  1  2 11  8 18 12  1  7 12 20  4  1  2 11  9  9 10\n",
      "  13]]\n",
      "['L', 'E', 'M', 'A', 'Q', 'N', 'A', 'A', 'R', 'L', 'G', 'W', 'K', 'A', 'E', 'K', 'V', 'D', 'A', 'R', 'L', 'H', 'H', 'I', 'M']\n",
      "tensor([[0.6297, 0.3703]], grad_fn=<SoftmaxBackward>)\n",
      "[[14  7 10 20  3  3  7 16  4 15  2 14 12  7 19 18 17  7 19 14  6 10 13 12\n",
      "   2]]\n",
      "['F', 'E', 'I', 'V', 'N', 'N', 'E', 'S', 'D', 'P', 'R', 'F', 'K', 'E', 'Y', 'W', 'T', 'E', 'Y', 'F', 'Q', 'I', 'M', 'K', 'R']\n",
      "tensor([[0.9982, 0.0018]], grad_fn=<SoftmaxBackward>)\n",
      "['L', 'E', 'M', 'A', 'Q', 'N', 'A', 'A', 'R', 'L', 'G', 'W', 'K', 'A', 'E', 'K', 'V', 'D', 'A', 'R', 'L', 'H', 'H', 'I', 'M'] [[0.62969226 0.3703077 ]]\n",
      "['A', 'A', 'A', 'N', 'K', 'R', 'V', 'S', 'N', 'I', 'L', 'A', 'K', 'S', 'D', 'E', 'V', 'L', 'S', 'D', 'R', 'V', 'N', 'A', 'S'] [[0.8890437  0.11095636]]\n",
      "['R', 'R', 'L', 'Q', 'A', 'R', 'L', 'E', 'A', 'L', 'G', 'I', 'K', 'T', 'L', 'L', 'C', 'D', 'P', 'P', 'R', 'A', 'D', 'R', 'G'] [[0.6924589  0.30754113]]\n",
      "['D', 'E', 'F', 'A', 'D', 'G', 'A', 'S', 'Y', 'L', 'Q', 'G', 'K', 'K', 'V', 'V', 'I', 'V', 'G', 'C', 'G', 'A', 'Q', 'G', 'L'] [[0.73328024 0.26671976]]\n",
      "['O', 'O', 'O', 'M', 'T', 'D', 'K', 'T', 'S', 'L', 'S', 'Y', 'K', 'D', 'A', 'G', 'V', 'D', 'I', 'D', 'A', 'G', 'N', 'A', 'L'] [[0.85895544 0.14104456]]\n",
      "['N', 'I', 'T', 'R', 'T', 'F', 'V', 'H', 'P', 'T', 'S', 'N', 'K', 'Q', 'L', 'C', 'F', 'M', 'Q', 'H', 'I', 'I', 'E', 'T', 'L'] [[0.952967   0.04703308]]\n",
      "['F', 'D', 'V', 'Y', 'T', 'P', 'D', 'I', 'L', 'R', 'C', 'R', 'K', 'S', 'G', 'V', 'L', 'T', 'G', 'L', 'P', 'D', 'A', 'Y', 'G'] [[0.6522277  0.34777236]]\n",
      "['T', 'E', 'R', 'I', 'L', 'F', 'Y', 'T', 'G', 'V', 'N', 'H', 'K', 'I', 'G', 'E', 'V', 'H', 'D', 'G', 'A', 'A', 'T', 'M', 'D'] [[0.7137314  0.28626862]]\n",
      "['A', 'A', 'I', 'A', 'E', 'A', 'R', 'E', 'H', 'G', 'D', 'L', 'K', 'E', 'N', 'A', 'E', 'Y', 'H', 'A', 'A', 'R', 'E', 'Q', 'Q'] [[0.7631328  0.23686713]]\n",
      "['S', 'Y', 'W', 'Y', 'P', 'V', 'R', 'Q', 'V', 'V', 'S', 'F', 'K', 'R', 'D', 'V', 'Y', 'R', 'R', 'V', 'M', 'K', 'E', 'F', 'A'] [[0.9728519  0.02714813]]\n",
      "['L', 'D', 'N', 'L', 'H', 'V', 'A', 'M', 'V', 'G', 'D', 'L', 'K', 'Y', 'G', 'R', 'T', 'V', 'H', 'S', 'L', 'T', 'Q', 'A', 'L'] [[0.5443951  0.45560485]]\n",
      "['F', 'A', 'G', 'F', 'V', 'K', 'A', 'A', 'S', 'E', 'F', 'Q', 'K', 'R', 'Q', 'A', 'K', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] [[0.5283657  0.47163436]]\n",
      "['E', 'K', 'G', 'M', 'K', 'L', 'R', 'Q', 'V', 'R', 'T', 'A', 'K', 'D', 'V', 'V', 'I', 'S', 'D', 'A', 'L', 'T', 'F', 'M', 'A'] [[0.58313465 0.41686538]]\n",
      "['F', 'L', 'V', 'P', 'Q', 'G', 'K', 'A', 'V', 'P', 'A', 'T', 'K', 'K', 'N', 'I', 'E', 'F', 'F', 'E', 'A', 'R', 'R', 'A', 'E'] [[0.8795932  0.12040684]]\n",
      "['O', 'O', 'O', 'O', 'O', 'M', 'Q', 'V', 'I', 'L', 'L', 'D', 'K', 'V', 'A', 'N', 'L', 'G', 'S', 'L', 'G', 'D', 'Q', 'V', 'N'] [[0.8065825  0.19341747]]\n",
      "['K', 'I', 'N', 'A', 'L', 'E', 'T', 'V', 'T', 'I', 'A', 'S', 'K', 'A', 'G', 'D', 'E', 'G', 'K', 'L', 'F', 'G', 'S', 'I', 'G'] [[0.65216243 0.3478376 ]]\n",
      "['H', 'L', 'R', 'L', 'V', 'D', 'I', 'V', 'E', 'P', 'T', 'E', 'K', 'T', 'V', 'D', 'A', 'L', 'M', 'R', 'L', 'D', 'L', 'A', 'A'] [[0.71955854 0.28044143]]\n",
      "['G', 'Y', 'G', 'A', 'I', 'L', 'R', 'Y', 'R', 'G', 'R', 'E', 'K', 'T', 'F', 'S', 'A', 'G', 'Y', 'T', 'R', 'T', 'T', 'N', 'N'] [[0.9958494  0.00415059]]\n",
      "['I', 'T', 'V', 'N', 'K', 'N', 'S', 'V', 'P', 'N', 'D', 'P', 'K', 'S', 'P', 'F', 'V', 'T', 'S', 'G', 'I', 'R', 'V', 'G', 'T'] [[0.95074755 0.04925244]]\n",
      "['A', 'N', 'I', 'K', 'G', 'L', 'T', 'F', 'T', 'Y', 'E', 'P', 'K', 'V', 'L', 'R', 'H', 'F', 'T', 'A', 'K', 'L', 'K', 'E', 'V'] [[0.58630437 0.41369566]]\n",
      "['R', 'R', 'I', 'T', 'A', 'R', 'H', 'I', 'R', 'Q', 'L', 'E', 'K', 'D', 'D', 'V', 'K', 'L', 'I', 'E', 'V', 'P', 'V', 'E', 'Y'] [[0.7815676  0.21843243]]\n",
      "['E', 'G', 'R', 'S', 'G', 'I', 'T', 'F', 'S', 'Q', 'E', 'L', 'K', 'D', 'S', 'G', 'M', 'R', 'S', 'H', 'V', 'W', 'G', 'N', 'V'] [[0.93801785 0.06198223]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'K', 'L', 'F', 'Y', 'K', 'P', 'G', 'A', 'C', 'S', 'L', 'A', 'S', 'H', 'I', 'T', 'L'] [[0.5125311  0.48746893]]\n",
      "['H', 'E', 'G', 'H', 'V', 'A', 'A', 'E', 'V', 'I', 'A', 'G', 'K', 'K', 'H', 'Y', 'F', 'D', 'P', 'K', 'V', 'I', 'P', 'S', 'I'] [[0.5628735 0.4371265]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'A', 'E', 'M', 'K', 'N', 'L', 'K', 'I', 'E', 'V', 'V', 'R', 'Y', 'N', 'P', 'E'] [[0.54845256 0.45154744]]\n",
      "['E', 'I', 'T', 'S', 'T', 'D', 'D', 'F', 'Y', 'R', 'L', 'G', 'K', 'E', 'L', 'A', 'L', 'Q', 'S', 'G', 'L', 'A', 'H', 'K', 'G'] [[0.9106241  0.08937591]]\n",
      "['I', 'Q', 'E', 'S', 'H', 'V', 'H', 'D', 'V', 'T', 'I', 'T', 'K', 'E', 'S', 'P', 'N', 'Y', 'R', 'L', 'G', 'S', 'O', 'O', 'O'] [[0.8452572  0.15474285]]\n",
      "['D', 'L', 'N', 'L', 'L', 'Q', 'F', 'L', 'Q', 'K', 'L', 'A', 'K', 'E', 'S', 'G', 'F', 'D', 'G', 'E', 'L', 'A', 'D', 'L', 'T'] [[0.85659504 0.14340495]]\n",
      "['T', 'Q', 'E', 'S', 'L', 'Y', 'L', 'A', 'L', 'R', 'M', 'V', 'K', 'P', 'G', 'I', 'N', 'L', 'R', 'E', 'I', 'G', 'A', 'A', 'I'] [[0.7208525  0.27914748]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'A', 'Y', 'K', 'H', 'I', 'L', 'I', 'A', 'V', 'D', 'L', 'S', 'P', 'E', 'S'] [[0.7282836 0.2717164]]\n",
      "['M', 'Y', 'Q', 'K', 'I', 'K', 'K', 'H', 'P', 'T', 'P', 'R', 'K', 'I', 'Y', 'A', 'D', 'K', 'L', 'E', 'Q', 'E', 'K', 'V', 'A'] [[0.5448226  0.45517743]]\n",
      "['K', 'G', 'I', 'Y', 'K', 'L', 'E', 'T', 'I', 'E', 'G', 'S', 'K', 'G', 'K', 'V', 'Q', 'L', 'L', 'G', 'S', 'G', 'S', 'I', 'L'] [[0.8941571  0.10584294]]\n",
      "['E', 'L', 'T', 'R', 'T', 'L', 'N', 'D', 'A', 'V', 'E', 'V', 'K', 'H', 'A', 'D', 'N', 'T', 'L', 'T', 'F', 'G', 'P', 'R', 'D'] [[0.52192974 0.47807032]]\n",
      "['D', 'S', 'Y', 'I', 'P', 'E', 'P', 'E', 'R', 'A', 'I', 'D', 'K', 'P', 'F', 'L', 'L', 'P', 'I', 'E', 'D', 'V', 'F', 'S', 'I'] [[0.76827425 0.23172578]]\n",
      "['I', 'I', 'P', 'D', 'P', 'F', 'D', 'P', 'S', 'K', 'K', 'R', 'K', 'P', 'T', 'M', 'L', 'V', 'T', 'D', 'L', 'T', 'L', 'R', 'F'] [[0.64934254 0.35065752]]\n",
      "['L', 'S', 'T', 'G', 'G', 'T', 'A', 'R', 'L', 'L', 'A', 'E', 'K', 'G', 'L', 'P', 'V', 'T', 'E', 'V', 'S', 'D', 'Y', 'T', 'G'] [[0.933369   0.06663103]]\n",
      "['Q', 'R', 'Y', 'W', 'L', 'V', 'D', 'P', 'L', 'D', 'G', 'T', 'K', 'E', 'F', 'I', 'K', 'R', 'N', 'G', 'E', 'F', 'T', 'V', 'N'] [[0.7976954  0.20230459]]\n",
      "['L', 'R', 'I', 'L', 'R', 'A', 'C', 'K', 'E', 'L', 'G', 'I', 'K', 'T', 'V', 'A', 'V', 'H', 'S', 'S', 'A', 'D', 'R', 'D', 'L'] [[0.8422097  0.15779035]]\n",
      "['Q', 'L', 'T', 'E', 'E', 'G', 'Y', 'Y', 'S', 'V', 'F', 'G', 'K', 'S', 'G', 'A', 'R', 'I', 'E', 'I', 'P', 'G', 'C', 'S', 'L'] [[0.6798368  0.32016316]]\n",
      "['S', 'Q', 'L', 'L', 'L', 'S', 'T', 'L', 'T', 'P', 'H', 'T', 'K', 'G', 'K', 'V', 'L', 'D', 'V', 'G', 'C', 'G', 'A', 'G', 'V'] [[0.9854913  0.01450879]]\n",
      "['K', 'G', 'N', 'Q', 'V', 'R', 'I', 'G', 'V', 'N', 'A', 'P', 'K', 'E', 'V', 'S', 'V', 'H', 'R', 'E', 'E', 'I', 'Y', 'Q', 'R'] [[0.73098975 0.26901028]]\n",
      "['H', 'M', 'Q', 'I', 'I', 'N', 'E', 'I', 'N', 'T', 'R', 'F', 'K', 'T', 'L', 'V', 'E', 'K', 'T', 'W', 'P', 'G', 'D', 'E', 'K'] [[0.3708584 0.6291416]]\n",
      "['A', 'I', 'Q', 'F', 'A', 'W', 'E', 'L', 'L', 'T', 'S', 'E', 'K', 'W', 'F', 'A', 'L', 'P', 'K', 'E', 'R', 'L', 'W', 'V', 'T'] [[0.39749673 0.6025033 ]]\n",
      "['I', 'R', 'Q', 'I', 'I', 'D', 'E', 'D', 'L', 'A', 'S', 'G', 'K', 'H', 'T', 'T', 'V', 'H', 'T', 'R', 'F', 'P', 'P', 'E', 'P'] [[0.352749   0.64725107]]\n",
      "['C', 'G', 'T', 'A', 'M', 'D', 'S', 'Y', 'L', 'I', 'D', 'P', 'K', 'R', 'K', 'L', 'H', 'V', 'C', 'G', 'N', 'N', 'P', 'T', 'C'] [[0.13834724 0.86165273]]\n",
      "['D', 'A', 'R', 'E', 'A', 'F', 'L', 'N', 'I', 'T', 'V', 'T', 'K', 'D', 'S', 'R', 'T', 'R', 'Y', 'S', 'E', 'A', 'G', 'H', 'P'] [[0.29894066 0.70105934]]\n",
      "['R', 'N', 'G', 'L', 'R', 'P', 'A', 'R', 'Y', 'V', 'I', 'T', 'K', 'D', 'K', 'L', 'I', 'T', 'C', 'A', 'S', 'E', 'V', 'G', 'I'] [[0.37389958 0.6261005 ]]\n",
      "['L', 'P', 'E', 'Q', 'V', 'R', 'T', 'N', 'A', 'D', 'L', 'E', 'K', 'M', 'V', 'D', 'T', 'S', 'D', 'E', 'W', 'I', 'V', 'T', 'R'] [[0.41497675 0.5850232 ]]\n",
      "['L', 'V', 'D', 'D', 'E', 'R', 'W', 'A', 'R', 'F', 'N', 'E', 'K', 'L', 'E', 'N', 'I', 'E', 'R', 'E', 'R', 'Q', 'R', 'L', 'K'] [[0.08985163 0.9101483 ]]\n",
      "['A', 'A', 'G', 'D', 'E', 'V', 'T', 'V', 'V', 'R', 'D', 'E', 'K', 'K', 'A', 'R', 'E', 'V', 'A', 'L', 'Y', 'R', 'Q', 'G', 'K'] [[0.06588472 0.93411523]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'K', 'P', 'Y', 'Q', 'R', 'Q', 'F', 'I', 'E', 'F', 'A', 'L', 'S'] [[0.4792671 0.5207329]]\n",
      "['A', 'A', 'A', 'L', 'G', 'Q', 'I', 'E', 'K', 'Q', 'F', 'G', 'K', 'G', 'S', 'I', 'M', 'R', 'L', 'G', 'E', 'D', 'R', 'S', 'M'] [[0.35431615 0.6456839 ]]\n",
      "['R', 'A', 'K', 'Y', 'Q', 'R', 'Q', 'L', 'A', 'R', 'A', 'I', 'K', 'R', 'A', 'R', 'Y', 'L', 'S', 'L', 'L', 'P', 'Y', 'T', 'D'] [[0.20060042 0.79939955]]\n",
      "['Y', 'L', 'A', 'V', 'K', 'R', 'R', 'I', 'Q', 'P', 'G', 'D', 'K', 'M', 'A', 'G', 'R', 'H', 'G', 'N', 'K', 'G', 'V', 'I', 'S'] [[0.00873549 0.9912646 ]]\n",
      "['S', 'N', 'G', 'K', 'S', 'A', 'S', 'A', 'K', 'S', 'L', 'F', 'K', 'L', 'Q', 'T', 'L', 'G', 'L', 'T', 'Q', 'G', 'T', 'V', 'V'] [[0.28404522 0.71595484]]\n",
      "['V', 'G', 'D', 'L', 'Q', 'R', 'S', 'I', 'D', 'F', 'Y', 'T', 'K', 'V', 'L', 'G', 'M', 'K', 'L', 'L', 'R', 'T', 'S', 'E', 'N'] [[0.13159364 0.86840636]]\n",
      "['N', 'D', 'R', 'R', 'C', 'L', 'H', 'L', 'Q', 'L', 'T', 'E', 'K', 'G', 'H', 'E', 'F', 'L', 'R', 'E', 'V', 'L', 'P', 'P', 'Q'] [[0.45513123 0.54486877]]\n",
      "['R', 'W', 'V', 'N', 'A', 'L', 'V', 'S', 'E', 'L', 'N', 'D', 'K', 'E', 'Q', 'H', 'G', 'S', 'Q', 'W', 'K', 'F', 'D', 'V', 'H'] [[0.26732957 0.7326705 ]]\n",
      "['T', 'I', 'A', 'I', 'A', 'I', 'N', 'L', 'F', 'N', 'P', 'Q', 'K', 'I', 'V', 'I', 'A', 'G', 'E', 'I', 'T', 'E', 'A', 'D', 'K'] [[0.34807467 0.6519253 ]]\n",
      "['F', 'L', 'G', 'D', 'G', 'E', 'M', 'D', 'E', 'P', 'E', 'S', 'K', 'G', 'A', 'I', 'T', 'I', 'A', 'T', 'R', 'E', 'K', 'L', 'D'] [[0.41542736 0.5845726 ]]\n",
      "['T', 'V', 'A', 'L', 'I', 'A', 'G', 'G', 'H', 'T', 'L', 'G', 'K', 'T', 'H', 'G', 'A', 'G', 'P', 'T', 'S', 'N', 'V', 'G', 'P'] [[0.14963317 0.8503668 ]]\n",
      "['N', 'A', 'D', 'W', 'V', 'I', 'D', 'G', 'E', 'Q', 'Q', 'P', 'K', 'S', 'L', 'F', 'K', 'M', 'I', 'K', 'N', 'T', 'F', 'E', 'T'] [[0.34880495 0.651195  ]]\n",
      "['V', 'L', 'F', 'D', 'M', 'A', 'R', 'E', 'V', 'N', 'R', 'L', 'K', 'A', 'E', 'D', 'M', 'A', 'A', 'A', 'N', 'A', 'M', 'A', 'S'] [[0.2190112  0.78098875]]\n",
      "['G', 'G', 'L', 'D', 'S', 'S', 'I', 'I', 'S', 'A', 'I', 'T', 'K', 'K', 'Y', 'A', 'A', 'R', 'R', 'V', 'E', 'D', 'Q', 'E', 'R'] [[0.28454873 0.71545124]]\n",
      "['E', 'I', 'L', 'Q', 'W', 'Q', 'R', 'E', 'R', 'L', 'V', 'A', 'K', 'L', 'E', 'D', 'A', 'Q', 'V', 'Q', 'L', 'E', 'N', 'N', 'R'] [[0.14865927 0.8513408 ]]\n",
      "['T', 'T', 'W', 'R', 'K', 'L', 'D', 'E', 'T', 'T', 'R', 'N', 'K', 'I', 'T', 'D', 'A', 'A', 'S', 'A', 'A', 'A', 'L', 'M', 'T'] [[0.3628353 0.6371648]]\n",
      "['Q', 'F', 'T', 'D', 'D', 'L', 'I', 'A', 'R', 'N', 'L', 'L', 'K', 'D', 'V', 'T', 'R', 'V', 'V', 'V', 'D', 'V', 'Y', 'G', 'S'] [[0.44168025 0.55831975]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'A', 'M', 'A', 'M', 'A', 'K', 'R', 'V', 'S', 'K', 'L', 'K', 'N', 'A', 'N', 'R', 'F', 'F', 'V', 'A', 'S', 'D', 'V', 'H'] [[0.00492229 0.9950777 ]]\n",
      "['P', 'L', 'G', 'E', 'E', 'E', 'V', 'A', 'L', 'A', 'R', 'Q', 'K', 'L', 'G', 'W', 'H', 'H', 'P', 'P', 'F', 'E', 'I', 'P', 'K'] [[0.19810833 0.8018917 ]]\n",
      "['L', 'A', 'Q', 'E', 'E', 'V', 'W', 'I', 'R', 'Q', 'G', 'I', 'K', 'A', 'R', 'R', 'T', 'R', 'N', 'E', 'G', 'R', 'V', 'R', 'A'] [[0.1397476 0.8602524]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'Q', 'K', 'V', 'V', 'L', 'A', 'T', 'G', 'N', 'V', 'G', 'K', 'V', 'R'] [[0.47485703 0.52514297]]\n",
      "['P', 'H', 'K', 'S', 'P', 'E', 'V', 'F', 'N', 'L', 'I', 'M', 'K', 'R', 'R', 'A', 'I', 'A', 'G', 'S', 'M', 'I', 'G', 'G', 'I'] [[0.0348836 0.9651164]]\n",
      "['K', 'V', 'A', 'L', 'A', 'Q', 'A', 'Q', 'G', 'Q', 'L', 'A', 'K', 'D', 'K', 'A', 'T', 'L', 'A', 'N', 'A', 'R', 'R', 'D', 'L'] [[0.16091985 0.83908015]]\n",
      "acc: 63.5\n",
      "sn: 64.83516483516483\n",
      "sp: 62.38532110091744\n",
      "mcc: 0.2711001881729744\n"
     ]
    }
   ],
   "source": [
    "#nạp mangjg đã train\n",
    "#############################################\n",
    "PATH = './Path/dlmal_e_net.pth'\n",
    "net = torch.load(PATH, map_location=\"cpu\")\n",
    "net.eval()\n",
    "################################################3\n",
    "print(r_test_x[0])\n",
    "print([int_to_char[i] for i in r_test_x[0][0]])\n",
    "y = net(torch.from_numpy(np.array([r_test_x[0]])))\n",
    "print(F.softmax(y, dim=1))\n",
    "\n",
    "print(r_test_x[-1])\n",
    "print([int_to_char[i] for i in r_test_x[-1][0]])\n",
    "y = net(torch.from_numpy(np.array([r_test_x[-1]])))\n",
    "print(F.softmax(y, dim=1))\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for i in range(len(r_test_x)):\n",
    "    y = net(torch.from_numpy(np.array([r_test_x[i]])))\n",
    "    y = F.softmax(y, dim=1)\n",
    "    y = y.detach().cpu().numpy()\n",
    "    if (y[0][0] > y[0][1] and r_test_y[i] == 0):\n",
    "        TN += 1\n",
    "    elif (y[0][0] < y[0][1] and r_test_y[i] == 1):\n",
    "        TP += 1\n",
    "    elif r_test_y[i] == 0:\n",
    "        FN += 1\n",
    "        print([int_to_char[i] for i in r_test_x[i][0]], y)\n",
    "    else:\n",
    "        FP +=1\n",
    "        print([int_to_char[i] for i in r_test_x[i][0]], y)\n",
    "       \n",
    "print(\"acc:\", (TP+TN)/(TP+TN+FP+FN)*100)\n",
    "print(\"sn:\", (TP)/(TP+FN)*100)\n",
    "print(\"sp:\", TN/(TN+FP)*100)\n",
    "print(\"mcc:\", (TP*TN-FP*FN)/((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb44b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
