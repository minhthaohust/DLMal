{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be19575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5212, 1, 25)\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "r_test_x = []\n",
    "r_test_y = []\n",
    "posit_1 = 1;\n",
    "negat_0 = 0;\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'OARNDCQEGHILKMFPSTWYV'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "i = 0\n",
    "#-------------------------TEST DATASET----------------------------------------\n",
    "#for positive sequence\n",
    "def innertest1():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #rint(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            print(data, i)\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded)\n",
    "    r_test_y.append(posit_1)\n",
    "for seq_record in SeqIO.parse(\"./Datasets/training_data/mus_train.fasta\", \"fasta\"):\n",
    "\n",
    "    innertest1()\n",
    "    i += 1\n",
    "    \n",
    "#print(len(r_test_x))\n",
    "\n",
    "#for negative sequence\n",
    "def innertest2():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #print(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded) \n",
    "    r_test_y.append(negat_0)\n",
    "\n",
    "for seq_record in SeqIO.parse(\"./Datasets/training_data/mus_train_neg.fasta\", \"fasta\"):\n",
    "    innertest2()\n",
    "# Changing to array (matrix)    \n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "\n",
    "# Balancing test dataset\n",
    "# Testing Data Balancing by undersampling####################################\n",
    "# trộn dữ liệu\n",
    "rus = RandomUnderSampler(random_state=7)\n",
    "x_res3, y_res3 = rus.fit_resample(r_test_x, r_test_y)\n",
    "#Shuffling\n",
    "r_test_x, r_test_y = shuffle(x_res3, y_res3, random_state=7)\n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "#print(r_test_y.shape)\n",
    "r_test_x = np.expand_dims(r_test_x, 1)\n",
    "print(r_test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43552639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0288, -0.1414]], grad_fn=<AddmmBackward>)\n",
      "[0 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DLMal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLMal, self).__init__()\n",
    "        self.embedding = nn.Embedding(25,21)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (15, 3))\n",
    "        self.dropout1 = nn.Dropout(0.6)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(4096, 768)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(768, 256)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.maxpool(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout4(x)\n",
    "        return self.fc3(x)\n",
    "net = DLMal()\n",
    "y = net(torch.from_numpy(np.array([r_test_x[0]])))#convert array numpy to tensor\n",
    "print(y)\n",
    "print(r_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa3ad2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] train loss: 0.454 train acc: 0.770\n",
      "[1,     2] train loss: 0.492 train acc: 0.781\n",
      "[1,     3] train loss: 0.446 train acc: 0.812\n",
      "[1,     4] train loss: 0.463 train acc: 0.781\n",
      "[1,     5] train loss: 0.423 train acc: 0.777\n",
      "[1,     6] train loss: 0.420 train acc: 0.793\n",
      "[1,     7] train loss: 0.444 train acc: 0.793\n",
      "[1,     8] train loss: 0.407 train acc: 0.805\n",
      "[1,     9] train loss: 0.412 train acc: 0.805\n",
      "[1,    10] train loss: 0.488 train acc: 0.746\n",
      "[1,    11] train loss: 0.409 train acc: 0.824\n",
      "[1,    12] train loss: 0.391 train acc: 0.844\n",
      "[1,    13] train loss: 0.447 train acc: 0.777\n",
      "[1,    14] train loss: 0.381 train acc: 0.820\n",
      "[1,    15] train loss: 0.539 train acc: 0.750\n",
      "[1] val loss: 0.355 val acc: 0.956\n",
      "[2,     1] train loss: 0.452 train acc: 0.777\n",
      "[2,     2] train loss: 0.407 train acc: 0.805\n",
      "[2,     3] train loss: 0.427 train acc: 0.809\n",
      "[2,     4] train loss: 0.481 train acc: 0.762\n",
      "[2,     5] train loss: 0.444 train acc: 0.773\n",
      "[2,     6] train loss: 0.482 train acc: 0.785\n",
      "[2,     7] train loss: 0.436 train acc: 0.809\n",
      "[2,     8] train loss: 0.450 train acc: 0.770\n",
      "[2,     9] train loss: 0.437 train acc: 0.785\n",
      "[2,    10] train loss: 0.442 train acc: 0.777\n",
      "[2,    11] train loss: 0.383 train acc: 0.809\n",
      "[2,    12] train loss: 0.384 train acc: 0.816\n",
      "[2,    13] train loss: 0.349 train acc: 0.863\n",
      "[2,    14] train loss: 0.492 train acc: 0.781\n",
      "[2,    15] train loss: 0.456 train acc: 0.719\n",
      "[2] val loss: 0.366 val acc: 0.957\n",
      "[3,     1] train loss: 0.389 train acc: 0.805\n",
      "[3,     2] train loss: 0.429 train acc: 0.773\n",
      "[3,     3] train loss: 0.379 train acc: 0.824\n",
      "[3,     4] train loss: 0.384 train acc: 0.840\n",
      "[3,     5] train loss: 0.405 train acc: 0.816\n",
      "[3,     6] train loss: 0.391 train acc: 0.844\n",
      "[3,     7] train loss: 0.446 train acc: 0.789\n",
      "[3,     8] train loss: 0.392 train acc: 0.820\n",
      "[3,     9] train loss: 0.388 train acc: 0.809\n",
      "[3,    10] train loss: 0.422 train acc: 0.836\n",
      "[3,    11] train loss: 0.439 train acc: 0.781\n",
      "[3,    12] train loss: 0.459 train acc: 0.773\n",
      "[3,    13] train loss: 0.436 train acc: 0.770\n",
      "[3,    14] train loss: 0.430 train acc: 0.793\n",
      "[3,    15] train loss: 0.405 train acc: 0.812\n",
      "[3] val loss: 0.339 val acc: 0.956\n",
      "[4,     1] train loss: 0.489 train acc: 0.781\n",
      "[4,     2] train loss: 0.427 train acc: 0.793\n",
      "[4,     3] train loss: 0.428 train acc: 0.773\n",
      "[4,     4] train loss: 0.414 train acc: 0.801\n",
      "[4,     5] train loss: 0.392 train acc: 0.836\n",
      "[4,     6] train loss: 0.462 train acc: 0.781\n",
      "[4,     7] train loss: 0.368 train acc: 0.852\n",
      "[4,     8] train loss: 0.471 train acc: 0.789\n",
      "[4,     9] train loss: 0.404 train acc: 0.828\n",
      "[4,    10] train loss: 0.414 train acc: 0.801\n",
      "[4,    11] train loss: 0.393 train acc: 0.805\n",
      "[4,    12] train loss: 0.369 train acc: 0.844\n",
      "[4,    13] train loss: 0.430 train acc: 0.789\n",
      "[4,    14] train loss: 0.430 train acc: 0.805\n",
      "[4,    15] train loss: 0.433 train acc: 0.812\n",
      "[4] val loss: 0.350 val acc: 0.959\n",
      "[5,     1] train loss: 0.395 train acc: 0.805\n",
      "[5,     2] train loss: 0.421 train acc: 0.785\n",
      "[5,     3] train loss: 0.464 train acc: 0.730\n",
      "[5,     4] train loss: 0.415 train acc: 0.781\n",
      "[5,     5] train loss: 0.436 train acc: 0.797\n",
      "[5,     6] train loss: 0.485 train acc: 0.770\n",
      "[5,     7] train loss: 0.442 train acc: 0.785\n",
      "[5,     8] train loss: 0.423 train acc: 0.789\n",
      "[5,     9] train loss: 0.381 train acc: 0.848\n",
      "[5,    10] train loss: 0.345 train acc: 0.859\n",
      "[5,    11] train loss: 0.448 train acc: 0.758\n",
      "[5,    12] train loss: 0.445 train acc: 0.797\n",
      "[5,    13] train loss: 0.387 train acc: 0.812\n",
      "[5,    14] train loss: 0.440 train acc: 0.820\n",
      "[5,    15] train loss: 0.315 train acc: 0.859\n",
      "[5] val loss: 0.347 val acc: 0.957\n",
      "[6,     1] train loss: 0.406 train acc: 0.820\n",
      "[6,     2] train loss: 0.451 train acc: 0.801\n",
      "[6,     3] train loss: 0.433 train acc: 0.805\n",
      "[6,     4] train loss: 0.364 train acc: 0.828\n",
      "[6,     5] train loss: 0.410 train acc: 0.789\n",
      "[6,     6] train loss: 0.487 train acc: 0.766\n",
      "[6,     7] train loss: 0.361 train acc: 0.859\n",
      "[6,     8] train loss: 0.373 train acc: 0.832\n",
      "[6,     9] train loss: 0.408 train acc: 0.828\n",
      "[6,    10] train loss: 0.407 train acc: 0.809\n",
      "[6,    11] train loss: 0.396 train acc: 0.809\n",
      "[6,    12] train loss: 0.378 train acc: 0.820\n",
      "[6,    13] train loss: 0.432 train acc: 0.828\n",
      "[6,    14] train loss: 0.410 train acc: 0.809\n",
      "[6,    15] train loss: 0.378 train acc: 0.766\n",
      "[6] val loss: 0.332 val acc: 0.970\n",
      "[7,     1] train loss: 0.421 train acc: 0.812\n",
      "[7,     2] train loss: 0.440 train acc: 0.805\n",
      "[7,     3] train loss: 0.410 train acc: 0.824\n",
      "[7,     4] train loss: 0.473 train acc: 0.770\n",
      "[7,     5] train loss: 0.373 train acc: 0.820\n",
      "[7,     6] train loss: 0.427 train acc: 0.816\n",
      "[7,     7] train loss: 0.387 train acc: 0.797\n",
      "[7,     8] train loss: 0.413 train acc: 0.789\n",
      "[7,     9] train loss: 0.360 train acc: 0.840\n",
      "[7,    10] train loss: 0.377 train acc: 0.828\n",
      "[7,    11] train loss: 0.376 train acc: 0.824\n",
      "[7,    12] train loss: 0.425 train acc: 0.797\n",
      "[7,    13] train loss: 0.366 train acc: 0.820\n",
      "[7,    14] train loss: 0.343 train acc: 0.836\n",
      "[7,    15] train loss: 0.435 train acc: 0.797\n",
      "[7] val loss: 0.315 val acc: 0.977\n",
      "[8,     1] train loss: 0.527 train acc: 0.762\n",
      "[8,     2] train loss: 0.436 train acc: 0.777\n",
      "[8,     3] train loss: 0.411 train acc: 0.809\n",
      "[8,     4] train loss: 0.381 train acc: 0.832\n",
      "[8,     5] train loss: 0.446 train acc: 0.773\n",
      "[8,     6] train loss: 0.374 train acc: 0.852\n",
      "[8,     7] train loss: 0.370 train acc: 0.832\n",
      "[8,     8] train loss: 0.385 train acc: 0.828\n",
      "[8,     9] train loss: 0.413 train acc: 0.805\n",
      "[8,    10] train loss: 0.447 train acc: 0.793\n",
      "[8,    11] train loss: 0.394 train acc: 0.836\n",
      "[8,    12] train loss: 0.409 train acc: 0.801\n",
      "[8,    13] train loss: 0.448 train acc: 0.773\n",
      "[8,    14] train loss: 0.403 train acc: 0.809\n",
      "[8,    15] train loss: 0.422 train acc: 0.812\n",
      "[8] val loss: 0.335 val acc: 0.970\n",
      "[9,     1] train loss: 0.368 train acc: 0.824\n",
      "[9,     2] train loss: 0.404 train acc: 0.824\n",
      "[9,     3] train loss: 0.387 train acc: 0.824\n",
      "[9,     4] train loss: 0.414 train acc: 0.820\n",
      "[9,     5] train loss: 0.418 train acc: 0.816\n",
      "[9,     6] train loss: 0.428 train acc: 0.805\n",
      "[9,     7] train loss: 0.368 train acc: 0.805\n",
      "[9,     8] train loss: 0.425 train acc: 0.824\n",
      "[9,     9] train loss: 0.434 train acc: 0.789\n",
      "[9,    10] train loss: 0.409 train acc: 0.820\n",
      "[9,    11] train loss: 0.405 train acc: 0.812\n",
      "[9,    12] train loss: 0.341 train acc: 0.852\n",
      "[9,    13] train loss: 0.407 train acc: 0.797\n",
      "[9,    14] train loss: 0.393 train acc: 0.852\n",
      "[9,    15] train loss: 0.351 train acc: 0.859\n",
      "[9] val loss: 0.324 val acc: 0.979\n",
      "[10,     1] train loss: 0.437 train acc: 0.797\n",
      "[10,     2] train loss: 0.443 train acc: 0.797\n",
      "[10,     3] train loss: 0.420 train acc: 0.840\n",
      "[10,     4] train loss: 0.332 train acc: 0.875\n",
      "[10,     5] train loss: 0.413 train acc: 0.836\n",
      "[10,     6] train loss: 0.427 train acc: 0.797\n",
      "[10,     7] train loss: 0.391 train acc: 0.836\n",
      "[10,     8] train loss: 0.413 train acc: 0.816\n",
      "[10,     9] train loss: 0.356 train acc: 0.848\n",
      "[10,    10] train loss: 0.415 train acc: 0.801\n",
      "[10,    11] train loss: 0.423 train acc: 0.832\n",
      "[10,    12] train loss: 0.360 train acc: 0.848\n",
      "[10,    13] train loss: 0.424 train acc: 0.801\n",
      "[10,    14] train loss: 0.408 train acc: 0.805\n",
      "[10,    15] train loss: 0.304 train acc: 0.891\n",
      "[10] val loss: 0.314 val acc: 0.981\n",
      "[11,     1] train loss: 0.397 train acc: 0.844\n",
      "[11,     2] train loss: 0.348 train acc: 0.859\n",
      "[11,     3] train loss: 0.379 train acc: 0.805\n",
      "[11,     4] train loss: 0.368 train acc: 0.820\n",
      "[11,     5] train loss: 0.423 train acc: 0.805\n",
      "[11,     6] train loss: 0.353 train acc: 0.859\n",
      "[11,     7] train loss: 0.466 train acc: 0.770\n",
      "[11,     8] train loss: 0.399 train acc: 0.820\n",
      "[11,     9] train loss: 0.409 train acc: 0.824\n",
      "[11,    10] train loss: 0.403 train acc: 0.812\n",
      "[11,    11] train loss: 0.400 train acc: 0.836\n",
      "[11,    12] train loss: 0.391 train acc: 0.812\n",
      "[11,    13] train loss: 0.390 train acc: 0.801\n",
      "[11,    14] train loss: 0.393 train acc: 0.820\n",
      "[11,    15] train loss: 0.390 train acc: 0.797\n",
      "[11] val loss: 0.319 val acc: 0.982\n",
      "[12,     1] train loss: 0.406 train acc: 0.828\n",
      "[12,     2] train loss: 0.386 train acc: 0.812\n",
      "[12,     3] train loss: 0.411 train acc: 0.789\n",
      "[12,     4] train loss: 0.422 train acc: 0.805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12,     5] train loss: 0.362 train acc: 0.840\n",
      "[12,     6] train loss: 0.398 train acc: 0.805\n",
      "[12,     7] train loss: 0.416 train acc: 0.812\n",
      "[12,     8] train loss: 0.391 train acc: 0.805\n",
      "[12,     9] train loss: 0.356 train acc: 0.832\n",
      "[12,    10] train loss: 0.322 train acc: 0.852\n",
      "[12,    11] train loss: 0.394 train acc: 0.816\n",
      "[12,    12] train loss: 0.407 train acc: 0.812\n",
      "[12,    13] train loss: 0.438 train acc: 0.773\n",
      "[12,    14] train loss: 0.406 train acc: 0.793\n",
      "[12,    15] train loss: 0.374 train acc: 0.766\n",
      "[12] val loss: 0.298 val acc: 0.982\n",
      "[13,     1] train loss: 0.410 train acc: 0.801\n",
      "[13,     2] train loss: 0.362 train acc: 0.828\n",
      "[13,     3] train loss: 0.397 train acc: 0.809\n",
      "[13,     4] train loss: 0.386 train acc: 0.805\n",
      "[13,     5] train loss: 0.399 train acc: 0.820\n",
      "[13,     6] train loss: 0.421 train acc: 0.781\n",
      "[13,     7] train loss: 0.399 train acc: 0.809\n",
      "[13,     8] train loss: 0.432 train acc: 0.793\n",
      "[13,     9] train loss: 0.365 train acc: 0.836\n",
      "[13,    10] train loss: 0.372 train acc: 0.836\n",
      "[13,    11] train loss: 0.336 train acc: 0.855\n",
      "[13,    12] train loss: 0.364 train acc: 0.832\n",
      "[13,    13] train loss: 0.389 train acc: 0.812\n",
      "[13,    14] train loss: 0.410 train acc: 0.832\n",
      "[13,    15] train loss: 0.281 train acc: 0.875\n",
      "[13] val loss: 0.295 val acc: 0.980\n",
      "[14,     1] train loss: 0.393 train acc: 0.812\n",
      "[14,     2] train loss: 0.372 train acc: 0.855\n",
      "[14,     3] train loss: 0.303 train acc: 0.883\n",
      "[14,     4] train loss: 0.374 train acc: 0.824\n",
      "[14,     5] train loss: 0.394 train acc: 0.824\n",
      "[14,     6] train loss: 0.414 train acc: 0.816\n",
      "[14,     7] train loss: 0.412 train acc: 0.812\n",
      "[14,     8] train loss: 0.368 train acc: 0.824\n",
      "[14,     9] train loss: 0.383 train acc: 0.832\n",
      "[14,    10] train loss: 0.380 train acc: 0.816\n",
      "[14,    11] train loss: 0.378 train acc: 0.836\n",
      "[14,    12] train loss: 0.382 train acc: 0.828\n",
      "[14,    13] train loss: 0.403 train acc: 0.832\n",
      "[14,    14] train loss: 0.342 train acc: 0.848\n",
      "[14,    15] train loss: 0.427 train acc: 0.828\n",
      "[14] val loss: 0.295 val acc: 0.981\n",
      "[15,     1] train loss: 0.332 train acc: 0.828\n",
      "[15,     2] train loss: 0.423 train acc: 0.793\n",
      "[15,     3] train loss: 0.408 train acc: 0.797\n",
      "[15,     4] train loss: 0.373 train acc: 0.848\n",
      "[15,     5] train loss: 0.342 train acc: 0.840\n",
      "[15,     6] train loss: 0.393 train acc: 0.824\n",
      "[15,     7] train loss: 0.385 train acc: 0.848\n",
      "[15,     8] train loss: 0.384 train acc: 0.824\n",
      "[15,     9] train loss: 0.329 train acc: 0.859\n",
      "[15,    10] train loss: 0.353 train acc: 0.844\n",
      "[15,    11] train loss: 0.333 train acc: 0.844\n",
      "[15,    12] train loss: 0.353 train acc: 0.805\n",
      "[15,    13] train loss: 0.474 train acc: 0.762\n",
      "[15,    14] train loss: 0.317 train acc: 0.855\n",
      "[15,    15] train loss: 0.295 train acc: 0.938\n",
      "[15] val loss: 0.283 val acc: 0.987\n",
      "[16,     1] train loss: 0.347 train acc: 0.836\n",
      "[16,     2] train loss: 0.359 train acc: 0.844\n",
      "[16,     3] train loss: 0.393 train acc: 0.840\n",
      "[16,     4] train loss: 0.372 train acc: 0.844\n",
      "[16,     5] train loss: 0.385 train acc: 0.824\n",
      "[16,     6] train loss: 0.341 train acc: 0.848\n",
      "[16,     7] train loss: 0.429 train acc: 0.781\n",
      "[16,     8] train loss: 0.402 train acc: 0.824\n",
      "[16,     9] train loss: 0.369 train acc: 0.793\n",
      "[16,    10] train loss: 0.406 train acc: 0.812\n",
      "[16,    11] train loss: 0.379 train acc: 0.836\n",
      "[16,    12] train loss: 0.349 train acc: 0.836\n",
      "[16,    13] train loss: 0.296 train acc: 0.879\n",
      "[16,    14] train loss: 0.430 train acc: 0.828\n",
      "[16,    15] train loss: 0.377 train acc: 0.812\n",
      "[16] val loss: 0.287 val acc: 0.983\n",
      "[17,     1] train loss: 0.347 train acc: 0.844\n",
      "[17,     2] train loss: 0.379 train acc: 0.840\n",
      "[17,     3] train loss: 0.342 train acc: 0.863\n",
      "[17,     4] train loss: 0.378 train acc: 0.844\n",
      "[17,     5] train loss: 0.370 train acc: 0.828\n",
      "[17,     6] train loss: 0.356 train acc: 0.863\n",
      "[17,     7] train loss: 0.387 train acc: 0.820\n",
      "[17,     8] train loss: 0.365 train acc: 0.859\n",
      "[17,     9] train loss: 0.313 train acc: 0.863\n",
      "[17,    10] train loss: 0.384 train acc: 0.855\n",
      "[17,    11] train loss: 0.394 train acc: 0.828\n",
      "[17,    12] train loss: 0.347 train acc: 0.844\n",
      "[17,    13] train loss: 0.361 train acc: 0.836\n",
      "[17,    14] train loss: 0.382 train acc: 0.840\n",
      "[17,    15] train loss: 0.351 train acc: 0.844\n",
      "[17] val loss: 0.271 val acc: 0.990\n",
      "[18,     1] train loss: 0.350 train acc: 0.844\n",
      "[18,     2] train loss: 0.368 train acc: 0.828\n",
      "[18,     3] train loss: 0.331 train acc: 0.848\n",
      "[18,     4] train loss: 0.383 train acc: 0.797\n",
      "[18,     5] train loss: 0.376 train acc: 0.832\n",
      "[18,     6] train loss: 0.352 train acc: 0.840\n",
      "[18,     7] train loss: 0.336 train acc: 0.840\n",
      "[18,     8] train loss: 0.368 train acc: 0.828\n",
      "[18,     9] train loss: 0.398 train acc: 0.812\n",
      "[18,    10] train loss: 0.362 train acc: 0.848\n",
      "[18,    11] train loss: 0.389 train acc: 0.828\n",
      "[18,    12] train loss: 0.351 train acc: 0.863\n",
      "[18,    13] train loss: 0.431 train acc: 0.812\n",
      "[18,    14] train loss: 0.316 train acc: 0.867\n",
      "[18,    15] train loss: 0.294 train acc: 0.875\n",
      "[18] val loss: 0.272 val acc: 0.983\n",
      "[19,     1] train loss: 0.373 train acc: 0.824\n",
      "[19,     2] train loss: 0.336 train acc: 0.867\n",
      "[19,     3] train loss: 0.421 train acc: 0.793\n",
      "[19,     4] train loss: 0.287 train acc: 0.906\n",
      "[19,     5] train loss: 0.395 train acc: 0.828\n",
      "[19,     6] train loss: 0.315 train acc: 0.832\n",
      "[19,     7] train loss: 0.379 train acc: 0.828\n",
      "[19,     8] train loss: 0.370 train acc: 0.824\n",
      "[19,     9] train loss: 0.389 train acc: 0.805\n",
      "[19,    10] train loss: 0.368 train acc: 0.832\n",
      "[19,    11] train loss: 0.407 train acc: 0.836\n",
      "[19,    12] train loss: 0.375 train acc: 0.832\n",
      "[19,    13] train loss: 0.320 train acc: 0.840\n",
      "[19,    14] train loss: 0.309 train acc: 0.852\n",
      "[19,    15] train loss: 0.259 train acc: 0.844\n",
      "[19] val loss: 0.260 val acc: 0.989\n",
      "[20,     1] train loss: 0.321 train acc: 0.863\n",
      "[20,     2] train loss: 0.339 train acc: 0.852\n",
      "[20,     3] train loss: 0.340 train acc: 0.852\n",
      "[20,     4] train loss: 0.283 train acc: 0.863\n",
      "[20,     5] train loss: 0.367 train acc: 0.859\n",
      "[20,     6] train loss: 0.326 train acc: 0.852\n",
      "[20,     7] train loss: 0.357 train acc: 0.816\n",
      "[20,     8] train loss: 0.357 train acc: 0.855\n",
      "[20,     9] train loss: 0.376 train acc: 0.840\n",
      "[20,    10] train loss: 0.342 train acc: 0.832\n",
      "[20,    11] train loss: 0.322 train acc: 0.855\n",
      "[20,    12] train loss: 0.339 train acc: 0.852\n",
      "[20,    13] train loss: 0.307 train acc: 0.867\n",
      "[20,    14] train loss: 0.341 train acc: 0.883\n",
      "[20,    15] train loss: 0.511 train acc: 0.750\n",
      "[20] val loss: 0.251 val acc: 0.989\n",
      "[21,     1] train loss: 0.350 train acc: 0.832\n",
      "[21,     2] train loss: 0.316 train acc: 0.879\n",
      "[21,     3] train loss: 0.396 train acc: 0.820\n",
      "[21,     4] train loss: 0.348 train acc: 0.844\n",
      "[21,     5] train loss: 0.390 train acc: 0.832\n",
      "[21,     6] train loss: 0.347 train acc: 0.852\n",
      "[21,     7] train loss: 0.361 train acc: 0.840\n",
      "[21,     8] train loss: 0.321 train acc: 0.852\n",
      "[21,     9] train loss: 0.351 train acc: 0.844\n",
      "[21,    10] train loss: 0.308 train acc: 0.852\n",
      "[21,    11] train loss: 0.353 train acc: 0.844\n",
      "[21,    12] train loss: 0.385 train acc: 0.797\n",
      "[21,    13] train loss: 0.342 train acc: 0.852\n",
      "[21,    14] train loss: 0.340 train acc: 0.855\n",
      "[21,    15] train loss: 0.345 train acc: 0.859\n",
      "[21] val loss: 0.254 val acc: 0.994\n",
      "[22,     1] train loss: 0.275 train acc: 0.898\n",
      "[22,     2] train loss: 0.363 train acc: 0.844\n",
      "[22,     3] train loss: 0.331 train acc: 0.848\n",
      "[22,     4] train loss: 0.345 train acc: 0.863\n",
      "[22,     5] train loss: 0.315 train acc: 0.863\n",
      "[22,     6] train loss: 0.354 train acc: 0.848\n",
      "[22,     7] train loss: 0.346 train acc: 0.809\n",
      "[22,     8] train loss: 0.364 train acc: 0.855\n",
      "[22,     9] train loss: 0.355 train acc: 0.848\n",
      "[22,    10] train loss: 0.358 train acc: 0.836\n",
      "[22,    11] train loss: 0.368 train acc: 0.848\n",
      "[22,    12] train loss: 0.364 train acc: 0.840\n",
      "[22,    13] train loss: 0.359 train acc: 0.832\n",
      "[22,    14] train loss: 0.345 train acc: 0.867\n",
      "[22,    15] train loss: 0.354 train acc: 0.828\n",
      "[22] val loss: 0.238 val acc: 0.993\n",
      "[23,     1] train loss: 0.345 train acc: 0.863\n",
      "[23,     2] train loss: 0.366 train acc: 0.828\n",
      "[23,     3] train loss: 0.375 train acc: 0.824\n",
      "[23,     4] train loss: 0.398 train acc: 0.852\n",
      "[23,     5] train loss: 0.355 train acc: 0.848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23,     6] train loss: 0.288 train acc: 0.863\n",
      "[23,     7] train loss: 0.360 train acc: 0.852\n",
      "[23,     8] train loss: 0.270 train acc: 0.898\n",
      "[23,     9] train loss: 0.379 train acc: 0.848\n",
      "[23,    10] train loss: 0.321 train acc: 0.836\n",
      "[23,    11] train loss: 0.313 train acc: 0.867\n",
      "[23,    12] train loss: 0.318 train acc: 0.871\n",
      "[23,    13] train loss: 0.331 train acc: 0.848\n",
      "[23,    14] train loss: 0.326 train acc: 0.855\n",
      "[23,    15] train loss: 0.365 train acc: 0.859\n",
      "[23] val loss: 0.246 val acc: 0.992\n",
      "[24,     1] train loss: 0.340 train acc: 0.855\n",
      "[24,     2] train loss: 0.366 train acc: 0.848\n",
      "[24,     3] train loss: 0.336 train acc: 0.836\n",
      "[24,     4] train loss: 0.364 train acc: 0.848\n",
      "[24,     5] train loss: 0.332 train acc: 0.859\n",
      "[24,     6] train loss: 0.288 train acc: 0.879\n",
      "[24,     7] train loss: 0.351 train acc: 0.836\n",
      "[24,     8] train loss: 0.376 train acc: 0.828\n",
      "[24,     9] train loss: 0.359 train acc: 0.852\n",
      "[24,    10] train loss: 0.373 train acc: 0.848\n",
      "[24,    11] train loss: 0.347 train acc: 0.836\n",
      "[24,    12] train loss: 0.357 train acc: 0.859\n",
      "[24,    13] train loss: 0.333 train acc: 0.848\n",
      "[24,    14] train loss: 0.353 train acc: 0.867\n",
      "[24,    15] train loss: 0.355 train acc: 0.891\n",
      "[24] val loss: 0.250 val acc: 0.992\n",
      "[25,     1] train loss: 0.293 train acc: 0.871\n",
      "[25,     2] train loss: 0.415 train acc: 0.812\n",
      "[25,     3] train loss: 0.315 train acc: 0.871\n",
      "[25,     4] train loss: 0.387 train acc: 0.828\n",
      "[25,     5] train loss: 0.355 train acc: 0.832\n",
      "[25,     6] train loss: 0.290 train acc: 0.871\n",
      "[25,     7] train loss: 0.313 train acc: 0.875\n",
      "[25,     8] train loss: 0.375 train acc: 0.816\n",
      "[25,     9] train loss: 0.348 train acc: 0.836\n",
      "[25,    10] train loss: 0.357 train acc: 0.836\n",
      "[25,    11] train loss: 0.346 train acc: 0.859\n",
      "[25,    12] train loss: 0.350 train acc: 0.824\n",
      "[25,    13] train loss: 0.367 train acc: 0.797\n",
      "[25,    14] train loss: 0.345 train acc: 0.852\n",
      "[25,    15] train loss: 0.374 train acc: 0.812\n",
      "[25] val loss: 0.263 val acc: 0.993\n",
      "[26,     1] train loss: 0.343 train acc: 0.859\n",
      "[26,     2] train loss: 0.324 train acc: 0.859\n",
      "[26,     3] train loss: 0.298 train acc: 0.875\n",
      "[26,     4] train loss: 0.352 train acc: 0.836\n",
      "[26,     5] train loss: 0.343 train acc: 0.855\n",
      "[26,     6] train loss: 0.292 train acc: 0.867\n",
      "[26,     7] train loss: 0.337 train acc: 0.844\n",
      "[26,     8] train loss: 0.405 train acc: 0.832\n",
      "[26,     9] train loss: 0.364 train acc: 0.844\n",
      "[26,    10] train loss: 0.327 train acc: 0.883\n",
      "[26,    11] train loss: 0.271 train acc: 0.898\n",
      "[26,    12] train loss: 0.445 train acc: 0.828\n",
      "[26,    13] train loss: 0.358 train acc: 0.832\n",
      "[26,    14] train loss: 0.408 train acc: 0.805\n",
      "[26,    15] train loss: 0.339 train acc: 0.844\n",
      "[26] val loss: 0.251 val acc: 0.995\n",
      "[27,     1] train loss: 0.286 train acc: 0.887\n",
      "[27,     2] train loss: 0.338 train acc: 0.832\n",
      "[27,     3] train loss: 0.299 train acc: 0.852\n",
      "[27,     4] train loss: 0.348 train acc: 0.832\n",
      "[27,     5] train loss: 0.414 train acc: 0.824\n",
      "[27,     6] train loss: 0.373 train acc: 0.867\n",
      "[27,     7] train loss: 0.299 train acc: 0.879\n",
      "[27,     8] train loss: 0.358 train acc: 0.867\n",
      "[27,     9] train loss: 0.309 train acc: 0.875\n",
      "[27,    10] train loss: 0.325 train acc: 0.832\n",
      "[27,    11] train loss: 0.369 train acc: 0.828\n",
      "[27,    12] train loss: 0.287 train acc: 0.848\n",
      "[27,    13] train loss: 0.338 train acc: 0.879\n",
      "[27,    14] train loss: 0.374 train acc: 0.824\n",
      "[27,    15] train loss: 0.408 train acc: 0.828\n",
      "[27] val loss: 0.246 val acc: 0.991\n",
      "[28,     1] train loss: 0.289 train acc: 0.875\n",
      "[28,     2] train loss: 0.318 train acc: 0.855\n",
      "[28,     3] train loss: 0.261 train acc: 0.898\n",
      "[28,     4] train loss: 0.337 train acc: 0.852\n",
      "[28,     5] train loss: 0.332 train acc: 0.840\n",
      "[28,     6] train loss: 0.338 train acc: 0.848\n",
      "[28,     7] train loss: 0.317 train acc: 0.871\n",
      "[28,     8] train loss: 0.310 train acc: 0.867\n",
      "[28,     9] train loss: 0.399 train acc: 0.820\n",
      "[28,    10] train loss: 0.328 train acc: 0.828\n",
      "[28,    11] train loss: 0.312 train acc: 0.859\n",
      "[28,    12] train loss: 0.201 train acc: 0.926\n",
      "[28,    13] train loss: 0.310 train acc: 0.867\n",
      "[28,    14] train loss: 0.359 train acc: 0.848\n",
      "[28,    15] train loss: 0.355 train acc: 0.828\n",
      "[28] val loss: 0.212 val acc: 0.993\n",
      "[29,     1] train loss: 0.312 train acc: 0.859\n",
      "[29,     2] train loss: 0.360 train acc: 0.824\n",
      "[29,     3] train loss: 0.329 train acc: 0.859\n",
      "[29,     4] train loss: 0.422 train acc: 0.793\n",
      "[29,     5] train loss: 0.328 train acc: 0.852\n",
      "[29,     6] train loss: 0.348 train acc: 0.871\n",
      "[29,     7] train loss: 0.340 train acc: 0.867\n",
      "[29,     8] train loss: 0.358 train acc: 0.824\n",
      "[29,     9] train loss: 0.330 train acc: 0.855\n",
      "[29,    10] train loss: 0.319 train acc: 0.852\n",
      "[29,    11] train loss: 0.302 train acc: 0.879\n",
      "[29,    12] train loss: 0.278 train acc: 0.875\n",
      "[29,    13] train loss: 0.343 train acc: 0.879\n",
      "[29,    14] train loss: 0.362 train acc: 0.863\n",
      "[29,    15] train loss: 0.294 train acc: 0.859\n",
      "[29] val loss: 0.237 val acc: 0.996\n",
      "[30,     1] train loss: 0.280 train acc: 0.871\n",
      "[30,     2] train loss: 0.369 train acc: 0.816\n",
      "[30,     3] train loss: 0.346 train acc: 0.848\n",
      "[30,     4] train loss: 0.346 train acc: 0.836\n",
      "[30,     5] train loss: 0.269 train acc: 0.875\n",
      "[30,     6] train loss: 0.320 train acc: 0.875\n",
      "[30,     7] train loss: 0.297 train acc: 0.875\n",
      "[30,     8] train loss: 0.339 train acc: 0.855\n",
      "[30,     9] train loss: 0.351 train acc: 0.848\n",
      "[30,    10] train loss: 0.375 train acc: 0.820\n",
      "[30,    11] train loss: 0.329 train acc: 0.855\n",
      "[30,    12] train loss: 0.320 train acc: 0.859\n",
      "[30,    13] train loss: 0.312 train acc: 0.852\n",
      "[30,    14] train loss: 0.405 train acc: 0.812\n",
      "[30,    15] train loss: 0.449 train acc: 0.781\n",
      "[30] val loss: 0.230 val acc: 0.995\n",
      "[31,     1] train loss: 0.280 train acc: 0.887\n",
      "[31,     2] train loss: 0.358 train acc: 0.836\n",
      "[31,     3] train loss: 0.287 train acc: 0.887\n",
      "[31,     4] train loss: 0.328 train acc: 0.844\n",
      "[31,     5] train loss: 0.305 train acc: 0.875\n",
      "[31,     6] train loss: 0.377 train acc: 0.828\n",
      "[31,     7] train loss: 0.269 train acc: 0.887\n",
      "[31,     8] train loss: 0.288 train acc: 0.887\n",
      "[31,     9] train loss: 0.315 train acc: 0.871\n",
      "[31,    10] train loss: 0.289 train acc: 0.863\n",
      "[31,    11] train loss: 0.295 train acc: 0.895\n",
      "[31,    12] train loss: 0.320 train acc: 0.859\n",
      "[31,    13] train loss: 0.385 train acc: 0.832\n",
      "[31,    14] train loss: 0.337 train acc: 0.863\n",
      "[31,    15] train loss: 0.161 train acc: 0.938\n",
      "[31] val loss: 0.207 val acc: 0.997\n",
      "[32,     1] train loss: 0.264 train acc: 0.891\n",
      "[32,     2] train loss: 0.292 train acc: 0.898\n",
      "[32,     3] train loss: 0.338 train acc: 0.867\n",
      "[32,     4] train loss: 0.304 train acc: 0.852\n",
      "[32,     5] train loss: 0.330 train acc: 0.852\n",
      "[32,     6] train loss: 0.321 train acc: 0.855\n",
      "[32,     7] train loss: 0.294 train acc: 0.871\n",
      "[32,     8] train loss: 0.362 train acc: 0.844\n",
      "[32,     9] train loss: 0.355 train acc: 0.840\n",
      "[32,    10] train loss: 0.328 train acc: 0.828\n",
      "[32,    11] train loss: 0.342 train acc: 0.852\n",
      "[32,    12] train loss: 0.293 train acc: 0.863\n",
      "[32,    13] train loss: 0.283 train acc: 0.895\n",
      "[32,    14] train loss: 0.258 train acc: 0.898\n",
      "[32,    15] train loss: 0.287 train acc: 0.891\n",
      "[32] val loss: 0.211 val acc: 0.996\n",
      "[33,     1] train loss: 0.320 train acc: 0.867\n",
      "[33,     2] train loss: 0.343 train acc: 0.844\n",
      "[33,     3] train loss: 0.309 train acc: 0.867\n",
      "[33,     4] train loss: 0.251 train acc: 0.898\n",
      "[33,     5] train loss: 0.271 train acc: 0.887\n",
      "[33,     6] train loss: 0.250 train acc: 0.891\n",
      "[33,     7] train loss: 0.343 train acc: 0.871\n",
      "[33,     8] train loss: 0.264 train acc: 0.879\n",
      "[33,     9] train loss: 0.323 train acc: 0.883\n",
      "[33,    10] train loss: 0.277 train acc: 0.871\n",
      "[33,    11] train loss: 0.332 train acc: 0.852\n",
      "[33,    12] train loss: 0.297 train acc: 0.871\n",
      "[33,    13] train loss: 0.296 train acc: 0.852\n",
      "[33,    14] train loss: 0.315 train acc: 0.867\n",
      "[33,    15] train loss: 0.292 train acc: 0.844\n",
      "[33] val loss: 0.184 val acc: 0.993\n",
      "[34,     1] train loss: 0.284 train acc: 0.875\n",
      "[34,     2] train loss: 0.326 train acc: 0.863\n",
      "[34,     3] train loss: 0.219 train acc: 0.902\n",
      "[34,     4] train loss: 0.343 train acc: 0.867\n",
      "[34,     5] train loss: 0.235 train acc: 0.910\n",
      "[34,     6] train loss: 0.314 train acc: 0.863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34,     7] train loss: 0.290 train acc: 0.895\n",
      "[34,     8] train loss: 0.303 train acc: 0.859\n",
      "[34,     9] train loss: 0.334 train acc: 0.828\n",
      "[34,    10] train loss: 0.401 train acc: 0.832\n",
      "[34,    11] train loss: 0.353 train acc: 0.855\n",
      "[34,    12] train loss: 0.307 train acc: 0.840\n",
      "[34,    13] train loss: 0.323 train acc: 0.863\n",
      "[34,    14] train loss: 0.276 train acc: 0.859\n",
      "[34,    15] train loss: 0.304 train acc: 0.859\n",
      "[34] val loss: 0.228 val acc: 0.992\n",
      "[35,     1] train loss: 0.305 train acc: 0.867\n",
      "[35,     2] train loss: 0.291 train acc: 0.867\n",
      "[35,     3] train loss: 0.373 train acc: 0.836\n",
      "[35,     4] train loss: 0.342 train acc: 0.863\n",
      "[35,     5] train loss: 0.335 train acc: 0.875\n",
      "[35,     6] train loss: 0.264 train acc: 0.891\n",
      "[35,     7] train loss: 0.297 train acc: 0.875\n",
      "[35,     8] train loss: 0.416 train acc: 0.871\n",
      "[35,     9] train loss: 0.249 train acc: 0.906\n",
      "[35,    10] train loss: 0.338 train acc: 0.844\n",
      "[35,    11] train loss: 0.280 train acc: 0.875\n",
      "[35,    12] train loss: 0.282 train acc: 0.883\n",
      "[35,    13] train loss: 0.292 train acc: 0.867\n",
      "[35,    14] train loss: 0.290 train acc: 0.883\n",
      "[35,    15] train loss: 0.207 train acc: 0.953\n",
      "[35] val loss: 0.203 val acc: 0.999\n",
      "[36,     1] train loss: 0.317 train acc: 0.852\n",
      "[36,     2] train loss: 0.312 train acc: 0.867\n",
      "[36,     3] train loss: 0.283 train acc: 0.887\n",
      "[36,     4] train loss: 0.279 train acc: 0.879\n",
      "[36,     5] train loss: 0.243 train acc: 0.883\n",
      "[36,     6] train loss: 0.323 train acc: 0.879\n",
      "[36,     7] train loss: 0.294 train acc: 0.871\n",
      "[36,     8] train loss: 0.313 train acc: 0.879\n",
      "[36,     9] train loss: 0.293 train acc: 0.875\n",
      "[36,    10] train loss: 0.325 train acc: 0.852\n",
      "[36,    11] train loss: 0.324 train acc: 0.867\n",
      "[36,    12] train loss: 0.279 train acc: 0.910\n",
      "[36,    13] train loss: 0.306 train acc: 0.852\n",
      "[36,    14] train loss: 0.340 train acc: 0.852\n",
      "[36,    15] train loss: 0.352 train acc: 0.844\n",
      "[36] val loss: 0.194 val acc: 0.998\n",
      "[37,     1] train loss: 0.306 train acc: 0.875\n",
      "[37,     2] train loss: 0.293 train acc: 0.883\n",
      "[37,     3] train loss: 0.329 train acc: 0.867\n",
      "[37,     4] train loss: 0.293 train acc: 0.859\n",
      "[37,     5] train loss: 0.252 train acc: 0.902\n",
      "[37,     6] train loss: 0.305 train acc: 0.855\n",
      "[37,     7] train loss: 0.366 train acc: 0.828\n",
      "[37,     8] train loss: 0.225 train acc: 0.895\n",
      "[37,     9] train loss: 0.257 train acc: 0.891\n",
      "[37,    10] train loss: 0.293 train acc: 0.879\n",
      "[37,    11] train loss: 0.304 train acc: 0.867\n",
      "[37,    12] train loss: 0.307 train acc: 0.887\n",
      "[37,    13] train loss: 0.337 train acc: 0.875\n",
      "[37,    14] train loss: 0.333 train acc: 0.871\n",
      "[37,    15] train loss: 0.302 train acc: 0.875\n",
      "[37] val loss: 0.190 val acc: 0.999\n",
      "[38,     1] train loss: 0.302 train acc: 0.855\n",
      "[38,     2] train loss: 0.256 train acc: 0.891\n",
      "[38,     3] train loss: 0.304 train acc: 0.871\n",
      "[38,     4] train loss: 0.329 train acc: 0.859\n",
      "[38,     5] train loss: 0.282 train acc: 0.891\n",
      "[38,     6] train loss: 0.300 train acc: 0.859\n",
      "[38,     7] train loss: 0.259 train acc: 0.898\n",
      "[38,     8] train loss: 0.260 train acc: 0.895\n",
      "[38,     9] train loss: 0.327 train acc: 0.859\n",
      "[38,    10] train loss: 0.313 train acc: 0.879\n",
      "[38,    11] train loss: 0.283 train acc: 0.871\n",
      "[38,    12] train loss: 0.341 train acc: 0.855\n",
      "[38,    13] train loss: 0.300 train acc: 0.891\n",
      "[38,    14] train loss: 0.204 train acc: 0.926\n",
      "[38,    15] train loss: 0.267 train acc: 0.891\n",
      "[38] val loss: 0.191 val acc: 0.996\n",
      "[39,     1] train loss: 0.281 train acc: 0.875\n",
      "[39,     2] train loss: 0.299 train acc: 0.879\n",
      "[39,     3] train loss: 0.302 train acc: 0.859\n",
      "[39,     4] train loss: 0.272 train acc: 0.875\n",
      "[39,     5] train loss: 0.363 train acc: 0.824\n",
      "[39,     6] train loss: 0.332 train acc: 0.863\n",
      "[39,     7] train loss: 0.275 train acc: 0.883\n",
      "[39,     8] train loss: 0.331 train acc: 0.871\n",
      "[39,     9] train loss: 0.274 train acc: 0.844\n",
      "[39,    10] train loss: 0.306 train acc: 0.871\n",
      "[39,    11] train loss: 0.285 train acc: 0.887\n",
      "[39,    12] train loss: 0.248 train acc: 0.906\n",
      "[39,    13] train loss: 0.217 train acc: 0.910\n",
      "[39,    14] train loss: 0.318 train acc: 0.855\n",
      "[39,    15] train loss: 0.306 train acc: 0.844\n",
      "[39] val loss: 0.171 val acc: 0.999\n",
      "[40,     1] train loss: 0.387 train acc: 0.844\n",
      "[40,     2] train loss: 0.262 train acc: 0.895\n",
      "[40,     3] train loss: 0.270 train acc: 0.895\n",
      "[40,     4] train loss: 0.339 train acc: 0.844\n",
      "[40,     5] train loss: 0.326 train acc: 0.852\n",
      "[40,     6] train loss: 0.302 train acc: 0.844\n",
      "[40,     7] train loss: 0.294 train acc: 0.863\n",
      "[40,     8] train loss: 0.319 train acc: 0.871\n",
      "[40,     9] train loss: 0.250 train acc: 0.902\n",
      "[40,    10] train loss: 0.340 train acc: 0.863\n",
      "[40,    11] train loss: 0.313 train acc: 0.863\n",
      "[40,    12] train loss: 0.275 train acc: 0.895\n",
      "[40,    13] train loss: 0.294 train acc: 0.883\n",
      "[40,    14] train loss: 0.297 train acc: 0.844\n",
      "[40,    15] train loss: 0.295 train acc: 0.844\n",
      "[40] val loss: 0.195 val acc: 0.997\n",
      "[41,     1] train loss: 0.264 train acc: 0.902\n",
      "[41,     2] train loss: 0.264 train acc: 0.887\n",
      "[41,     3] train loss: 0.329 train acc: 0.852\n",
      "[41,     4] train loss: 0.233 train acc: 0.898\n",
      "[41,     5] train loss: 0.349 train acc: 0.852\n",
      "[41,     6] train loss: 0.250 train acc: 0.898\n",
      "[41,     7] train loss: 0.291 train acc: 0.859\n",
      "[41,     8] train loss: 0.283 train acc: 0.867\n",
      "[41,     9] train loss: 0.262 train acc: 0.875\n",
      "[41,    10] train loss: 0.233 train acc: 0.910\n",
      "[41,    11] train loss: 0.253 train acc: 0.879\n",
      "[41,    12] train loss: 0.315 train acc: 0.879\n",
      "[41,    13] train loss: 0.222 train acc: 0.914\n",
      "[41,    14] train loss: 0.316 train acc: 0.855\n",
      "[41,    15] train loss: 0.212 train acc: 0.922\n",
      "[41] val loss: 0.159 val acc: 0.998\n",
      "[42,     1] train loss: 0.254 train acc: 0.887\n",
      "[42,     2] train loss: 0.211 train acc: 0.910\n",
      "[42,     3] train loss: 0.330 train acc: 0.855\n",
      "[42,     4] train loss: 0.264 train acc: 0.863\n",
      "[42,     5] train loss: 0.300 train acc: 0.887\n",
      "[42,     6] train loss: 0.277 train acc: 0.891\n",
      "[42,     7] train loss: 0.316 train acc: 0.848\n",
      "[42,     8] train loss: 0.302 train acc: 0.875\n",
      "[42,     9] train loss: 0.373 train acc: 0.824\n",
      "[42,    10] train loss: 0.252 train acc: 0.902\n",
      "[42,    11] train loss: 0.290 train acc: 0.895\n",
      "[42,    12] train loss: 0.248 train acc: 0.895\n",
      "[42,    13] train loss: 0.297 train acc: 0.879\n",
      "[42,    14] train loss: 0.335 train acc: 0.863\n",
      "[42,    15] train loss: 0.207 train acc: 0.922\n",
      "[42] val loss: 0.179 val acc: 0.996\n",
      "[43,     1] train loss: 0.278 train acc: 0.898\n",
      "[43,     2] train loss: 0.315 train acc: 0.871\n",
      "[43,     3] train loss: 0.272 train acc: 0.891\n",
      "[43,     4] train loss: 0.309 train acc: 0.859\n",
      "[43,     5] train loss: 0.269 train acc: 0.883\n",
      "[43,     6] train loss: 0.337 train acc: 0.875\n",
      "[43,     7] train loss: 0.274 train acc: 0.898\n",
      "[43,     8] train loss: 0.256 train acc: 0.871\n",
      "[43,     9] train loss: 0.308 train acc: 0.848\n",
      "[43,    10] train loss: 0.263 train acc: 0.883\n",
      "[43,    11] train loss: 0.225 train acc: 0.902\n",
      "[43,    12] train loss: 0.227 train acc: 0.898\n",
      "[43,    13] train loss: 0.280 train acc: 0.879\n",
      "[43,    14] train loss: 0.309 train acc: 0.859\n",
      "[43,    15] train loss: 0.305 train acc: 0.859\n",
      "[43] val loss: 0.148 val acc: 0.999\n",
      "[44,     1] train loss: 0.314 train acc: 0.859\n",
      "[44,     2] train loss: 0.294 train acc: 0.879\n",
      "[44,     3] train loss: 0.325 train acc: 0.867\n",
      "[44,     4] train loss: 0.271 train acc: 0.879\n",
      "[44,     5] train loss: 0.284 train acc: 0.863\n",
      "[44,     6] train loss: 0.307 train acc: 0.875\n",
      "[44,     7] train loss: 0.236 train acc: 0.918\n",
      "[44,     8] train loss: 0.278 train acc: 0.898\n",
      "[44,     9] train loss: 0.244 train acc: 0.879\n",
      "[44,    10] train loss: 0.292 train acc: 0.859\n",
      "[44,    11] train loss: 0.325 train acc: 0.859\n",
      "[44,    12] train loss: 0.272 train acc: 0.883\n",
      "[44,    13] train loss: 0.293 train acc: 0.871\n",
      "[44,    14] train loss: 0.290 train acc: 0.891\n",
      "[44,    15] train loss: 0.303 train acc: 0.859\n",
      "[44] val loss: 0.169 val acc: 0.998\n",
      "[45,     1] train loss: 0.313 train acc: 0.863\n",
      "[45,     2] train loss: 0.220 train acc: 0.902\n",
      "[45,     3] train loss: 0.248 train acc: 0.879\n",
      "[45,     4] train loss: 0.239 train acc: 0.906\n",
      "[45,     5] train loss: 0.279 train acc: 0.855\n",
      "[45,     6] train loss: 0.294 train acc: 0.867\n",
      "[45,     7] train loss: 0.309 train acc: 0.879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45,     8] train loss: 0.274 train acc: 0.879\n",
      "[45,     9] train loss: 0.298 train acc: 0.875\n",
      "[45,    10] train loss: 0.297 train acc: 0.867\n",
      "[45,    11] train loss: 0.323 train acc: 0.883\n",
      "[45,    12] train loss: 0.255 train acc: 0.914\n",
      "[45,    13] train loss: 0.295 train acc: 0.887\n",
      "[45,    14] train loss: 0.271 train acc: 0.891\n",
      "[45,    15] train loss: 0.164 train acc: 0.922\n",
      "[45] val loss: 0.159 val acc: 0.999\n",
      "[46,     1] train loss: 0.215 train acc: 0.949\n",
      "[46,     2] train loss: 0.300 train acc: 0.855\n",
      "[46,     3] train loss: 0.217 train acc: 0.898\n",
      "[46,     4] train loss: 0.307 train acc: 0.883\n",
      "[46,     5] train loss: 0.248 train acc: 0.887\n",
      "[46,     6] train loss: 0.302 train acc: 0.879\n",
      "[46,     7] train loss: 0.246 train acc: 0.898\n",
      "[46,     8] train loss: 0.213 train acc: 0.906\n",
      "[46,     9] train loss: 0.269 train acc: 0.891\n",
      "[46,    10] train loss: 0.267 train acc: 0.871\n",
      "[46,    11] train loss: 0.249 train acc: 0.898\n",
      "[46,    12] train loss: 0.245 train acc: 0.902\n",
      "[46,    13] train loss: 0.247 train acc: 0.906\n",
      "[46,    14] train loss: 0.279 train acc: 0.898\n",
      "[46,    15] train loss: 0.179 train acc: 0.938\n",
      "[46] val loss: 0.134 val acc: 1.000\n",
      "[47,     1] train loss: 0.269 train acc: 0.898\n",
      "[47,     2] train loss: 0.229 train acc: 0.906\n",
      "[47,     3] train loss: 0.280 train acc: 0.871\n",
      "[47,     4] train loss: 0.268 train acc: 0.879\n",
      "[47,     5] train loss: 0.219 train acc: 0.926\n",
      "[47,     6] train loss: 0.284 train acc: 0.879\n",
      "[47,     7] train loss: 0.224 train acc: 0.906\n",
      "[47,     8] train loss: 0.223 train acc: 0.895\n",
      "[47,     9] train loss: 0.257 train acc: 0.906\n",
      "[47,    10] train loss: 0.220 train acc: 0.902\n",
      "[47,    11] train loss: 0.249 train acc: 0.875\n",
      "[47,    12] train loss: 0.235 train acc: 0.898\n",
      "[47,    13] train loss: 0.221 train acc: 0.914\n",
      "[47,    14] train loss: 0.274 train acc: 0.891\n",
      "[47,    15] train loss: 0.331 train acc: 0.891\n",
      "[47] val loss: 0.130 val acc: 0.999\n",
      "[48,     1] train loss: 0.248 train acc: 0.883\n",
      "[48,     2] train loss: 0.251 train acc: 0.898\n",
      "[48,     3] train loss: 0.261 train acc: 0.887\n",
      "[48,     4] train loss: 0.232 train acc: 0.914\n",
      "[48,     5] train loss: 0.228 train acc: 0.910\n",
      "[48,     6] train loss: 0.241 train acc: 0.887\n",
      "[48,     7] train loss: 0.249 train acc: 0.895\n",
      "[48,     8] train loss: 0.266 train acc: 0.891\n",
      "[48,     9] train loss: 0.280 train acc: 0.879\n",
      "[48,    10] train loss: 0.253 train acc: 0.910\n",
      "[48,    11] train loss: 0.268 train acc: 0.898\n",
      "[48,    12] train loss: 0.293 train acc: 0.883\n",
      "[48,    13] train loss: 0.272 train acc: 0.875\n",
      "[48,    14] train loss: 0.237 train acc: 0.887\n",
      "[48,    15] train loss: 0.310 train acc: 0.891\n",
      "[48] val loss: 0.142 val acc: 0.998\n",
      "[49,     1] train loss: 0.230 train acc: 0.898\n",
      "[49,     2] train loss: 0.241 train acc: 0.883\n",
      "[49,     3] train loss: 0.227 train acc: 0.898\n",
      "[49,     4] train loss: 0.215 train acc: 0.902\n",
      "[49,     5] train loss: 0.266 train acc: 0.883\n",
      "[49,     6] train loss: 0.232 train acc: 0.898\n",
      "[49,     7] train loss: 0.207 train acc: 0.918\n",
      "[49,     8] train loss: 0.282 train acc: 0.891\n",
      "[49,     9] train loss: 0.204 train acc: 0.902\n",
      "[49,    10] train loss: 0.298 train acc: 0.883\n",
      "[49,    11] train loss: 0.255 train acc: 0.898\n",
      "[49,    12] train loss: 0.283 train acc: 0.895\n",
      "[49,    13] train loss: 0.280 train acc: 0.855\n",
      "[49,    14] train loss: 0.275 train acc: 0.898\n",
      "[49,    15] train loss: 0.432 train acc: 0.828\n",
      "[49] val loss: 0.140 val acc: 0.999\n",
      "[50,     1] train loss: 0.276 train acc: 0.902\n",
      "[50,     2] train loss: 0.223 train acc: 0.902\n",
      "[50,     3] train loss: 0.244 train acc: 0.906\n",
      "[50,     4] train loss: 0.233 train acc: 0.918\n",
      "[50,     5] train loss: 0.265 train acc: 0.883\n",
      "[50,     6] train loss: 0.264 train acc: 0.887\n",
      "[50,     7] train loss: 0.227 train acc: 0.902\n",
      "[50,     8] train loss: 0.281 train acc: 0.883\n",
      "[50,     9] train loss: 0.272 train acc: 0.891\n",
      "[50,    10] train loss: 0.256 train acc: 0.887\n",
      "[50,    11] train loss: 0.270 train acc: 0.887\n",
      "[50,    12] train loss: 0.240 train acc: 0.922\n",
      "[50,    13] train loss: 0.224 train acc: 0.910\n",
      "[50,    14] train loss: 0.238 train acc: 0.891\n",
      "[50,    15] train loss: 0.201 train acc: 0.938\n",
      "[50] val loss: 0.139 val acc: 0.999\n",
      "[51,     1] train loss: 0.330 train acc: 0.867\n",
      "[51,     2] train loss: 0.233 train acc: 0.910\n",
      "[51,     3] train loss: 0.278 train acc: 0.875\n",
      "[51,     4] train loss: 0.252 train acc: 0.898\n",
      "[51,     5] train loss: 0.290 train acc: 0.902\n",
      "[51,     6] train loss: 0.280 train acc: 0.863\n",
      "[51,     7] train loss: 0.269 train acc: 0.895\n",
      "[51,     8] train loss: 0.289 train acc: 0.863\n",
      "[51,     9] train loss: 0.247 train acc: 0.883\n",
      "[51,    10] train loss: 0.211 train acc: 0.918\n",
      "[51,    11] train loss: 0.207 train acc: 0.918\n",
      "[51,    12] train loss: 0.195 train acc: 0.922\n",
      "[51,    13] train loss: 0.289 train acc: 0.867\n",
      "[51,    14] train loss: 0.189 train acc: 0.934\n",
      "[51,    15] train loss: 0.206 train acc: 0.922\n",
      "[51] val loss: 0.132 val acc: 0.999\n",
      "[52,     1] train loss: 0.250 train acc: 0.895\n",
      "[52,     2] train loss: 0.288 train acc: 0.891\n",
      "[52,     3] train loss: 0.303 train acc: 0.871\n",
      "[52,     4] train loss: 0.279 train acc: 0.867\n",
      "[52,     5] train loss: 0.312 train acc: 0.859\n",
      "[52,     6] train loss: 0.266 train acc: 0.891\n",
      "[52,     7] train loss: 0.251 train acc: 0.906\n",
      "[52,     8] train loss: 0.194 train acc: 0.906\n",
      "[52,     9] train loss: 0.193 train acc: 0.926\n",
      "[52,    10] train loss: 0.240 train acc: 0.895\n",
      "[52,    11] train loss: 0.294 train acc: 0.871\n",
      "[52,    12] train loss: 0.288 train acc: 0.875\n",
      "[52,    13] train loss: 0.263 train acc: 0.898\n",
      "[52,    14] train loss: 0.233 train acc: 0.891\n",
      "[52,    15] train loss: 0.390 train acc: 0.859\n",
      "[52] val loss: 0.134 val acc: 0.998\n",
      "[53,     1] train loss: 0.352 train acc: 0.848\n",
      "[53,     2] train loss: 0.259 train acc: 0.922\n",
      "[53,     3] train loss: 0.243 train acc: 0.887\n",
      "[53,     4] train loss: 0.253 train acc: 0.895\n",
      "[53,     5] train loss: 0.223 train acc: 0.891\n",
      "[53,     6] train loss: 0.249 train acc: 0.902\n",
      "[53,     7] train loss: 0.192 train acc: 0.934\n",
      "[53,     8] train loss: 0.238 train acc: 0.918\n",
      "[53,     9] train loss: 0.230 train acc: 0.914\n",
      "[53,    10] train loss: 0.237 train acc: 0.887\n",
      "[53,    11] train loss: 0.242 train acc: 0.891\n",
      "[53,    12] train loss: 0.264 train acc: 0.883\n",
      "[53,    13] train loss: 0.251 train acc: 0.891\n",
      "[53,    14] train loss: 0.276 train acc: 0.906\n",
      "[53,    15] train loss: 0.352 train acc: 0.891\n",
      "[53] val loss: 0.141 val acc: 0.999\n",
      "[54,     1] train loss: 0.244 train acc: 0.898\n",
      "[54,     2] train loss: 0.269 train acc: 0.906\n",
      "[54,     3] train loss: 0.263 train acc: 0.875\n",
      "[54,     4] train loss: 0.223 train acc: 0.902\n",
      "[54,     5] train loss: 0.262 train acc: 0.875\n",
      "[54,     6] train loss: 0.236 train acc: 0.910\n",
      "[54,     7] train loss: 0.247 train acc: 0.895\n",
      "[54,     8] train loss: 0.273 train acc: 0.902\n",
      "[54,     9] train loss: 0.316 train acc: 0.891\n",
      "[54,    10] train loss: 0.290 train acc: 0.883\n",
      "[54,    11] train loss: 0.280 train acc: 0.883\n",
      "[54,    12] train loss: 0.289 train acc: 0.879\n",
      "[54,    13] train loss: 0.252 train acc: 0.879\n",
      "[54,    14] train loss: 0.290 train acc: 0.883\n",
      "[54,    15] train loss: 0.347 train acc: 0.875\n",
      "[54] val loss: 0.127 val acc: 0.999\n",
      "[55,     1] train loss: 0.274 train acc: 0.883\n",
      "[55,     2] train loss: 0.249 train acc: 0.914\n",
      "[55,     3] train loss: 0.252 train acc: 0.895\n",
      "[55,     4] train loss: 0.272 train acc: 0.883\n",
      "[55,     5] train loss: 0.277 train acc: 0.871\n",
      "[55,     6] train loss: 0.257 train acc: 0.898\n",
      "[55,     7] train loss: 0.291 train acc: 0.891\n",
      "[55,     8] train loss: 0.238 train acc: 0.922\n",
      "[55,     9] train loss: 0.220 train acc: 0.887\n",
      "[55,    10] train loss: 0.273 train acc: 0.895\n",
      "[55,    11] train loss: 0.341 train acc: 0.844\n",
      "[55,    12] train loss: 0.227 train acc: 0.887\n",
      "[55,    13] train loss: 0.269 train acc: 0.887\n",
      "[55,    14] train loss: 0.259 train acc: 0.891\n",
      "[55,    15] train loss: 0.206 train acc: 0.922\n",
      "[55] val loss: 0.144 val acc: 0.998\n",
      "[56,     1] train loss: 0.296 train acc: 0.887\n",
      "[56,     2] train loss: 0.229 train acc: 0.902\n",
      "[56,     3] train loss: 0.229 train acc: 0.910\n",
      "[56,     4] train loss: 0.201 train acc: 0.926\n",
      "[56,     5] train loss: 0.214 train acc: 0.910\n",
      "[56,     6] train loss: 0.185 train acc: 0.914\n",
      "[56,     7] train loss: 0.298 train acc: 0.895\n",
      "[56,     8] train loss: 0.208 train acc: 0.934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56,     9] train loss: 0.272 train acc: 0.902\n",
      "[56,    10] train loss: 0.260 train acc: 0.898\n",
      "[56,    11] train loss: 0.249 train acc: 0.910\n",
      "[56,    12] train loss: 0.214 train acc: 0.910\n",
      "[56,    13] train loss: 0.186 train acc: 0.926\n",
      "[56,    14] train loss: 0.218 train acc: 0.910\n",
      "[56,    15] train loss: 0.252 train acc: 0.891\n",
      "[56] val loss: 0.116 val acc: 0.999\n",
      "[57,     1] train loss: 0.258 train acc: 0.906\n",
      "[57,     2] train loss: 0.248 train acc: 0.914\n",
      "[57,     3] train loss: 0.309 train acc: 0.883\n",
      "[57,     4] train loss: 0.188 train acc: 0.926\n",
      "[57,     5] train loss: 0.221 train acc: 0.906\n",
      "[57,     6] train loss: 0.195 train acc: 0.914\n",
      "[57,     7] train loss: 0.212 train acc: 0.906\n",
      "[57,     8] train loss: 0.246 train acc: 0.926\n",
      "[57,     9] train loss: 0.270 train acc: 0.891\n",
      "[57,    10] train loss: 0.239 train acc: 0.918\n",
      "[57,    11] train loss: 0.284 train acc: 0.867\n",
      "[57,    12] train loss: 0.279 train acc: 0.891\n",
      "[57,    13] train loss: 0.304 train acc: 0.875\n",
      "[57,    14] train loss: 0.186 train acc: 0.918\n",
      "[57,    15] train loss: 0.164 train acc: 0.922\n",
      "[57] val loss: 0.110 val acc: 0.999\n",
      "[58,     1] train loss: 0.261 train acc: 0.895\n",
      "[58,     2] train loss: 0.249 train acc: 0.891\n",
      "[58,     3] train loss: 0.235 train acc: 0.883\n",
      "[58,     4] train loss: 0.162 train acc: 0.926\n",
      "[58,     5] train loss: 0.223 train acc: 0.906\n",
      "[58,     6] train loss: 0.313 train acc: 0.867\n",
      "[58,     7] train loss: 0.217 train acc: 0.898\n",
      "[58,     8] train loss: 0.237 train acc: 0.918\n",
      "[58,     9] train loss: 0.207 train acc: 0.922\n",
      "[58,    10] train loss: 0.282 train acc: 0.891\n",
      "[58,    11] train loss: 0.210 train acc: 0.926\n",
      "[58,    12] train loss: 0.301 train acc: 0.867\n",
      "[58,    13] train loss: 0.244 train acc: 0.887\n",
      "[58,    14] train loss: 0.216 train acc: 0.914\n",
      "[58,    15] train loss: 0.156 train acc: 0.969\n",
      "[58] val loss: 0.119 val acc: 0.997\n",
      "[59,     1] train loss: 0.259 train acc: 0.895\n",
      "[59,     2] train loss: 0.273 train acc: 0.895\n",
      "[59,     3] train loss: 0.240 train acc: 0.891\n",
      "[59,     4] train loss: 0.252 train acc: 0.926\n",
      "[59,     5] train loss: 0.211 train acc: 0.898\n",
      "[59,     6] train loss: 0.228 train acc: 0.906\n",
      "[59,     7] train loss: 0.238 train acc: 0.910\n",
      "[59,     8] train loss: 0.238 train acc: 0.891\n",
      "[59,     9] train loss: 0.247 train acc: 0.914\n",
      "[59,    10] train loss: 0.170 train acc: 0.926\n",
      "[59,    11] train loss: 0.217 train acc: 0.910\n",
      "[59,    12] train loss: 0.240 train acc: 0.918\n",
      "[59,    13] train loss: 0.207 train acc: 0.922\n",
      "[59,    14] train loss: 0.288 train acc: 0.891\n",
      "[59,    15] train loss: 0.347 train acc: 0.859\n",
      "[59] val loss: 0.097 val acc: 1.000\n",
      "[60,     1] train loss: 0.209 train acc: 0.906\n",
      "[60,     2] train loss: 0.240 train acc: 0.902\n",
      "[60,     3] train loss: 0.287 train acc: 0.883\n",
      "[60,     4] train loss: 0.173 train acc: 0.934\n",
      "[60,     5] train loss: 0.201 train acc: 0.918\n",
      "[60,     6] train loss: 0.208 train acc: 0.891\n",
      "[60,     7] train loss: 0.277 train acc: 0.887\n",
      "[60,     8] train loss: 0.261 train acc: 0.910\n",
      "[60,     9] train loss: 0.305 train acc: 0.879\n",
      "[60,    10] train loss: 0.220 train acc: 0.914\n",
      "[60,    11] train loss: 0.191 train acc: 0.934\n",
      "[60,    12] train loss: 0.250 train acc: 0.918\n",
      "[60,    13] train loss: 0.216 train acc: 0.918\n",
      "[60,    14] train loss: 0.201 train acc: 0.918\n",
      "[60,    15] train loss: 0.299 train acc: 0.906\n",
      "[60] val loss: 0.112 val acc: 1.000\n",
      "[61,     1] train loss: 0.195 train acc: 0.922\n",
      "[61,     2] train loss: 0.232 train acc: 0.910\n",
      "[61,     3] train loss: 0.212 train acc: 0.918\n",
      "[61,     4] train loss: 0.235 train acc: 0.906\n",
      "[61,     5] train loss: 0.258 train acc: 0.898\n",
      "[61,     6] train loss: 0.219 train acc: 0.902\n",
      "[61,     7] train loss: 0.256 train acc: 0.902\n",
      "[61,     8] train loss: 0.294 train acc: 0.855\n",
      "[61,     9] train loss: 0.201 train acc: 0.895\n",
      "[61,    10] train loss: 0.195 train acc: 0.926\n",
      "[61,    11] train loss: 0.206 train acc: 0.918\n",
      "[61,    12] train loss: 0.210 train acc: 0.922\n",
      "[61,    13] train loss: 0.221 train acc: 0.914\n",
      "[61,    14] train loss: 0.279 train acc: 0.883\n",
      "[61,    15] train loss: 0.234 train acc: 0.906\n",
      "[61] val loss: 0.095 val acc: 1.000\n",
      "[62,     1] train loss: 0.209 train acc: 0.934\n",
      "[62,     2] train loss: 0.218 train acc: 0.906\n",
      "[62,     3] train loss: 0.234 train acc: 0.910\n",
      "[62,     4] train loss: 0.315 train acc: 0.895\n",
      "[62,     5] train loss: 0.185 train acc: 0.934\n",
      "[62,     6] train loss: 0.262 train acc: 0.891\n",
      "[62,     7] train loss: 0.224 train acc: 0.910\n",
      "[62,     8] train loss: 0.229 train acc: 0.914\n",
      "[62,     9] train loss: 0.223 train acc: 0.914\n",
      "[62,    10] train loss: 0.217 train acc: 0.906\n",
      "[62,    11] train loss: 0.224 train acc: 0.902\n",
      "[62,    12] train loss: 0.207 train acc: 0.918\n",
      "[62,    13] train loss: 0.209 train acc: 0.902\n",
      "[62,    14] train loss: 0.171 train acc: 0.930\n",
      "[62,    15] train loss: 0.234 train acc: 0.875\n",
      "[62] val loss: 0.106 val acc: 1.000\n",
      "[63,     1] train loss: 0.239 train acc: 0.891\n",
      "[63,     2] train loss: 0.201 train acc: 0.898\n",
      "[63,     3] train loss: 0.174 train acc: 0.938\n",
      "[63,     4] train loss: 0.279 train acc: 0.859\n",
      "[63,     5] train loss: 0.229 train acc: 0.906\n",
      "[63,     6] train loss: 0.221 train acc: 0.902\n",
      "[63,     7] train loss: 0.191 train acc: 0.922\n",
      "[63,     8] train loss: 0.188 train acc: 0.934\n",
      "[63,     9] train loss: 0.290 train acc: 0.859\n",
      "[63,    10] train loss: 0.242 train acc: 0.887\n",
      "[63,    11] train loss: 0.218 train acc: 0.922\n",
      "[63,    12] train loss: 0.202 train acc: 0.922\n",
      "[63,    13] train loss: 0.284 train acc: 0.887\n",
      "[63,    14] train loss: 0.184 train acc: 0.906\n",
      "[63,    15] train loss: 0.330 train acc: 0.906\n",
      "[63] val loss: 0.096 val acc: 1.000\n",
      "[64,     1] train loss: 0.230 train acc: 0.902\n",
      "[64,     2] train loss: 0.249 train acc: 0.902\n",
      "[64,     3] train loss: 0.252 train acc: 0.887\n",
      "[64,     4] train loss: 0.235 train acc: 0.887\n",
      "[64,     5] train loss: 0.204 train acc: 0.910\n",
      "[64,     6] train loss: 0.259 train acc: 0.887\n",
      "[64,     7] train loss: 0.244 train acc: 0.891\n",
      "[64,     8] train loss: 0.199 train acc: 0.914\n",
      "[64,     9] train loss: 0.220 train acc: 0.910\n",
      "[64,    10] train loss: 0.300 train acc: 0.895\n",
      "[64,    11] train loss: 0.236 train acc: 0.906\n",
      "[64,    12] train loss: 0.252 train acc: 0.895\n",
      "[64,    13] train loss: 0.224 train acc: 0.910\n",
      "[64,    14] train loss: 0.247 train acc: 0.895\n",
      "[64,    15] train loss: 0.231 train acc: 0.922\n",
      "[64] val loss: 0.110 val acc: 1.000\n",
      "[65,     1] train loss: 0.247 train acc: 0.902\n",
      "[65,     2] train loss: 0.257 train acc: 0.895\n",
      "[65,     3] train loss: 0.277 train acc: 0.871\n",
      "[65,     4] train loss: 0.222 train acc: 0.926\n",
      "[65,     5] train loss: 0.202 train acc: 0.926\n",
      "[65,     6] train loss: 0.225 train acc: 0.902\n",
      "[65,     7] train loss: 0.221 train acc: 0.906\n",
      "[65,     8] train loss: 0.191 train acc: 0.914\n",
      "[65,     9] train loss: 0.227 train acc: 0.895\n",
      "[65,    10] train loss: 0.246 train acc: 0.902\n",
      "[65,    11] train loss: 0.217 train acc: 0.926\n",
      "[65,    12] train loss: 0.215 train acc: 0.902\n",
      "[65,    13] train loss: 0.279 train acc: 0.879\n",
      "[65,    14] train loss: 0.180 train acc: 0.930\n",
      "[65,    15] train loss: 0.166 train acc: 0.922\n",
      "[65] val loss: 0.097 val acc: 1.000\n",
      "[66,     1] train loss: 0.189 train acc: 0.930\n",
      "[66,     2] train loss: 0.169 train acc: 0.938\n",
      "[66,     3] train loss: 0.239 train acc: 0.922\n",
      "[66,     4] train loss: 0.233 train acc: 0.910\n",
      "[66,     5] train loss: 0.201 train acc: 0.906\n",
      "[66,     6] train loss: 0.225 train acc: 0.922\n",
      "[66,     7] train loss: 0.192 train acc: 0.930\n",
      "[66,     8] train loss: 0.232 train acc: 0.914\n",
      "[66,     9] train loss: 0.192 train acc: 0.914\n",
      "[66,    10] train loss: 0.191 train acc: 0.926\n",
      "[66,    11] train loss: 0.202 train acc: 0.898\n",
      "[66,    12] train loss: 0.214 train acc: 0.906\n",
      "[66,    13] train loss: 0.214 train acc: 0.914\n",
      "[66,    14] train loss: 0.181 train acc: 0.922\n",
      "[66,    15] train loss: 0.245 train acc: 0.922\n",
      "[66] val loss: 0.079 val acc: 1.000\n",
      "[67,     1] train loss: 0.251 train acc: 0.883\n",
      "[67,     2] train loss: 0.227 train acc: 0.914\n",
      "[67,     3] train loss: 0.215 train acc: 0.926\n",
      "[67,     4] train loss: 0.183 train acc: 0.918\n",
      "[67,     5] train loss: 0.187 train acc: 0.914\n",
      "[67,     6] train loss: 0.293 train acc: 0.898\n",
      "[67,     7] train loss: 0.170 train acc: 0.926\n",
      "[67,     8] train loss: 0.211 train acc: 0.926\n",
      "[67,     9] train loss: 0.208 train acc: 0.914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67,    10] train loss: 0.235 train acc: 0.902\n",
      "[67,    11] train loss: 0.215 train acc: 0.914\n",
      "[67,    12] train loss: 0.215 train acc: 0.926\n",
      "[67,    13] train loss: 0.270 train acc: 0.875\n",
      "[67,    14] train loss: 0.230 train acc: 0.910\n",
      "[67,    15] train loss: 0.275 train acc: 0.844\n",
      "[67] val loss: 0.097 val acc: 1.000\n",
      "[68,     1] train loss: 0.189 train acc: 0.918\n",
      "[68,     2] train loss: 0.228 train acc: 0.922\n",
      "[68,     3] train loss: 0.207 train acc: 0.914\n",
      "[68,     4] train loss: 0.227 train acc: 0.918\n",
      "[68,     5] train loss: 0.218 train acc: 0.930\n",
      "[68,     6] train loss: 0.211 train acc: 0.910\n",
      "[68,     7] train loss: 0.251 train acc: 0.898\n",
      "[68,     8] train loss: 0.234 train acc: 0.887\n",
      "[68,     9] train loss: 0.252 train acc: 0.898\n",
      "[68,    10] train loss: 0.256 train acc: 0.887\n",
      "[68,    11] train loss: 0.184 train acc: 0.934\n",
      "[68,    12] train loss: 0.239 train acc: 0.895\n",
      "[68,    13] train loss: 0.151 train acc: 0.938\n",
      "[68,    14] train loss: 0.232 train acc: 0.891\n",
      "[68,    15] train loss: 0.226 train acc: 0.891\n",
      "[68] val loss: 0.097 val acc: 1.000\n",
      "[69,     1] train loss: 0.182 train acc: 0.934\n",
      "[69,     2] train loss: 0.191 train acc: 0.906\n",
      "[69,     3] train loss: 0.229 train acc: 0.906\n",
      "[69,     4] train loss: 0.193 train acc: 0.934\n",
      "[69,     5] train loss: 0.254 train acc: 0.902\n",
      "[69,     6] train loss: 0.219 train acc: 0.902\n",
      "[69,     7] train loss: 0.195 train acc: 0.922\n",
      "[69,     8] train loss: 0.164 train acc: 0.938\n",
      "[69,     9] train loss: 0.269 train acc: 0.898\n",
      "[69,    10] train loss: 0.193 train acc: 0.922\n",
      "[69,    11] train loss: 0.200 train acc: 0.926\n",
      "[69,    12] train loss: 0.251 train acc: 0.879\n",
      "[69,    13] train loss: 0.192 train acc: 0.895\n",
      "[69,    14] train loss: 0.222 train acc: 0.914\n",
      "[69,    15] train loss: 0.090 train acc: 0.984\n",
      "[69] val loss: 0.093 val acc: 1.000\n",
      "[70,     1] train loss: 0.159 train acc: 0.934\n",
      "[70,     2] train loss: 0.182 train acc: 0.918\n",
      "[70,     3] train loss: 0.167 train acc: 0.934\n",
      "[70,     4] train loss: 0.179 train acc: 0.918\n",
      "[70,     5] train loss: 0.184 train acc: 0.922\n",
      "[70,     6] train loss: 0.166 train acc: 0.930\n",
      "[70,     7] train loss: 0.199 train acc: 0.906\n",
      "[70,     8] train loss: 0.252 train acc: 0.891\n",
      "[70,     9] train loss: 0.170 train acc: 0.906\n",
      "[70,    10] train loss: 0.162 train acc: 0.926\n",
      "[70,    11] train loss: 0.252 train acc: 0.898\n",
      "[70,    12] train loss: 0.192 train acc: 0.918\n",
      "[70,    13] train loss: 0.191 train acc: 0.914\n",
      "[70,    14] train loss: 0.222 train acc: 0.938\n",
      "[70,    15] train loss: 0.187 train acc: 0.906\n",
      "[70] val loss: 0.068 val acc: 1.000\n",
      "[71,     1] train loss: 0.205 train acc: 0.926\n",
      "[71,     2] train loss: 0.193 train acc: 0.926\n",
      "[71,     3] train loss: 0.155 train acc: 0.938\n",
      "[71,     4] train loss: 0.228 train acc: 0.906\n",
      "[71,     5] train loss: 0.153 train acc: 0.949\n",
      "[71,     6] train loss: 0.237 train acc: 0.906\n",
      "[71,     7] train loss: 0.181 train acc: 0.918\n",
      "[71,     8] train loss: 0.160 train acc: 0.934\n",
      "[71,     9] train loss: 0.204 train acc: 0.910\n",
      "[71,    10] train loss: 0.252 train acc: 0.883\n",
      "[71,    11] train loss: 0.202 train acc: 0.914\n",
      "[71,    12] train loss: 0.212 train acc: 0.926\n",
      "[71,    13] train loss: 0.246 train acc: 0.906\n",
      "[71,    14] train loss: 0.166 train acc: 0.918\n",
      "[71,    15] train loss: 0.404 train acc: 0.797\n",
      "[71] val loss: 0.076 val acc: 1.000\n",
      "[72,     1] train loss: 0.201 train acc: 0.934\n",
      "[72,     2] train loss: 0.208 train acc: 0.934\n",
      "[72,     3] train loss: 0.218 train acc: 0.926\n",
      "[72,     4] train loss: 0.193 train acc: 0.926\n",
      "[72,     5] train loss: 0.201 train acc: 0.902\n",
      "[72,     6] train loss: 0.172 train acc: 0.926\n",
      "[72,     7] train loss: 0.198 train acc: 0.926\n",
      "[72,     8] train loss: 0.226 train acc: 0.926\n",
      "[72,     9] train loss: 0.221 train acc: 0.922\n",
      "[72,    10] train loss: 0.260 train acc: 0.879\n",
      "[72,    11] train loss: 0.195 train acc: 0.926\n",
      "[72,    12] train loss: 0.208 train acc: 0.902\n",
      "[72,    13] train loss: 0.203 train acc: 0.930\n",
      "[72,    14] train loss: 0.266 train acc: 0.895\n",
      "[72,    15] train loss: 0.226 train acc: 0.922\n",
      "[72] val loss: 0.085 val acc: 1.000\n",
      "[73,     1] train loss: 0.188 train acc: 0.918\n",
      "[73,     2] train loss: 0.220 train acc: 0.934\n",
      "[73,     3] train loss: 0.204 train acc: 0.918\n",
      "[73,     4] train loss: 0.199 train acc: 0.918\n",
      "[73,     5] train loss: 0.223 train acc: 0.910\n",
      "[73,     6] train loss: 0.239 train acc: 0.891\n",
      "[73,     7] train loss: 0.231 train acc: 0.898\n",
      "[73,     8] train loss: 0.200 train acc: 0.930\n",
      "[73,     9] train loss: 0.227 train acc: 0.914\n",
      "[73,    10] train loss: 0.184 train acc: 0.914\n",
      "[73,    11] train loss: 0.143 train acc: 0.934\n",
      "[73,    12] train loss: 0.208 train acc: 0.918\n",
      "[73,    13] train loss: 0.209 train acc: 0.918\n",
      "[73,    14] train loss: 0.211 train acc: 0.918\n",
      "[73,    15] train loss: 0.342 train acc: 0.859\n",
      "[73] val loss: 0.082 val acc: 1.000\n",
      "[74,     1] train loss: 0.169 train acc: 0.930\n",
      "[74,     2] train loss: 0.241 train acc: 0.895\n",
      "[74,     3] train loss: 0.201 train acc: 0.922\n",
      "[74,     4] train loss: 0.142 train acc: 0.938\n",
      "[74,     5] train loss: 0.192 train acc: 0.906\n",
      "[74,     6] train loss: 0.169 train acc: 0.926\n",
      "[74,     7] train loss: 0.210 train acc: 0.914\n",
      "[74,     8] train loss: 0.156 train acc: 0.938\n",
      "[74,     9] train loss: 0.186 train acc: 0.926\n",
      "[74,    10] train loss: 0.183 train acc: 0.922\n",
      "[74,    11] train loss: 0.207 train acc: 0.895\n",
      "[74,    12] train loss: 0.225 train acc: 0.918\n",
      "[74,    13] train loss: 0.186 train acc: 0.914\n",
      "[74,    14] train loss: 0.170 train acc: 0.930\n",
      "[74,    15] train loss: 0.230 train acc: 0.875\n",
      "[74] val loss: 0.080 val acc: 1.000\n",
      "[75,     1] train loss: 0.165 train acc: 0.930\n",
      "[75,     2] train loss: 0.183 train acc: 0.926\n",
      "[75,     3] train loss: 0.204 train acc: 0.902\n",
      "[75,     4] train loss: 0.152 train acc: 0.938\n",
      "[75,     5] train loss: 0.174 train acc: 0.918\n",
      "[75,     6] train loss: 0.180 train acc: 0.930\n",
      "[75,     7] train loss: 0.258 train acc: 0.887\n",
      "[75,     8] train loss: 0.230 train acc: 0.910\n",
      "[75,     9] train loss: 0.199 train acc: 0.902\n",
      "[75,    10] train loss: 0.198 train acc: 0.934\n",
      "[75,    11] train loss: 0.196 train acc: 0.934\n",
      "[75,    12] train loss: 0.195 train acc: 0.906\n",
      "[75,    13] train loss: 0.159 train acc: 0.934\n",
      "[75,    14] train loss: 0.197 train acc: 0.914\n",
      "[75,    15] train loss: 0.201 train acc: 0.953\n",
      "[75] val loss: 0.073 val acc: 1.000\n",
      "[76,     1] train loss: 0.165 train acc: 0.934\n",
      "[76,     2] train loss: 0.213 train acc: 0.914\n",
      "[76,     3] train loss: 0.307 train acc: 0.883\n",
      "[76,     4] train loss: 0.172 train acc: 0.938\n",
      "[76,     5] train loss: 0.240 train acc: 0.910\n",
      "[76,     6] train loss: 0.194 train acc: 0.934\n",
      "[76,     7] train loss: 0.176 train acc: 0.938\n",
      "[76,     8] train loss: 0.224 train acc: 0.922\n",
      "[76,     9] train loss: 0.196 train acc: 0.934\n",
      "[76,    10] train loss: 0.166 train acc: 0.910\n",
      "[76,    11] train loss: 0.221 train acc: 0.918\n",
      "[76,    12] train loss: 0.238 train acc: 0.906\n",
      "[76,    13] train loss: 0.195 train acc: 0.922\n",
      "[76,    14] train loss: 0.214 train acc: 0.926\n",
      "[76,    15] train loss: 0.144 train acc: 0.938\n",
      "[76] val loss: 0.084 val acc: 1.000\n",
      "[77,     1] train loss: 0.174 train acc: 0.930\n",
      "[77,     2] train loss: 0.203 train acc: 0.914\n",
      "[77,     3] train loss: 0.179 train acc: 0.922\n",
      "[77,     4] train loss: 0.268 train acc: 0.895\n",
      "[77,     5] train loss: 0.172 train acc: 0.938\n",
      "[77,     6] train loss: 0.178 train acc: 0.914\n",
      "[77,     7] train loss: 0.178 train acc: 0.930\n",
      "[77,     8] train loss: 0.220 train acc: 0.910\n",
      "[77,     9] train loss: 0.198 train acc: 0.906\n",
      "[77,    10] train loss: 0.174 train acc: 0.934\n",
      "[77,    11] train loss: 0.154 train acc: 0.945\n",
      "[77,    12] train loss: 0.221 train acc: 0.914\n",
      "[77,    13] train loss: 0.213 train acc: 0.918\n",
      "[77,    14] train loss: 0.169 train acc: 0.934\n",
      "[77,    15] train loss: 0.296 train acc: 0.875\n",
      "[77] val loss: 0.071 val acc: 1.000\n",
      "[78,     1] train loss: 0.215 train acc: 0.914\n",
      "[78,     2] train loss: 0.186 train acc: 0.930\n",
      "[78,     3] train loss: 0.231 train acc: 0.914\n",
      "[78,     4] train loss: 0.222 train acc: 0.914\n",
      "[78,     5] train loss: 0.195 train acc: 0.914\n",
      "[78,     6] train loss: 0.278 train acc: 0.887\n",
      "[78,     7] train loss: 0.203 train acc: 0.918\n",
      "[78,     8] train loss: 0.215 train acc: 0.914\n",
      "[78,     9] train loss: 0.233 train acc: 0.895\n",
      "[78,    10] train loss: 0.141 train acc: 0.941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78,    11] train loss: 0.216 train acc: 0.898\n",
      "[78,    12] train loss: 0.249 train acc: 0.883\n",
      "[78,    13] train loss: 0.199 train acc: 0.922\n",
      "[78,    14] train loss: 0.172 train acc: 0.926\n",
      "[78,    15] train loss: 0.183 train acc: 0.953\n",
      "[78] val loss: 0.078 val acc: 1.000\n",
      "[79,     1] train loss: 0.204 train acc: 0.930\n",
      "[79,     2] train loss: 0.178 train acc: 0.930\n",
      "[79,     3] train loss: 0.182 train acc: 0.922\n",
      "[79,     4] train loss: 0.191 train acc: 0.934\n",
      "[79,     5] train loss: 0.264 train acc: 0.875\n",
      "[79,     6] train loss: 0.164 train acc: 0.922\n",
      "[79,     7] train loss: 0.207 train acc: 0.902\n",
      "[79,     8] train loss: 0.165 train acc: 0.930\n",
      "[79,     9] train loss: 0.152 train acc: 0.934\n",
      "[79,    10] train loss: 0.174 train acc: 0.949\n",
      "[79,    11] train loss: 0.191 train acc: 0.926\n",
      "[79,    12] train loss: 0.162 train acc: 0.953\n",
      "[79,    13] train loss: 0.219 train acc: 0.930\n",
      "[79,    14] train loss: 0.166 train acc: 0.949\n",
      "[79,    15] train loss: 0.128 train acc: 0.953\n",
      "[79] val loss: 0.066 val acc: 1.000\n",
      "[80,     1] train loss: 0.174 train acc: 0.926\n",
      "[80,     2] train loss: 0.218 train acc: 0.910\n",
      "[80,     3] train loss: 0.187 train acc: 0.914\n",
      "[80,     4] train loss: 0.176 train acc: 0.934\n",
      "[80,     5] train loss: 0.185 train acc: 0.914\n",
      "[80,     6] train loss: 0.224 train acc: 0.902\n",
      "[80,     7] train loss: 0.146 train acc: 0.949\n",
      "[80,     8] train loss: 0.186 train acc: 0.914\n",
      "[80,     9] train loss: 0.192 train acc: 0.906\n",
      "[80,    10] train loss: 0.238 train acc: 0.922\n",
      "[80,    11] train loss: 0.169 train acc: 0.949\n",
      "[80,    12] train loss: 0.198 train acc: 0.930\n",
      "[80,    13] train loss: 0.153 train acc: 0.934\n",
      "[80,    14] train loss: 0.167 train acc: 0.922\n",
      "[80,    15] train loss: 0.356 train acc: 0.875\n",
      "[80] val loss: 0.061 val acc: 1.000\n",
      "[81,     1] train loss: 0.137 train acc: 0.945\n",
      "[81,     2] train loss: 0.149 train acc: 0.938\n",
      "[81,     3] train loss: 0.184 train acc: 0.918\n",
      "[81,     4] train loss: 0.175 train acc: 0.945\n",
      "[81,     5] train loss: 0.219 train acc: 0.930\n",
      "[81,     6] train loss: 0.209 train acc: 0.910\n",
      "[81,     7] train loss: 0.155 train acc: 0.938\n",
      "[81,     8] train loss: 0.246 train acc: 0.906\n",
      "[81,     9] train loss: 0.228 train acc: 0.906\n",
      "[81,    10] train loss: 0.240 train acc: 0.895\n",
      "[81,    11] train loss: 0.171 train acc: 0.930\n",
      "[81,    12] train loss: 0.204 train acc: 0.914\n",
      "[81,    13] train loss: 0.194 train acc: 0.926\n",
      "[81,    14] train loss: 0.196 train acc: 0.930\n",
      "[81,    15] train loss: 0.136 train acc: 0.953\n",
      "[81] val loss: 0.076 val acc: 1.000\n",
      "[82,     1] train loss: 0.162 train acc: 0.934\n",
      "[82,     2] train loss: 0.198 train acc: 0.930\n",
      "[82,     3] train loss: 0.174 train acc: 0.941\n",
      "[82,     4] train loss: 0.176 train acc: 0.953\n",
      "[82,     5] train loss: 0.192 train acc: 0.910\n",
      "[82,     6] train loss: 0.136 train acc: 0.949\n",
      "[82,     7] train loss: 0.208 train acc: 0.922\n",
      "[82,     8] train loss: 0.294 train acc: 0.906\n",
      "[82,     9] train loss: 0.197 train acc: 0.914\n",
      "[82,    10] train loss: 0.134 train acc: 0.941\n",
      "[82,    11] train loss: 0.182 train acc: 0.922\n",
      "[82,    12] train loss: 0.173 train acc: 0.934\n",
      "[82,    13] train loss: 0.210 train acc: 0.922\n",
      "[82,    14] train loss: 0.163 train acc: 0.945\n",
      "[82,    15] train loss: 0.382 train acc: 0.844\n",
      "[82] val loss: 0.064 val acc: 1.000\n",
      "[83,     1] train loss: 0.280 train acc: 0.891\n",
      "[83,     2] train loss: 0.165 train acc: 0.926\n",
      "[83,     3] train loss: 0.197 train acc: 0.926\n",
      "[83,     4] train loss: 0.208 train acc: 0.902\n",
      "[83,     5] train loss: 0.211 train acc: 0.918\n",
      "[83,     6] train loss: 0.174 train acc: 0.934\n",
      "[83,     7] train loss: 0.194 train acc: 0.922\n",
      "[83,     8] train loss: 0.233 train acc: 0.914\n",
      "[83,     9] train loss: 0.153 train acc: 0.938\n",
      "[83,    10] train loss: 0.179 train acc: 0.938\n",
      "[83,    11] train loss: 0.173 train acc: 0.930\n",
      "[83,    12] train loss: 0.167 train acc: 0.922\n",
      "[83,    13] train loss: 0.164 train acc: 0.930\n",
      "[83,    14] train loss: 0.250 train acc: 0.902\n",
      "[83,    15] train loss: 0.122 train acc: 0.953\n",
      "[83] val loss: 0.066 val acc: 0.999\n",
      "[84,     1] train loss: 0.191 train acc: 0.930\n",
      "[84,     2] train loss: 0.156 train acc: 0.934\n",
      "[84,     3] train loss: 0.191 train acc: 0.930\n",
      "[84,     4] train loss: 0.139 train acc: 0.957\n",
      "[84,     5] train loss: 0.167 train acc: 0.930\n",
      "[84,     6] train loss: 0.165 train acc: 0.941\n",
      "[84,     7] train loss: 0.204 train acc: 0.930\n",
      "[84,     8] train loss: 0.179 train acc: 0.926\n",
      "[84,     9] train loss: 0.209 train acc: 0.910\n",
      "[84,    10] train loss: 0.256 train acc: 0.895\n",
      "[84,    11] train loss: 0.144 train acc: 0.949\n",
      "[84,    12] train loss: 0.200 train acc: 0.918\n",
      "[84,    13] train loss: 0.155 train acc: 0.949\n",
      "[84,    14] train loss: 0.164 train acc: 0.945\n",
      "[84,    15] train loss: 0.241 train acc: 0.906\n",
      "[84] val loss: 0.059 val acc: 1.000\n",
      "[85,     1] train loss: 0.133 train acc: 0.926\n",
      "[85,     2] train loss: 0.140 train acc: 0.961\n",
      "[85,     3] train loss: 0.209 train acc: 0.914\n",
      "[85,     4] train loss: 0.204 train acc: 0.914\n",
      "[85,     5] train loss: 0.230 train acc: 0.910\n",
      "[85,     6] train loss: 0.153 train acc: 0.941\n",
      "[85,     7] train loss: 0.189 train acc: 0.918\n",
      "[85,     8] train loss: 0.174 train acc: 0.938\n",
      "[85,     9] train loss: 0.185 train acc: 0.926\n",
      "[85,    10] train loss: 0.162 train acc: 0.930\n",
      "[85,    11] train loss: 0.187 train acc: 0.930\n",
      "[85,    12] train loss: 0.145 train acc: 0.949\n",
      "[85,    13] train loss: 0.159 train acc: 0.938\n",
      "[85,    14] train loss: 0.153 train acc: 0.953\n",
      "[85,    15] train loss: 0.097 train acc: 0.969\n",
      "[85] val loss: 0.053 val acc: 1.000\n",
      "[86,     1] train loss: 0.121 train acc: 0.961\n",
      "[86,     2] train loss: 0.165 train acc: 0.945\n",
      "[86,     3] train loss: 0.149 train acc: 0.926\n",
      "[86,     4] train loss: 0.199 train acc: 0.926\n",
      "[86,     5] train loss: 0.151 train acc: 0.938\n",
      "[86,     6] train loss: 0.140 train acc: 0.961\n",
      "[86,     7] train loss: 0.215 train acc: 0.906\n",
      "[86,     8] train loss: 0.201 train acc: 0.918\n",
      "[86,     9] train loss: 0.142 train acc: 0.945\n",
      "[86,    10] train loss: 0.231 train acc: 0.922\n",
      "[86,    11] train loss: 0.144 train acc: 0.953\n",
      "[86,    12] train loss: 0.129 train acc: 0.949\n",
      "[86,    13] train loss: 0.181 train acc: 0.930\n",
      "[86,    14] train loss: 0.116 train acc: 0.934\n",
      "[86,    15] train loss: 0.154 train acc: 0.922\n",
      "[86] val loss: 0.047 val acc: 1.000\n",
      "[87,     1] train loss: 0.198 train acc: 0.926\n",
      "[87,     2] train loss: 0.119 train acc: 0.949\n",
      "[87,     3] train loss: 0.147 train acc: 0.953\n",
      "[87,     4] train loss: 0.157 train acc: 0.949\n",
      "[87,     5] train loss: 0.181 train acc: 0.934\n",
      "[87,     6] train loss: 0.155 train acc: 0.926\n",
      "[87,     7] train loss: 0.190 train acc: 0.922\n",
      "[87,     8] train loss: 0.231 train acc: 0.891\n",
      "[87,     9] train loss: 0.231 train acc: 0.898\n",
      "[87,    10] train loss: 0.237 train acc: 0.895\n",
      "[87,    11] train loss: 0.214 train acc: 0.902\n",
      "[87,    12] train loss: 0.229 train acc: 0.902\n",
      "[87,    13] train loss: 0.191 train acc: 0.914\n",
      "[87,    14] train loss: 0.195 train acc: 0.906\n",
      "[87,    15] train loss: 0.115 train acc: 0.969\n",
      "[87] val loss: 0.063 val acc: 1.000\n",
      "[88,     1] train loss: 0.215 train acc: 0.906\n",
      "[88,     2] train loss: 0.167 train acc: 0.930\n",
      "[88,     3] train loss: 0.201 train acc: 0.930\n",
      "[88,     4] train loss: 0.137 train acc: 0.957\n",
      "[88,     5] train loss: 0.144 train acc: 0.934\n",
      "[88,     6] train loss: 0.263 train acc: 0.898\n",
      "[88,     7] train loss: 0.173 train acc: 0.941\n",
      "[88,     8] train loss: 0.173 train acc: 0.941\n",
      "[88,     9] train loss: 0.164 train acc: 0.941\n",
      "[88,    10] train loss: 0.159 train acc: 0.926\n",
      "[88,    11] train loss: 0.165 train acc: 0.945\n",
      "[88,    12] train loss: 0.166 train acc: 0.965\n",
      "[88,    13] train loss: 0.207 train acc: 0.934\n",
      "[88,    14] train loss: 0.195 train acc: 0.926\n",
      "[88,    15] train loss: 0.175 train acc: 0.938\n",
      "[88] val loss: 0.060 val acc: 1.000\n",
      "[89,     1] train loss: 0.153 train acc: 0.941\n",
      "[89,     2] train loss: 0.176 train acc: 0.918\n",
      "[89,     3] train loss: 0.136 train acc: 0.957\n",
      "[89,     4] train loss: 0.116 train acc: 0.961\n",
      "[89,     5] train loss: 0.215 train acc: 0.902\n",
      "[89,     6] train loss: 0.167 train acc: 0.914\n",
      "[89,     7] train loss: 0.229 train acc: 0.914\n",
      "[89,     8] train loss: 0.175 train acc: 0.918\n",
      "[89,     9] train loss: 0.197 train acc: 0.930\n",
      "[89,    10] train loss: 0.192 train acc: 0.926\n",
      "[89,    11] train loss: 0.128 train acc: 0.934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89,    12] train loss: 0.133 train acc: 0.945\n",
      "[89,    13] train loss: 0.201 train acc: 0.918\n",
      "[89,    14] train loss: 0.180 train acc: 0.938\n",
      "[89,    15] train loss: 0.187 train acc: 0.906\n",
      "[89] val loss: 0.051 val acc: 1.000\n",
      "[90,     1] train loss: 0.155 train acc: 0.934\n",
      "[90,     2] train loss: 0.214 train acc: 0.906\n",
      "[90,     3] train loss: 0.186 train acc: 0.926\n",
      "[90,     4] train loss: 0.203 train acc: 0.926\n",
      "[90,     5] train loss: 0.170 train acc: 0.930\n",
      "[90,     6] train loss: 0.151 train acc: 0.945\n",
      "[90,     7] train loss: 0.153 train acc: 0.945\n",
      "[90,     8] train loss: 0.191 train acc: 0.922\n",
      "[90,     9] train loss: 0.122 train acc: 0.949\n",
      "[90,    10] train loss: 0.132 train acc: 0.938\n",
      "[90,    11] train loss: 0.161 train acc: 0.934\n",
      "[90,    12] train loss: 0.185 train acc: 0.945\n",
      "[90,    13] train loss: 0.163 train acc: 0.934\n",
      "[90,    14] train loss: 0.166 train acc: 0.914\n",
      "[90,    15] train loss: 0.271 train acc: 0.922\n",
      "[90] val loss: 0.052 val acc: 1.000\n",
      "[91,     1] train loss: 0.143 train acc: 0.945\n",
      "[91,     2] train loss: 0.173 train acc: 0.934\n",
      "[91,     3] train loss: 0.174 train acc: 0.926\n",
      "[91,     4] train loss: 0.175 train acc: 0.934\n",
      "[91,     5] train loss: 0.172 train acc: 0.930\n",
      "[91,     6] train loss: 0.174 train acc: 0.930\n",
      "[91,     7] train loss: 0.148 train acc: 0.941\n",
      "[91,     8] train loss: 0.200 train acc: 0.926\n",
      "[91,     9] train loss: 0.167 train acc: 0.934\n",
      "[91,    10] train loss: 0.143 train acc: 0.938\n",
      "[91,    11] train loss: 0.168 train acc: 0.961\n",
      "[91,    12] train loss: 0.179 train acc: 0.910\n",
      "[91,    13] train loss: 0.166 train acc: 0.930\n",
      "[91,    14] train loss: 0.148 train acc: 0.926\n",
      "[91,    15] train loss: 0.212 train acc: 0.953\n",
      "[91] val loss: 0.049 val acc: 1.000\n",
      "[92,     1] train loss: 0.183 train acc: 0.922\n",
      "[92,     2] train loss: 0.138 train acc: 0.938\n",
      "[92,     3] train loss: 0.195 train acc: 0.934\n",
      "[92,     4] train loss: 0.152 train acc: 0.934\n",
      "[92,     5] train loss: 0.167 train acc: 0.934\n",
      "[92,     6] train loss: 0.160 train acc: 0.953\n",
      "[92,     7] train loss: 0.181 train acc: 0.914\n",
      "[92,     8] train loss: 0.120 train acc: 0.949\n",
      "[92,     9] train loss: 0.177 train acc: 0.941\n",
      "[92,    10] train loss: 0.170 train acc: 0.930\n",
      "[92,    11] train loss: 0.148 train acc: 0.945\n",
      "[92,    12] train loss: 0.178 train acc: 0.938\n",
      "[92,    13] train loss: 0.120 train acc: 0.957\n",
      "[92,    14] train loss: 0.193 train acc: 0.910\n",
      "[92,    15] train loss: 0.229 train acc: 0.891\n",
      "[92] val loss: 0.050 val acc: 1.000\n",
      "[93,     1] train loss: 0.135 train acc: 0.945\n",
      "[93,     2] train loss: 0.155 train acc: 0.941\n",
      "[93,     3] train loss: 0.134 train acc: 0.934\n",
      "[93,     4] train loss: 0.154 train acc: 0.938\n",
      "[93,     5] train loss: 0.240 train acc: 0.918\n",
      "[93,     6] train loss: 0.196 train acc: 0.910\n",
      "[93,     7] train loss: 0.158 train acc: 0.941\n",
      "[93,     8] train loss: 0.188 train acc: 0.930\n",
      "[93,     9] train loss: 0.110 train acc: 0.961\n",
      "[93,    10] train loss: 0.144 train acc: 0.930\n",
      "[93,    11] train loss: 0.132 train acc: 0.949\n",
      "[93,    12] train loss: 0.159 train acc: 0.945\n",
      "[93,    13] train loss: 0.161 train acc: 0.934\n",
      "[93,    14] train loss: 0.166 train acc: 0.926\n",
      "[93,    15] train loss: 0.133 train acc: 0.953\n",
      "[93] val loss: 0.045 val acc: 1.000\n",
      "[94,     1] train loss: 0.129 train acc: 0.953\n",
      "[94,     2] train loss: 0.173 train acc: 0.918\n",
      "[94,     3] train loss: 0.120 train acc: 0.949\n",
      "[94,     4] train loss: 0.191 train acc: 0.922\n",
      "[94,     5] train loss: 0.142 train acc: 0.949\n",
      "[94,     6] train loss: 0.131 train acc: 0.949\n",
      "[94,     7] train loss: 0.124 train acc: 0.957\n",
      "[94,     8] train loss: 0.152 train acc: 0.934\n",
      "[94,     9] train loss: 0.148 train acc: 0.934\n",
      "[94,    10] train loss: 0.123 train acc: 0.969\n",
      "[94,    11] train loss: 0.190 train acc: 0.910\n",
      "[94,    12] train loss: 0.123 train acc: 0.949\n",
      "[94,    13] train loss: 0.180 train acc: 0.918\n",
      "[94,    14] train loss: 0.229 train acc: 0.902\n",
      "[94,    15] train loss: 0.138 train acc: 0.938\n",
      "[94] val loss: 0.038 val acc: 1.000\n",
      "[95,     1] train loss: 0.250 train acc: 0.918\n",
      "[95,     2] train loss: 0.189 train acc: 0.938\n",
      "[95,     3] train loss: 0.160 train acc: 0.938\n",
      "[95,     4] train loss: 0.201 train acc: 0.918\n",
      "[95,     5] train loss: 0.222 train acc: 0.898\n",
      "[95,     6] train loss: 0.189 train acc: 0.922\n",
      "[95,     7] train loss: 0.152 train acc: 0.949\n",
      "[95,     8] train loss: 0.200 train acc: 0.918\n",
      "[95,     9] train loss: 0.215 train acc: 0.926\n",
      "[95,    10] train loss: 0.175 train acc: 0.910\n",
      "[95,    11] train loss: 0.143 train acc: 0.930\n",
      "[95,    12] train loss: 0.131 train acc: 0.949\n",
      "[95,    13] train loss: 0.110 train acc: 0.965\n",
      "[95,    14] train loss: 0.148 train acc: 0.941\n",
      "[95,    15] train loss: 0.229 train acc: 0.922\n",
      "[95] val loss: 0.050 val acc: 1.000\n",
      "[96,     1] train loss: 0.176 train acc: 0.914\n",
      "[96,     2] train loss: 0.177 train acc: 0.941\n",
      "[96,     3] train loss: 0.135 train acc: 0.941\n",
      "[96,     4] train loss: 0.219 train acc: 0.902\n",
      "[96,     5] train loss: 0.145 train acc: 0.953\n",
      "[96,     6] train loss: 0.109 train acc: 0.957\n",
      "[96,     7] train loss: 0.190 train acc: 0.922\n",
      "[96,     8] train loss: 0.174 train acc: 0.930\n",
      "[96,     9] train loss: 0.157 train acc: 0.941\n",
      "[96,    10] train loss: 0.174 train acc: 0.918\n",
      "[96,    11] train loss: 0.148 train acc: 0.957\n",
      "[96,    12] train loss: 0.135 train acc: 0.953\n",
      "[96,    13] train loss: 0.195 train acc: 0.918\n",
      "[96,    14] train loss: 0.224 train acc: 0.914\n",
      "[96,    15] train loss: 0.153 train acc: 0.938\n",
      "[96] val loss: 0.046 val acc: 1.000\n",
      "[97,     1] train loss: 0.198 train acc: 0.918\n",
      "[97,     2] train loss: 0.208 train acc: 0.926\n",
      "[97,     3] train loss: 0.210 train acc: 0.930\n",
      "[97,     4] train loss: 0.171 train acc: 0.934\n",
      "[97,     5] train loss: 0.191 train acc: 0.922\n",
      "[97,     6] train loss: 0.156 train acc: 0.914\n",
      "[97,     7] train loss: 0.182 train acc: 0.938\n",
      "[97,     8] train loss: 0.204 train acc: 0.922\n",
      "[97,     9] train loss: 0.125 train acc: 0.953\n",
      "[97,    10] train loss: 0.256 train acc: 0.906\n",
      "[97,    11] train loss: 0.179 train acc: 0.926\n",
      "[97,    12] train loss: 0.182 train acc: 0.918\n",
      "[97,    13] train loss: 0.185 train acc: 0.918\n",
      "[97,    14] train loss: 0.139 train acc: 0.949\n",
      "[97,    15] train loss: 0.235 train acc: 0.938\n",
      "[97] val loss: 0.057 val acc: 1.000\n",
      "[98,     1] train loss: 0.163 train acc: 0.922\n",
      "[98,     2] train loss: 0.112 train acc: 0.969\n",
      "[98,     3] train loss: 0.144 train acc: 0.949\n",
      "[98,     4] train loss: 0.126 train acc: 0.957\n",
      "[98,     5] train loss: 0.163 train acc: 0.926\n",
      "[98,     6] train loss: 0.164 train acc: 0.938\n",
      "[98,     7] train loss: 0.184 train acc: 0.918\n",
      "[98,     8] train loss: 0.134 train acc: 0.945\n",
      "[98,     9] train loss: 0.232 train acc: 0.895\n",
      "[98,    10] train loss: 0.162 train acc: 0.941\n",
      "[98,    11] train loss: 0.150 train acc: 0.934\n",
      "[98,    12] train loss: 0.156 train acc: 0.941\n",
      "[98,    13] train loss: 0.173 train acc: 0.926\n",
      "[98,    14] train loss: 0.185 train acc: 0.906\n",
      "[98,    15] train loss: 0.165 train acc: 0.922\n",
      "[98] val loss: 0.050 val acc: 1.000\n",
      "[99,     1] train loss: 0.176 train acc: 0.930\n",
      "[99,     2] train loss: 0.152 train acc: 0.934\n",
      "[99,     3] train loss: 0.121 train acc: 0.961\n",
      "[99,     4] train loss: 0.222 train acc: 0.930\n",
      "[99,     5] train loss: 0.110 train acc: 0.969\n",
      "[99,     6] train loss: 0.186 train acc: 0.918\n",
      "[99,     7] train loss: 0.184 train acc: 0.934\n",
      "[99,     8] train loss: 0.137 train acc: 0.941\n",
      "[99,     9] train loss: 0.131 train acc: 0.953\n",
      "[99,    10] train loss: 0.152 train acc: 0.949\n",
      "[99,    11] train loss: 0.206 train acc: 0.906\n",
      "[99,    12] train loss: 0.115 train acc: 0.949\n",
      "[99,    13] train loss: 0.172 train acc: 0.957\n",
      "[99,    14] train loss: 0.195 train acc: 0.938\n",
      "[99,    15] train loss: 0.158 train acc: 0.969\n",
      "[99] val loss: 0.044 val acc: 1.000\n",
      "[100,     1] train loss: 0.144 train acc: 0.945\n",
      "[100,     2] train loss: 0.161 train acc: 0.934\n",
      "[100,     3] train loss: 0.191 train acc: 0.918\n",
      "[100,     4] train loss: 0.144 train acc: 0.957\n",
      "[100,     5] train loss: 0.186 train acc: 0.926\n",
      "[100,     6] train loss: 0.124 train acc: 0.953\n",
      "[100,     7] train loss: 0.132 train acc: 0.945\n",
      "[100,     8] train loss: 0.160 train acc: 0.949\n",
      "[100,     9] train loss: 0.204 train acc: 0.918\n",
      "[100,    10] train loss: 0.175 train acc: 0.926\n",
      "[100,    11] train loss: 0.129 train acc: 0.949\n",
      "[100,    12] train loss: 0.149 train acc: 0.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100,    13] train loss: 0.139 train acc: 0.934\n",
      "[100,    14] train loss: 0.101 train acc: 0.961\n",
      "[100,    15] train loss: 0.307 train acc: 0.859\n",
      "[100] val loss: 0.045 val acc: 1.000\n",
      "[101,     1] train loss: 0.109 train acc: 0.965\n",
      "[101,     2] train loss: 0.237 train acc: 0.926\n",
      "[101,     3] train loss: 0.128 train acc: 0.945\n",
      "[101,     4] train loss: 0.165 train acc: 0.938\n",
      "[101,     5] train loss: 0.154 train acc: 0.941\n",
      "[101,     6] train loss: 0.179 train acc: 0.934\n",
      "[101,     7] train loss: 0.169 train acc: 0.941\n",
      "[101,     8] train loss: 0.183 train acc: 0.926\n",
      "[101,     9] train loss: 0.148 train acc: 0.945\n",
      "[101,    10] train loss: 0.114 train acc: 0.953\n",
      "[101,    11] train loss: 0.115 train acc: 0.957\n",
      "[101,    12] train loss: 0.124 train acc: 0.953\n",
      "[101,    13] train loss: 0.173 train acc: 0.930\n",
      "[101,    14] train loss: 0.167 train acc: 0.938\n",
      "[101,    15] train loss: 0.085 train acc: 0.984\n",
      "[101] val loss: 0.043 val acc: 1.000\n",
      "[102,     1] train loss: 0.135 train acc: 0.945\n",
      "[102,     2] train loss: 0.190 train acc: 0.930\n",
      "[102,     3] train loss: 0.173 train acc: 0.941\n",
      "[102,     4] train loss: 0.166 train acc: 0.930\n",
      "[102,     5] train loss: 0.133 train acc: 0.949\n",
      "[102,     6] train loss: 0.129 train acc: 0.953\n",
      "[102,     7] train loss: 0.140 train acc: 0.945\n",
      "[102,     8] train loss: 0.160 train acc: 0.938\n",
      "[102,     9] train loss: 0.135 train acc: 0.949\n",
      "[102,    10] train loss: 0.208 train acc: 0.906\n",
      "[102,    11] train loss: 0.146 train acc: 0.922\n",
      "[102,    12] train loss: 0.124 train acc: 0.949\n",
      "[102,    13] train loss: 0.184 train acc: 0.922\n",
      "[102,    14] train loss: 0.131 train acc: 0.945\n",
      "[102,    15] train loss: 0.068 train acc: 1.000\n",
      "[102] val loss: 0.037 val acc: 1.000\n",
      "[103,     1] train loss: 0.114 train acc: 0.973\n",
      "[103,     2] train loss: 0.113 train acc: 0.953\n",
      "[103,     3] train loss: 0.129 train acc: 0.957\n",
      "[103,     4] train loss: 0.169 train acc: 0.938\n",
      "[103,     5] train loss: 0.134 train acc: 0.949\n",
      "[103,     6] train loss: 0.125 train acc: 0.961\n",
      "[103,     7] train loss: 0.153 train acc: 0.934\n",
      "[103,     8] train loss: 0.153 train acc: 0.926\n",
      "[103,     9] train loss: 0.161 train acc: 0.926\n",
      "[103,    10] train loss: 0.162 train acc: 0.938\n",
      "[103,    11] train loss: 0.165 train acc: 0.918\n",
      "[103,    12] train loss: 0.122 train acc: 0.949\n",
      "[103,    13] train loss: 0.156 train acc: 0.934\n",
      "[103,    14] train loss: 0.111 train acc: 0.949\n",
      "[103,    15] train loss: 0.287 train acc: 0.922\n",
      "[103] val loss: 0.039 val acc: 1.000\n",
      "[104,     1] train loss: 0.151 train acc: 0.949\n",
      "[104,     2] train loss: 0.159 train acc: 0.934\n",
      "[104,     3] train loss: 0.157 train acc: 0.941\n",
      "[104,     4] train loss: 0.160 train acc: 0.930\n",
      "[104,     5] train loss: 0.156 train acc: 0.938\n",
      "[104,     6] train loss: 0.144 train acc: 0.930\n",
      "[104,     7] train loss: 0.136 train acc: 0.953\n",
      "[104,     8] train loss: 0.139 train acc: 0.945\n",
      "[104,     9] train loss: 0.114 train acc: 0.953\n",
      "[104,    10] train loss: 0.085 train acc: 0.980\n",
      "[104,    11] train loss: 0.127 train acc: 0.941\n",
      "[104,    12] train loss: 0.134 train acc: 0.941\n",
      "[104,    13] train loss: 0.142 train acc: 0.949\n",
      "[104,    14] train loss: 0.130 train acc: 0.945\n",
      "[104,    15] train loss: 0.120 train acc: 0.969\n",
      "[104] val loss: 0.031 val acc: 1.000\n",
      "[105,     1] train loss: 0.133 train acc: 0.957\n",
      "[105,     2] train loss: 0.148 train acc: 0.945\n",
      "[105,     3] train loss: 0.143 train acc: 0.941\n",
      "[105,     4] train loss: 0.133 train acc: 0.957\n",
      "[105,     5] train loss: 0.114 train acc: 0.961\n",
      "[105,     6] train loss: 0.168 train acc: 0.941\n",
      "[105,     7] train loss: 0.158 train acc: 0.941\n",
      "[105,     8] train loss: 0.178 train acc: 0.934\n",
      "[105,     9] train loss: 0.128 train acc: 0.953\n",
      "[105,    10] train loss: 0.122 train acc: 0.965\n",
      "[105,    11] train loss: 0.162 train acc: 0.941\n",
      "[105,    12] train loss: 0.112 train acc: 0.961\n",
      "[105,    13] train loss: 0.165 train acc: 0.941\n",
      "[105,    14] train loss: 0.102 train acc: 0.961\n",
      "[105,    15] train loss: 0.070 train acc: 0.984\n",
      "[105] val loss: 0.030 val acc: 1.000\n",
      "[106,     1] train loss: 0.150 train acc: 0.941\n",
      "[106,     2] train loss: 0.105 train acc: 0.965\n",
      "[106,     3] train loss: 0.179 train acc: 0.930\n",
      "[106,     4] train loss: 0.122 train acc: 0.953\n",
      "[106,     5] train loss: 0.178 train acc: 0.926\n",
      "[106,     6] train loss: 0.138 train acc: 0.949\n",
      "[106,     7] train loss: 0.200 train acc: 0.945\n",
      "[106,     8] train loss: 0.195 train acc: 0.926\n",
      "[106,     9] train loss: 0.164 train acc: 0.934\n",
      "[106,    10] train loss: 0.194 train acc: 0.941\n",
      "[106,    11] train loss: 0.212 train acc: 0.930\n",
      "[106,    12] train loss: 0.186 train acc: 0.926\n",
      "[106,    13] train loss: 0.158 train acc: 0.945\n",
      "[106,    14] train loss: 0.081 train acc: 0.984\n",
      "[106,    15] train loss: 0.198 train acc: 0.938\n",
      "[106] val loss: 0.033 val acc: 1.000\n",
      "[107,     1] train loss: 0.213 train acc: 0.910\n",
      "[107,     2] train loss: 0.130 train acc: 0.949\n",
      "[107,     3] train loss: 0.125 train acc: 0.953\n",
      "[107,     4] train loss: 0.229 train acc: 0.918\n",
      "[107,     5] train loss: 0.142 train acc: 0.938\n",
      "[107,     6] train loss: 0.236 train acc: 0.922\n",
      "[107,     7] train loss: 0.127 train acc: 0.957\n",
      "[107,     8] train loss: 0.145 train acc: 0.938\n",
      "[107,     9] train loss: 0.156 train acc: 0.938\n",
      "[107,    10] train loss: 0.148 train acc: 0.938\n",
      "[107,    11] train loss: 0.147 train acc: 0.953\n",
      "[107,    12] train loss: 0.121 train acc: 0.930\n",
      "[107,    13] train loss: 0.144 train acc: 0.930\n",
      "[107,    14] train loss: 0.198 train acc: 0.930\n",
      "[107,    15] train loss: 0.122 train acc: 0.969\n",
      "[107] val loss: 0.044 val acc: 1.000\n",
      "[108,     1] train loss: 0.155 train acc: 0.941\n",
      "[108,     2] train loss: 0.145 train acc: 0.945\n",
      "[108,     3] train loss: 0.146 train acc: 0.941\n",
      "[108,     4] train loss: 0.134 train acc: 0.945\n",
      "[108,     5] train loss: 0.103 train acc: 0.953\n",
      "[108,     6] train loss: 0.132 train acc: 0.953\n",
      "[108,     7] train loss: 0.190 train acc: 0.922\n",
      "[108,     8] train loss: 0.114 train acc: 0.957\n",
      "[108,     9] train loss: 0.152 train acc: 0.945\n",
      "[108,    10] train loss: 0.168 train acc: 0.918\n",
      "[108,    11] train loss: 0.138 train acc: 0.941\n",
      "[108,    12] train loss: 0.158 train acc: 0.930\n",
      "[108,    13] train loss: 0.108 train acc: 0.953\n",
      "[108,    14] train loss: 0.138 train acc: 0.941\n",
      "[108,    15] train loss: 0.125 train acc: 0.938\n",
      "[108] val loss: 0.035 val acc: 1.000\n",
      "[109,     1] train loss: 0.125 train acc: 0.953\n",
      "[109,     2] train loss: 0.178 train acc: 0.934\n",
      "[109,     3] train loss: 0.164 train acc: 0.918\n",
      "[109,     4] train loss: 0.173 train acc: 0.926\n",
      "[109,     5] train loss: 0.179 train acc: 0.910\n",
      "[109,     6] train loss: 0.107 train acc: 0.965\n",
      "[109,     7] train loss: 0.163 train acc: 0.930\n",
      "[109,     8] train loss: 0.131 train acc: 0.949\n",
      "[109,     9] train loss: 0.170 train acc: 0.918\n",
      "[109,    10] train loss: 0.181 train acc: 0.922\n",
      "[109,    11] train loss: 0.118 train acc: 0.953\n",
      "[109,    12] train loss: 0.214 train acc: 0.922\n",
      "[109,    13] train loss: 0.174 train acc: 0.914\n",
      "[109,    14] train loss: 0.080 train acc: 0.973\n",
      "[109,    15] train loss: 0.046 train acc: 1.000\n",
      "[109] val loss: 0.035 val acc: 1.000\n",
      "[110,     1] train loss: 0.149 train acc: 0.938\n",
      "[110,     2] train loss: 0.215 train acc: 0.918\n",
      "[110,     3] train loss: 0.153 train acc: 0.945\n",
      "[110,     4] train loss: 0.174 train acc: 0.938\n",
      "[110,     5] train loss: 0.078 train acc: 0.973\n",
      "[110,     6] train loss: 0.141 train acc: 0.957\n",
      "[110,     7] train loss: 0.232 train acc: 0.895\n",
      "[110,     8] train loss: 0.149 train acc: 0.934\n",
      "[110,     9] train loss: 0.143 train acc: 0.953\n",
      "[110,    10] train loss: 0.104 train acc: 0.961\n",
      "[110,    11] train loss: 0.125 train acc: 0.969\n",
      "[110,    12] train loss: 0.173 train acc: 0.922\n",
      "[110,    13] train loss: 0.082 train acc: 0.973\n",
      "[110,    14] train loss: 0.135 train acc: 0.945\n",
      "[110,    15] train loss: 0.212 train acc: 0.906\n",
      "[110] val loss: 0.031 val acc: 1.000\n",
      "[111,     1] train loss: 0.137 train acc: 0.945\n",
      "[111,     2] train loss: 0.145 train acc: 0.945\n",
      "[111,     3] train loss: 0.098 train acc: 0.965\n",
      "[111,     4] train loss: 0.130 train acc: 0.945\n",
      "[111,     5] train loss: 0.145 train acc: 0.934\n",
      "[111,     6] train loss: 0.121 train acc: 0.938\n",
      "[111,     7] train loss: 0.163 train acc: 0.930\n",
      "[111,     8] train loss: 0.101 train acc: 0.973\n",
      "[111,     9] train loss: 0.148 train acc: 0.945\n",
      "[111,    10] train loss: 0.156 train acc: 0.938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111,    11] train loss: 0.153 train acc: 0.949\n",
      "[111,    12] train loss: 0.166 train acc: 0.941\n",
      "[111,    13] train loss: 0.148 train acc: 0.941\n",
      "[111,    14] train loss: 0.124 train acc: 0.957\n",
      "[111,    15] train loss: 0.093 train acc: 0.953\n",
      "[111] val loss: 0.031 val acc: 1.000\n",
      "[112,     1] train loss: 0.123 train acc: 0.949\n",
      "[112,     2] train loss: 0.143 train acc: 0.945\n",
      "[112,     3] train loss: 0.094 train acc: 0.957\n",
      "[112,     4] train loss: 0.107 train acc: 0.953\n",
      "[112,     5] train loss: 0.106 train acc: 0.965\n",
      "[112,     6] train loss: 0.137 train acc: 0.934\n",
      "[112,     7] train loss: 0.205 train acc: 0.926\n",
      "[112,     8] train loss: 0.145 train acc: 0.957\n",
      "[112,     9] train loss: 0.162 train acc: 0.949\n",
      "[112,    10] train loss: 0.174 train acc: 0.918\n",
      "[112,    11] train loss: 0.121 train acc: 0.945\n",
      "[112,    12] train loss: 0.130 train acc: 0.949\n",
      "[112,    13] train loss: 0.129 train acc: 0.953\n",
      "[112,    14] train loss: 0.190 train acc: 0.941\n",
      "[112,    15] train loss: 0.114 train acc: 0.969\n",
      "[112] val loss: 0.035 val acc: 1.000\n",
      "[113,     1] train loss: 0.134 train acc: 0.953\n",
      "[113,     2] train loss: 0.180 train acc: 0.910\n",
      "[113,     3] train loss: 0.136 train acc: 0.938\n",
      "[113,     4] train loss: 0.126 train acc: 0.961\n",
      "[113,     5] train loss: 0.108 train acc: 0.949\n",
      "[113,     6] train loss: 0.131 train acc: 0.945\n",
      "[113,     7] train loss: 0.174 train acc: 0.922\n",
      "[113,     8] train loss: 0.093 train acc: 0.965\n",
      "[113,     9] train loss: 0.143 train acc: 0.941\n",
      "[113,    10] train loss: 0.099 train acc: 0.969\n",
      "[113,    11] train loss: 0.123 train acc: 0.965\n",
      "[113,    12] train loss: 0.103 train acc: 0.965\n",
      "[113,    13] train loss: 0.098 train acc: 0.969\n",
      "[113,    14] train loss: 0.161 train acc: 0.938\n",
      "[113,    15] train loss: 0.115 train acc: 0.938\n",
      "[113] val loss: 0.031 val acc: 1.000\n",
      "[114,     1] train loss: 0.137 train acc: 0.934\n",
      "[114,     2] train loss: 0.120 train acc: 0.957\n",
      "[114,     3] train loss: 0.105 train acc: 0.965\n",
      "[114,     4] train loss: 0.151 train acc: 0.945\n",
      "[114,     5] train loss: 0.132 train acc: 0.949\n",
      "[114,     6] train loss: 0.175 train acc: 0.945\n",
      "[114,     7] train loss: 0.127 train acc: 0.953\n",
      "[114,     8] train loss: 0.132 train acc: 0.953\n",
      "[114,     9] train loss: 0.107 train acc: 0.957\n",
      "[114,    10] train loss: 0.118 train acc: 0.957\n",
      "[114,    11] train loss: 0.151 train acc: 0.938\n",
      "[114,    12] train loss: 0.110 train acc: 0.961\n",
      "[114,    13] train loss: 0.104 train acc: 0.969\n",
      "[114,    14] train loss: 0.124 train acc: 0.961\n",
      "[114,    15] train loss: 0.267 train acc: 0.859\n",
      "[114] val loss: 0.028 val acc: 1.000\n",
      "[115,     1] train loss: 0.140 train acc: 0.957\n",
      "[115,     2] train loss: 0.173 train acc: 0.930\n",
      "[115,     3] train loss: 0.095 train acc: 0.969\n",
      "[115,     4] train loss: 0.110 train acc: 0.941\n",
      "[115,     5] train loss: 0.117 train acc: 0.945\n",
      "[115,     6] train loss: 0.089 train acc: 0.965\n",
      "[115,     7] train loss: 0.103 train acc: 0.965\n",
      "[115,     8] train loss: 0.111 train acc: 0.953\n",
      "[115,     9] train loss: 0.177 train acc: 0.930\n",
      "[115,    10] train loss: 0.166 train acc: 0.922\n",
      "[115,    11] train loss: 0.167 train acc: 0.934\n",
      "[115,    12] train loss: 0.114 train acc: 0.953\n",
      "[115,    13] train loss: 0.106 train acc: 0.965\n",
      "[115,    14] train loss: 0.146 train acc: 0.922\n",
      "[115,    15] train loss: 0.196 train acc: 0.922\n",
      "[115] val loss: 0.032 val acc: 1.000\n",
      "[116,     1] train loss: 0.230 train acc: 0.914\n",
      "[116,     2] train loss: 0.086 train acc: 0.973\n",
      "[116,     3] train loss: 0.177 train acc: 0.945\n",
      "[116,     4] train loss: 0.132 train acc: 0.953\n",
      "[116,     5] train loss: 0.117 train acc: 0.953\n",
      "[116,     6] train loss: 0.143 train acc: 0.930\n",
      "[116,     7] train loss: 0.085 train acc: 0.977\n",
      "[116,     8] train loss: 0.198 train acc: 0.934\n",
      "[116,     9] train loss: 0.127 train acc: 0.949\n",
      "[116,    10] train loss: 0.134 train acc: 0.934\n",
      "[116,    11] train loss: 0.202 train acc: 0.918\n",
      "[116,    12] train loss: 0.193 train acc: 0.941\n",
      "[116,    13] train loss: 0.096 train acc: 0.969\n",
      "[116,    14] train loss: 0.105 train acc: 0.961\n",
      "[116,    15] train loss: 0.119 train acc: 0.938\n",
      "[116] val loss: 0.031 val acc: 1.000\n",
      "[117,     1] train loss: 0.121 train acc: 0.953\n",
      "[117,     2] train loss: 0.185 train acc: 0.922\n",
      "[117,     3] train loss: 0.128 train acc: 0.953\n",
      "[117,     4] train loss: 0.154 train acc: 0.938\n",
      "[117,     5] train loss: 0.141 train acc: 0.930\n",
      "[117,     6] train loss: 0.133 train acc: 0.949\n",
      "[117,     7] train loss: 0.106 train acc: 0.957\n",
      "[117,     8] train loss: 0.092 train acc: 0.969\n",
      "[117,     9] train loss: 0.172 train acc: 0.926\n",
      "[117,    10] train loss: 0.114 train acc: 0.957\n",
      "[117,    11] train loss: 0.224 train acc: 0.906\n",
      "[117,    12] train loss: 0.132 train acc: 0.949\n",
      "[117,    13] train loss: 0.084 train acc: 0.969\n",
      "[117,    14] train loss: 0.137 train acc: 0.945\n",
      "[117,    15] train loss: 0.233 train acc: 0.906\n",
      "[117] val loss: 0.028 val acc: 1.000\n",
      "[118,     1] train loss: 0.157 train acc: 0.945\n",
      "[118,     2] train loss: 0.128 train acc: 0.953\n",
      "[118,     3] train loss: 0.159 train acc: 0.938\n",
      "[118,     4] train loss: 0.162 train acc: 0.922\n",
      "[118,     5] train loss: 0.097 train acc: 0.965\n",
      "[118,     6] train loss: 0.111 train acc: 0.961\n",
      "[118,     7] train loss: 0.087 train acc: 0.965\n",
      "[118,     8] train loss: 0.114 train acc: 0.957\n",
      "[118,     9] train loss: 0.121 train acc: 0.957\n",
      "[118,    10] train loss: 0.180 train acc: 0.934\n",
      "[118,    11] train loss: 0.149 train acc: 0.941\n",
      "[118,    12] train loss: 0.120 train acc: 0.945\n",
      "[118,    13] train loss: 0.148 train acc: 0.938\n",
      "[118,    14] train loss: 0.113 train acc: 0.941\n",
      "[118,    15] train loss: 0.074 train acc: 0.969\n",
      "[118] val loss: 0.032 val acc: 1.000\n",
      "[119,     1] train loss: 0.154 train acc: 0.945\n",
      "[119,     2] train loss: 0.114 train acc: 0.949\n",
      "[119,     3] train loss: 0.155 train acc: 0.941\n",
      "[119,     4] train loss: 0.124 train acc: 0.953\n",
      "[119,     5] train loss: 0.175 train acc: 0.926\n",
      "[119,     6] train loss: 0.137 train acc: 0.945\n",
      "[119,     7] train loss: 0.117 train acc: 0.949\n",
      "[119,     8] train loss: 0.153 train acc: 0.934\n",
      "[119,     9] train loss: 0.190 train acc: 0.922\n",
      "[119,    10] train loss: 0.101 train acc: 0.957\n",
      "[119,    11] train loss: 0.144 train acc: 0.945\n",
      "[119,    12] train loss: 0.123 train acc: 0.965\n",
      "[119,    13] train loss: 0.127 train acc: 0.949\n",
      "[119,    14] train loss: 0.131 train acc: 0.957\n",
      "[119,    15] train loss: 0.123 train acc: 0.938\n",
      "[119] val loss: 0.033 val acc: 1.000\n",
      "[120,     1] train loss: 0.148 train acc: 0.941\n",
      "[120,     2] train loss: 0.101 train acc: 0.965\n",
      "[120,     3] train loss: 0.171 train acc: 0.934\n",
      "[120,     4] train loss: 0.133 train acc: 0.941\n",
      "[120,     5] train loss: 0.143 train acc: 0.949\n",
      "[120,     6] train loss: 0.124 train acc: 0.945\n",
      "[120,     7] train loss: 0.107 train acc: 0.973\n",
      "[120,     8] train loss: 0.093 train acc: 0.961\n",
      "[120,     9] train loss: 0.150 train acc: 0.938\n",
      "[120,    10] train loss: 0.098 train acc: 0.961\n",
      "[120,    11] train loss: 0.135 train acc: 0.945\n",
      "[120,    12] train loss: 0.095 train acc: 0.953\n",
      "[120,    13] train loss: 0.115 train acc: 0.941\n",
      "[120,    14] train loss: 0.241 train acc: 0.895\n",
      "[120,    15] train loss: 0.154 train acc: 0.938\n",
      "[120] val loss: 0.022 val acc: 1.000\n",
      "[121,     1] train loss: 0.168 train acc: 0.934\n",
      "[121,     2] train loss: 0.133 train acc: 0.949\n",
      "[121,     3] train loss: 0.186 train acc: 0.945\n",
      "[121,     4] train loss: 0.117 train acc: 0.949\n",
      "[121,     5] train loss: 0.118 train acc: 0.965\n",
      "[121,     6] train loss: 0.110 train acc: 0.957\n",
      "[121,     7] train loss: 0.133 train acc: 0.953\n",
      "[121,     8] train loss: 0.135 train acc: 0.949\n",
      "[121,     9] train loss: 0.179 train acc: 0.941\n",
      "[121,    10] train loss: 0.119 train acc: 0.949\n",
      "[121,    11] train loss: 0.122 train acc: 0.941\n",
      "[121,    12] train loss: 0.106 train acc: 0.957\n",
      "[121,    13] train loss: 0.133 train acc: 0.949\n",
      "[121,    14] train loss: 0.159 train acc: 0.949\n",
      "[121,    15] train loss: 0.084 train acc: 0.969\n",
      "[121] val loss: 0.029 val acc: 1.000\n",
      "[122,     1] train loss: 0.108 train acc: 0.961\n",
      "[122,     2] train loss: 0.188 train acc: 0.938\n",
      "[122,     3] train loss: 0.155 train acc: 0.945\n",
      "[122,     4] train loss: 0.102 train acc: 0.973\n",
      "[122,     5] train loss: 0.091 train acc: 0.957\n",
      "[122,     6] train loss: 0.115 train acc: 0.949\n",
      "[122,     7] train loss: 0.134 train acc: 0.949\n",
      "[122,     8] train loss: 0.094 train acc: 0.973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122,     9] train loss: 0.126 train acc: 0.953\n",
      "[122,    10] train loss: 0.109 train acc: 0.965\n",
      "[122,    11] train loss: 0.154 train acc: 0.938\n",
      "[122,    12] train loss: 0.135 train acc: 0.953\n",
      "[122,    13] train loss: 0.121 train acc: 0.957\n",
      "[122,    14] train loss: 0.128 train acc: 0.949\n",
      "[122,    15] train loss: 0.048 train acc: 0.969\n",
      "[122] val loss: 0.021 val acc: 1.000\n",
      "[123,     1] train loss: 0.118 train acc: 0.961\n",
      "[123,     2] train loss: 0.154 train acc: 0.949\n",
      "[123,     3] train loss: 0.171 train acc: 0.918\n",
      "[123,     4] train loss: 0.162 train acc: 0.926\n",
      "[123,     5] train loss: 0.174 train acc: 0.926\n",
      "[123,     6] train loss: 0.111 train acc: 0.953\n",
      "[123,     7] train loss: 0.106 train acc: 0.953\n",
      "[123,     8] train loss: 0.134 train acc: 0.941\n",
      "[123,     9] train loss: 0.140 train acc: 0.941\n",
      "[123,    10] train loss: 0.074 train acc: 0.977\n",
      "[123,    11] train loss: 0.149 train acc: 0.938\n",
      "[123,    12] train loss: 0.123 train acc: 0.957\n",
      "[123,    13] train loss: 0.203 train acc: 0.941\n",
      "[123,    14] train loss: 0.149 train acc: 0.953\n",
      "[123,    15] train loss: 0.198 train acc: 0.938\n",
      "[123] val loss: 0.023 val acc: 1.000\n",
      "[124,     1] train loss: 0.154 train acc: 0.941\n",
      "[124,     2] train loss: 0.100 train acc: 0.961\n",
      "[124,     3] train loss: 0.098 train acc: 0.957\n",
      "[124,     4] train loss: 0.117 train acc: 0.953\n",
      "[124,     5] train loss: 0.175 train acc: 0.926\n",
      "[124,     6] train loss: 0.146 train acc: 0.941\n",
      "[124,     7] train loss: 0.105 train acc: 0.969\n",
      "[124,     8] train loss: 0.172 train acc: 0.926\n",
      "[124,     9] train loss: 0.089 train acc: 0.965\n",
      "[124,    10] train loss: 0.156 train acc: 0.941\n",
      "[124,    11] train loss: 0.119 train acc: 0.945\n",
      "[124,    12] train loss: 0.147 train acc: 0.941\n",
      "[124,    13] train loss: 0.207 train acc: 0.934\n",
      "[124,    14] train loss: 0.129 train acc: 0.957\n",
      "[124,    15] train loss: 0.173 train acc: 0.922\n",
      "[124] val loss: 0.027 val acc: 1.000\n",
      "[125,     1] train loss: 0.140 train acc: 0.934\n",
      "[125,     2] train loss: 0.140 train acc: 0.945\n",
      "[125,     3] train loss: 0.145 train acc: 0.957\n",
      "[125,     4] train loss: 0.114 train acc: 0.945\n",
      "[125,     5] train loss: 0.096 train acc: 0.969\n",
      "[125,     6] train loss: 0.079 train acc: 0.977\n",
      "[125,     7] train loss: 0.134 train acc: 0.934\n",
      "[125,     8] train loss: 0.133 train acc: 0.949\n",
      "[125,     9] train loss: 0.128 train acc: 0.949\n",
      "[125,    10] train loss: 0.136 train acc: 0.945\n",
      "[125,    11] train loss: 0.114 train acc: 0.961\n",
      "[125,    12] train loss: 0.165 train acc: 0.934\n",
      "[125,    13] train loss: 0.175 train acc: 0.930\n",
      "[125,    14] train loss: 0.150 train acc: 0.945\n",
      "[125,    15] train loss: 0.096 train acc: 0.969\n",
      "[125] val loss: 0.031 val acc: 1.000\n",
      "[126,     1] train loss: 0.135 train acc: 0.953\n",
      "[126,     2] train loss: 0.141 train acc: 0.938\n",
      "[126,     3] train loss: 0.107 train acc: 0.965\n",
      "[126,     4] train loss: 0.183 train acc: 0.930\n",
      "[126,     5] train loss: 0.087 train acc: 0.961\n",
      "[126,     6] train loss: 0.102 train acc: 0.961\n",
      "[126,     7] train loss: 0.130 train acc: 0.945\n",
      "[126,     8] train loss: 0.182 train acc: 0.930\n",
      "[126,     9] train loss: 0.148 train acc: 0.926\n",
      "[126,    10] train loss: 0.181 train acc: 0.945\n",
      "[126,    11] train loss: 0.130 train acc: 0.949\n",
      "[126,    12] train loss: 0.097 train acc: 0.961\n",
      "[126,    13] train loss: 0.155 train acc: 0.945\n",
      "[126,    14] train loss: 0.089 train acc: 0.969\n",
      "[126,    15] train loss: 0.095 train acc: 0.953\n",
      "[126] val loss: 0.028 val acc: 1.000\n",
      "[127,     1] train loss: 0.101 train acc: 0.965\n",
      "[127,     2] train loss: 0.134 train acc: 0.969\n",
      "[127,     3] train loss: 0.095 train acc: 0.965\n",
      "[127,     4] train loss: 0.103 train acc: 0.965\n",
      "[127,     5] train loss: 0.143 train acc: 0.945\n",
      "[127,     6] train loss: 0.107 train acc: 0.961\n",
      "[127,     7] train loss: 0.104 train acc: 0.965\n",
      "[127,     8] train loss: 0.115 train acc: 0.965\n",
      "[127,     9] train loss: 0.123 train acc: 0.957\n",
      "[127,    10] train loss: 0.154 train acc: 0.938\n",
      "[127,    11] train loss: 0.132 train acc: 0.945\n",
      "[127,    12] train loss: 0.094 train acc: 0.957\n",
      "[127,    13] train loss: 0.173 train acc: 0.938\n",
      "[127,    14] train loss: 0.172 train acc: 0.934\n",
      "[127,    15] train loss: 0.178 train acc: 0.922\n",
      "[127] val loss: 0.023 val acc: 1.000\n",
      "[128,     1] train loss: 0.157 train acc: 0.938\n",
      "[128,     2] train loss: 0.081 train acc: 0.965\n",
      "[128,     3] train loss: 0.150 train acc: 0.941\n",
      "[128,     4] train loss: 0.116 train acc: 0.957\n",
      "[128,     5] train loss: 0.128 train acc: 0.941\n",
      "[128,     6] train loss: 0.114 train acc: 0.953\n",
      "[128,     7] train loss: 0.137 train acc: 0.941\n",
      "[128,     8] train loss: 0.137 train acc: 0.941\n",
      "[128,     9] train loss: 0.149 train acc: 0.934\n",
      "[128,    10] train loss: 0.147 train acc: 0.938\n",
      "[128,    11] train loss: 0.086 train acc: 0.980\n",
      "[128,    12] train loss: 0.111 train acc: 0.953\n",
      "[128,    13] train loss: 0.089 train acc: 0.973\n",
      "[128,    14] train loss: 0.130 train acc: 0.945\n",
      "[128,    15] train loss: 0.068 train acc: 0.984\n",
      "[128] val loss: 0.024 val acc: 1.000\n",
      "[129,     1] train loss: 0.090 train acc: 0.961\n",
      "[129,     2] train loss: 0.087 train acc: 0.965\n",
      "[129,     3] train loss: 0.077 train acc: 0.980\n",
      "[129,     4] train loss: 0.116 train acc: 0.961\n",
      "[129,     5] train loss: 0.135 train acc: 0.938\n",
      "[129,     6] train loss: 0.078 train acc: 0.980\n",
      "[129,     7] train loss: 0.151 train acc: 0.941\n",
      "[129,     8] train loss: 0.109 train acc: 0.957\n",
      "[129,     9] train loss: 0.144 train acc: 0.949\n",
      "[129,    10] train loss: 0.111 train acc: 0.969\n",
      "[129,    11] train loss: 0.124 train acc: 0.953\n",
      "[129,    12] train loss: 0.183 train acc: 0.922\n",
      "[129,    13] train loss: 0.176 train acc: 0.914\n",
      "[129,    14] train loss: 0.087 train acc: 0.961\n",
      "[129,    15] train loss: 0.103 train acc: 0.969\n",
      "[129] val loss: 0.019 val acc: 1.000\n",
      "[130,     1] train loss: 0.112 train acc: 0.938\n",
      "[130,     2] train loss: 0.091 train acc: 0.957\n",
      "[130,     3] train loss: 0.124 train acc: 0.938\n",
      "[130,     4] train loss: 0.122 train acc: 0.941\n",
      "[130,     5] train loss: 0.126 train acc: 0.953\n",
      "[130,     6] train loss: 0.117 train acc: 0.945\n",
      "[130,     7] train loss: 0.078 train acc: 0.977\n",
      "[130,     8] train loss: 0.082 train acc: 0.961\n",
      "[130,     9] train loss: 0.142 train acc: 0.934\n",
      "[130,    10] train loss: 0.104 train acc: 0.957\n",
      "[130,    11] train loss: 0.140 train acc: 0.945\n",
      "[130,    12] train loss: 0.094 train acc: 0.965\n",
      "[130,    13] train loss: 0.105 train acc: 0.953\n",
      "[130,    14] train loss: 0.123 train acc: 0.953\n",
      "[130,    15] train loss: 0.090 train acc: 0.969\n",
      "[130] val loss: 0.019 val acc: 0.999\n",
      "[131,     1] train loss: 0.130 train acc: 0.938\n",
      "[131,     2] train loss: 0.126 train acc: 0.949\n",
      "[131,     3] train loss: 0.122 train acc: 0.953\n",
      "[131,     4] train loss: 0.173 train acc: 0.941\n",
      "[131,     5] train loss: 0.098 train acc: 0.953\n",
      "[131,     6] train loss: 0.140 train acc: 0.949\n",
      "[131,     7] train loss: 0.121 train acc: 0.961\n",
      "[131,     8] train loss: 0.103 train acc: 0.953\n",
      "[131,     9] train loss: 0.191 train acc: 0.922\n",
      "[131,    10] train loss: 0.133 train acc: 0.957\n",
      "[131,    11] train loss: 0.191 train acc: 0.938\n",
      "[131,    12] train loss: 0.140 train acc: 0.949\n",
      "[131,    13] train loss: 0.112 train acc: 0.957\n",
      "[131,    14] train loss: 0.151 train acc: 0.957\n",
      "[131,    15] train loss: 0.048 train acc: 0.984\n",
      "[131] val loss: 0.024 val acc: 1.000\n",
      "[132,     1] train loss: 0.078 train acc: 0.973\n",
      "[132,     2] train loss: 0.090 train acc: 0.977\n",
      "[132,     3] train loss: 0.161 train acc: 0.941\n",
      "[132,     4] train loss: 0.138 train acc: 0.957\n",
      "[132,     5] train loss: 0.141 train acc: 0.945\n",
      "[132,     6] train loss: 0.100 train acc: 0.953\n",
      "[132,     7] train loss: 0.127 train acc: 0.941\n",
      "[132,     8] train loss: 0.134 train acc: 0.949\n",
      "[132,     9] train loss: 0.125 train acc: 0.945\n",
      "[132,    10] train loss: 0.084 train acc: 0.973\n",
      "[132,    11] train loss: 0.100 train acc: 0.953\n",
      "[132,    12] train loss: 0.138 train acc: 0.938\n",
      "[132,    13] train loss: 0.106 train acc: 0.953\n",
      "[132,    14] train loss: 0.118 train acc: 0.957\n",
      "[132,    15] train loss: 0.091 train acc: 0.953\n",
      "[132] val loss: 0.025 val acc: 1.000\n",
      "[133,     1] train loss: 0.087 train acc: 0.961\n",
      "[133,     2] train loss: 0.106 train acc: 0.965\n",
      "[133,     3] train loss: 0.113 train acc: 0.957\n",
      "[133,     4] train loss: 0.123 train acc: 0.961\n",
      "[133,     5] train loss: 0.077 train acc: 0.980\n",
      "[133,     6] train loss: 0.128 train acc: 0.957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[133,     7] train loss: 0.121 train acc: 0.973\n",
      "[133,     8] train loss: 0.156 train acc: 0.949\n",
      "[133,     9] train loss: 0.083 train acc: 0.969\n",
      "[133,    10] train loss: 0.092 train acc: 0.965\n",
      "[133,    11] train loss: 0.101 train acc: 0.965\n",
      "[133,    12] train loss: 0.154 train acc: 0.930\n",
      "[133,    13] train loss: 0.115 train acc: 0.953\n",
      "[133,    14] train loss: 0.106 train acc: 0.949\n",
      "[133,    15] train loss: 0.127 train acc: 0.969\n",
      "[133] val loss: 0.018 val acc: 1.000\n",
      "[134,     1] train loss: 0.120 train acc: 0.961\n",
      "[134,     2] train loss: 0.100 train acc: 0.953\n",
      "[134,     3] train loss: 0.089 train acc: 0.969\n",
      "[134,     4] train loss: 0.134 train acc: 0.941\n",
      "[134,     5] train loss: 0.116 train acc: 0.961\n",
      "[134,     6] train loss: 0.125 train acc: 0.961\n",
      "[134,     7] train loss: 0.143 train acc: 0.953\n",
      "[134,     8] train loss: 0.136 train acc: 0.945\n",
      "[134,     9] train loss: 0.127 train acc: 0.961\n",
      "[134,    10] train loss: 0.092 train acc: 0.961\n",
      "[134,    11] train loss: 0.118 train acc: 0.969\n",
      "[134,    12] train loss: 0.095 train acc: 0.973\n",
      "[134,    13] train loss: 0.139 train acc: 0.945\n",
      "[134,    14] train loss: 0.117 train acc: 0.961\n",
      "[134,    15] train loss: 0.156 train acc: 0.953\n",
      "[134] val loss: 0.021 val acc: 1.000\n",
      "[135,     1] train loss: 0.106 train acc: 0.977\n",
      "[135,     2] train loss: 0.095 train acc: 0.969\n",
      "[135,     3] train loss: 0.110 train acc: 0.957\n",
      "[135,     4] train loss: 0.084 train acc: 0.961\n",
      "[135,     5] train loss: 0.116 train acc: 0.973\n",
      "[135,     6] train loss: 0.092 train acc: 0.953\n",
      "[135,     7] train loss: 0.116 train acc: 0.953\n",
      "[135,     8] train loss: 0.178 train acc: 0.938\n",
      "[135,     9] train loss: 0.121 train acc: 0.953\n",
      "[135,    10] train loss: 0.109 train acc: 0.953\n",
      "[135,    11] train loss: 0.069 train acc: 0.969\n",
      "[135,    12] train loss: 0.124 train acc: 0.953\n",
      "[135,    13] train loss: 0.070 train acc: 0.980\n",
      "[135,    14] train loss: 0.135 train acc: 0.957\n",
      "[135,    15] train loss: 0.104 train acc: 0.953\n",
      "[135] val loss: 0.021 val acc: 1.000\n",
      "[136,     1] train loss: 0.105 train acc: 0.961\n",
      "[136,     2] train loss: 0.118 train acc: 0.953\n",
      "[136,     3] train loss: 0.139 train acc: 0.938\n",
      "[136,     4] train loss: 0.111 train acc: 0.957\n",
      "[136,     5] train loss: 0.114 train acc: 0.949\n",
      "[136,     6] train loss: 0.094 train acc: 0.957\n",
      "[136,     7] train loss: 0.086 train acc: 0.957\n",
      "[136,     8] train loss: 0.148 train acc: 0.945\n",
      "[136,     9] train loss: 0.108 train acc: 0.965\n",
      "[136,    10] train loss: 0.115 train acc: 0.961\n",
      "[136,    11] train loss: 0.114 train acc: 0.949\n",
      "[136,    12] train loss: 0.109 train acc: 0.961\n",
      "[136,    13] train loss: 0.100 train acc: 0.965\n",
      "[136,    14] train loss: 0.126 train acc: 0.953\n",
      "[136,    15] train loss: 0.169 train acc: 0.938\n",
      "[136] val loss: 0.021 val acc: 1.000\n",
      "[137,     1] train loss: 0.121 train acc: 0.953\n",
      "[137,     2] train loss: 0.151 train acc: 0.926\n",
      "[137,     3] train loss: 0.090 train acc: 0.969\n",
      "[137,     4] train loss: 0.129 train acc: 0.953\n",
      "[137,     5] train loss: 0.131 train acc: 0.949\n",
      "[137,     6] train loss: 0.152 train acc: 0.938\n",
      "[137,     7] train loss: 0.115 train acc: 0.945\n",
      "[137,     8] train loss: 0.112 train acc: 0.961\n",
      "[137,     9] train loss: 0.101 train acc: 0.973\n",
      "[137,    10] train loss: 0.121 train acc: 0.949\n",
      "[137,    11] train loss: 0.107 train acc: 0.961\n",
      "[137,    12] train loss: 0.131 train acc: 0.957\n",
      "[137,    13] train loss: 0.229 train acc: 0.910\n",
      "[137,    14] train loss: 0.102 train acc: 0.957\n",
      "[137,    15] train loss: 0.150 train acc: 0.969\n",
      "[137] val loss: 0.022 val acc: 1.000\n",
      "[138,     1] train loss: 0.070 train acc: 0.977\n",
      "[138,     2] train loss: 0.096 train acc: 0.961\n",
      "[138,     3] train loss: 0.137 train acc: 0.949\n",
      "[138,     4] train loss: 0.114 train acc: 0.953\n",
      "[138,     5] train loss: 0.152 train acc: 0.938\n",
      "[138,     6] train loss: 0.149 train acc: 0.949\n",
      "[138,     7] train loss: 0.109 train acc: 0.949\n",
      "[138,     8] train loss: 0.154 train acc: 0.945\n",
      "[138,     9] train loss: 0.107 train acc: 0.961\n",
      "[138,    10] train loss: 0.071 train acc: 0.988\n",
      "[138,    11] train loss: 0.107 train acc: 0.965\n",
      "[138,    12] train loss: 0.098 train acc: 0.961\n",
      "[138,    13] train loss: 0.136 train acc: 0.953\n",
      "[138,    14] train loss: 0.106 train acc: 0.953\n",
      "[138,    15] train loss: 0.105 train acc: 0.953\n",
      "[138] val loss: 0.022 val acc: 1.000\n",
      "[139,     1] train loss: 0.106 train acc: 0.953\n",
      "[139,     2] train loss: 0.132 train acc: 0.945\n",
      "[139,     3] train loss: 0.111 train acc: 0.941\n",
      "[139,     4] train loss: 0.079 train acc: 0.980\n",
      "[139,     5] train loss: 0.098 train acc: 0.965\n",
      "[139,     6] train loss: 0.070 train acc: 0.984\n",
      "[139,     7] train loss: 0.146 train acc: 0.938\n",
      "[139,     8] train loss: 0.099 train acc: 0.949\n",
      "[139,     9] train loss: 0.132 train acc: 0.949\n",
      "[139,    10] train loss: 0.089 train acc: 0.965\n",
      "[139,    11] train loss: 0.125 train acc: 0.961\n",
      "[139,    12] train loss: 0.150 train acc: 0.945\n",
      "[139,    13] train loss: 0.050 train acc: 0.988\n",
      "[139,    14] train loss: 0.116 train acc: 0.953\n",
      "[139,    15] train loss: 0.126 train acc: 0.938\n",
      "[139] val loss: 0.019 val acc: 1.000\n",
      "[140,     1] train loss: 0.117 train acc: 0.945\n",
      "[140,     2] train loss: 0.100 train acc: 0.961\n",
      "[140,     3] train loss: 0.143 train acc: 0.934\n",
      "[140,     4] train loss: 0.164 train acc: 0.957\n",
      "[140,     5] train loss: 0.150 train acc: 0.930\n",
      "[140,     6] train loss: 0.104 train acc: 0.961\n",
      "[140,     7] train loss: 0.143 train acc: 0.957\n",
      "[140,     8] train loss: 0.102 train acc: 0.965\n",
      "[140,     9] train loss: 0.111 train acc: 0.961\n",
      "[140,    10] train loss: 0.176 train acc: 0.930\n",
      "[140,    11] train loss: 0.135 train acc: 0.949\n",
      "[140,    12] train loss: 0.124 train acc: 0.953\n",
      "[140,    13] train loss: 0.127 train acc: 0.949\n",
      "[140,    14] train loss: 0.157 train acc: 0.938\n",
      "[140,    15] train loss: 0.124 train acc: 0.938\n",
      "[140] val loss: 0.024 val acc: 1.000\n",
      "[141,     1] train loss: 0.131 train acc: 0.953\n",
      "[141,     2] train loss: 0.098 train acc: 0.965\n",
      "[141,     3] train loss: 0.108 train acc: 0.957\n",
      "[141,     4] train loss: 0.128 train acc: 0.953\n",
      "[141,     5] train loss: 0.156 train acc: 0.945\n",
      "[141,     6] train loss: 0.114 train acc: 0.953\n",
      "[141,     7] train loss: 0.147 train acc: 0.949\n",
      "[141,     8] train loss: 0.126 train acc: 0.949\n",
      "[141,     9] train loss: 0.078 train acc: 0.969\n",
      "[141,    10] train loss: 0.112 train acc: 0.961\n",
      "[141,    11] train loss: 0.089 train acc: 0.961\n",
      "[141,    12] train loss: 0.117 train acc: 0.957\n",
      "[141,    13] train loss: 0.077 train acc: 0.965\n",
      "[141,    14] train loss: 0.122 train acc: 0.941\n",
      "[141,    15] train loss: 0.131 train acc: 0.969\n",
      "[141] val loss: 0.021 val acc: 1.000\n",
      "[142,     1] train loss: 0.109 train acc: 0.965\n",
      "[142,     2] train loss: 0.124 train acc: 0.949\n",
      "[142,     3] train loss: 0.095 train acc: 0.961\n",
      "[142,     4] train loss: 0.142 train acc: 0.953\n",
      "[142,     5] train loss: 0.109 train acc: 0.949\n",
      "[142,     6] train loss: 0.089 train acc: 0.957\n",
      "[142,     7] train loss: 0.096 train acc: 0.957\n",
      "[142,     8] train loss: 0.117 train acc: 0.969\n",
      "[142,     9] train loss: 0.091 train acc: 0.973\n",
      "[142,    10] train loss: 0.126 train acc: 0.941\n",
      "[142,    11] train loss: 0.124 train acc: 0.957\n",
      "[142,    12] train loss: 0.133 train acc: 0.953\n",
      "[142,    13] train loss: 0.110 train acc: 0.949\n",
      "[142,    14] train loss: 0.085 train acc: 0.965\n",
      "[142,    15] train loss: 0.150 train acc: 0.906\n",
      "[142] val loss: 0.018 val acc: 1.000\n",
      "[143,     1] train loss: 0.112 train acc: 0.953\n",
      "[143,     2] train loss: 0.145 train acc: 0.945\n",
      "[143,     3] train loss: 0.130 train acc: 0.949\n",
      "[143,     4] train loss: 0.102 train acc: 0.957\n",
      "[143,     5] train loss: 0.123 train acc: 0.949\n",
      "[143,     6] train loss: 0.090 train acc: 0.973\n",
      "[143,     7] train loss: 0.103 train acc: 0.969\n",
      "[143,     8] train loss: 0.097 train acc: 0.957\n",
      "[143,     9] train loss: 0.127 train acc: 0.961\n",
      "[143,    10] train loss: 0.068 train acc: 0.977\n",
      "[143,    11] train loss: 0.120 train acc: 0.953\n",
      "[143,    12] train loss: 0.119 train acc: 0.949\n",
      "[143,    13] train loss: 0.129 train acc: 0.957\n",
      "[143,    14] train loss: 0.176 train acc: 0.926\n",
      "[143,    15] train loss: 0.047 train acc: 0.984\n",
      "[143] val loss: 0.018 val acc: 1.000\n",
      "[144,     1] train loss: 0.088 train acc: 0.961\n",
      "[144,     2] train loss: 0.104 train acc: 0.969\n",
      "[144,     3] train loss: 0.168 train acc: 0.934\n",
      "[144,     4] train loss: 0.209 train acc: 0.926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144,     5] train loss: 0.134 train acc: 0.941\n",
      "[144,     6] train loss: 0.099 train acc: 0.961\n",
      "[144,     7] train loss: 0.117 train acc: 0.945\n",
      "[144,     8] train loss: 0.104 train acc: 0.965\n",
      "[144,     9] train loss: 0.150 train acc: 0.953\n",
      "[144,    10] train loss: 0.092 train acc: 0.977\n",
      "[144,    11] train loss: 0.093 train acc: 0.965\n",
      "[144,    12] train loss: 0.123 train acc: 0.953\n",
      "[144,    13] train loss: 0.105 train acc: 0.961\n",
      "[144,    14] train loss: 0.118 train acc: 0.949\n",
      "[144,    15] train loss: 0.101 train acc: 0.938\n",
      "[144] val loss: 0.022 val acc: 1.000\n",
      "[145,     1] train loss: 0.110 train acc: 0.957\n",
      "[145,     2] train loss: 0.115 train acc: 0.953\n",
      "[145,     3] train loss: 0.094 train acc: 0.973\n",
      "[145,     4] train loss: 0.090 train acc: 0.969\n",
      "[145,     5] train loss: 0.096 train acc: 0.969\n",
      "[145,     6] train loss: 0.078 train acc: 0.969\n",
      "[145,     7] train loss: 0.148 train acc: 0.941\n",
      "[145,     8] train loss: 0.140 train acc: 0.945\n",
      "[145,     9] train loss: 0.148 train acc: 0.949\n",
      "[145,    10] train loss: 0.083 train acc: 0.965\n",
      "[145,    11] train loss: 0.249 train acc: 0.906\n",
      "[145,    12] train loss: 0.115 train acc: 0.945\n",
      "[145,    13] train loss: 0.081 train acc: 0.969\n",
      "[145,    14] train loss: 0.123 train acc: 0.953\n",
      "[145,    15] train loss: 0.121 train acc: 0.938\n",
      "[145] val loss: 0.023 val acc: 1.000\n",
      "[146,     1] train loss: 0.073 train acc: 0.980\n",
      "[146,     2] train loss: 0.063 train acc: 0.984\n",
      "[146,     3] train loss: 0.115 train acc: 0.949\n",
      "[146,     4] train loss: 0.120 train acc: 0.957\n",
      "[146,     5] train loss: 0.086 train acc: 0.961\n",
      "[146,     6] train loss: 0.106 train acc: 0.977\n",
      "[146,     7] train loss: 0.152 train acc: 0.930\n",
      "[146,     8] train loss: 0.097 train acc: 0.969\n",
      "[146,     9] train loss: 0.111 train acc: 0.957\n",
      "[146,    10] train loss: 0.218 train acc: 0.934\n",
      "[146,    11] train loss: 0.091 train acc: 0.984\n",
      "[146,    12] train loss: 0.104 train acc: 0.961\n",
      "[146,    13] train loss: 0.144 train acc: 0.949\n",
      "[146,    14] train loss: 0.086 train acc: 0.969\n",
      "[146,    15] train loss: 0.098 train acc: 0.969\n",
      "[146] val loss: 0.022 val acc: 1.000\n",
      "[147,     1] train loss: 0.090 train acc: 0.969\n",
      "[147,     2] train loss: 0.063 train acc: 0.980\n",
      "[147,     3] train loss: 0.091 train acc: 0.953\n",
      "[147,     4] train loss: 0.150 train acc: 0.965\n",
      "[147,     5] train loss: 0.092 train acc: 0.969\n",
      "[147,     6] train loss: 0.080 train acc: 0.973\n",
      "[147,     7] train loss: 0.099 train acc: 0.965\n",
      "[147,     8] train loss: 0.086 train acc: 0.965\n",
      "[147,     9] train loss: 0.102 train acc: 0.957\n",
      "[147,    10] train loss: 0.108 train acc: 0.953\n",
      "[147,    11] train loss: 0.117 train acc: 0.945\n",
      "[147,    12] train loss: 0.103 train acc: 0.969\n",
      "[147,    13] train loss: 0.096 train acc: 0.957\n",
      "[147,    14] train loss: 0.079 train acc: 0.980\n",
      "[147,    15] train loss: 0.071 train acc: 0.984\n",
      "[147] val loss: 0.017 val acc: 1.000\n",
      "[148,     1] train loss: 0.085 train acc: 0.965\n",
      "[148,     2] train loss: 0.149 train acc: 0.941\n",
      "[148,     3] train loss: 0.152 train acc: 0.949\n",
      "[148,     4] train loss: 0.123 train acc: 0.961\n",
      "[148,     5] train loss: 0.134 train acc: 0.949\n",
      "[148,     6] train loss: 0.073 train acc: 0.969\n",
      "[148,     7] train loss: 0.165 train acc: 0.934\n",
      "[148,     8] train loss: 0.067 train acc: 0.969\n",
      "[148,     9] train loss: 0.055 train acc: 0.980\n",
      "[148,    10] train loss: 0.088 train acc: 0.969\n",
      "[148,    11] train loss: 0.131 train acc: 0.941\n",
      "[148,    12] train loss: 0.161 train acc: 0.957\n",
      "[148,    13] train loss: 0.066 train acc: 0.980\n",
      "[148,    14] train loss: 0.074 train acc: 0.965\n",
      "[148,    15] train loss: 0.079 train acc: 0.984\n",
      "[148] val loss: 0.017 val acc: 1.000\n",
      "[149,     1] train loss: 0.116 train acc: 0.930\n",
      "[149,     2] train loss: 0.096 train acc: 0.965\n",
      "[149,     3] train loss: 0.112 train acc: 0.961\n",
      "[149,     4] train loss: 0.119 train acc: 0.961\n",
      "[149,     5] train loss: 0.063 train acc: 0.977\n",
      "[149,     6] train loss: 0.096 train acc: 0.969\n",
      "[149,     7] train loss: 0.120 train acc: 0.938\n",
      "[149,     8] train loss: 0.124 train acc: 0.949\n",
      "[149,     9] train loss: 0.135 train acc: 0.945\n",
      "[149,    10] train loss: 0.150 train acc: 0.945\n",
      "[149,    11] train loss: 0.131 train acc: 0.953\n",
      "[149,    12] train loss: 0.102 train acc: 0.949\n",
      "[149,    13] train loss: 0.069 train acc: 0.973\n",
      "[149,    14] train loss: 0.097 train acc: 0.965\n",
      "[149,    15] train loss: 0.032 train acc: 0.984\n",
      "[149] val loss: 0.016 val acc: 1.000\n",
      "[150,     1] train loss: 0.076 train acc: 0.977\n",
      "[150,     2] train loss: 0.086 train acc: 0.957\n",
      "[150,     3] train loss: 0.161 train acc: 0.945\n",
      "[150,     4] train loss: 0.158 train acc: 0.945\n",
      "[150,     5] train loss: 0.105 train acc: 0.965\n",
      "[150,     6] train loss: 0.097 train acc: 0.965\n",
      "[150,     7] train loss: 0.108 train acc: 0.957\n",
      "[150,     8] train loss: 0.075 train acc: 0.969\n",
      "[150,     9] train loss: 0.099 train acc: 0.965\n",
      "[150,    10] train loss: 0.068 train acc: 0.973\n",
      "[150,    11] train loss: 0.099 train acc: 0.965\n",
      "[150,    12] train loss: 0.096 train acc: 0.965\n",
      "[150,    13] train loss: 0.078 train acc: 0.969\n",
      "[150,    14] train loss: 0.102 train acc: 0.965\n",
      "[150,    15] train loss: 0.189 train acc: 0.953\n",
      "[150] val loss: 0.020 val acc: 1.000\n",
      "[151,     1] train loss: 0.060 train acc: 0.984\n",
      "[151,     2] train loss: 0.151 train acc: 0.953\n",
      "[151,     3] train loss: 0.106 train acc: 0.949\n",
      "[151,     4] train loss: 0.113 train acc: 0.953\n",
      "[151,     5] train loss: 0.077 train acc: 0.973\n",
      "[151,     6] train loss: 0.141 train acc: 0.938\n",
      "[151,     7] train loss: 0.155 train acc: 0.949\n",
      "[151,     8] train loss: 0.113 train acc: 0.957\n",
      "[151,     9] train loss: 0.112 train acc: 0.949\n",
      "[151,    10] train loss: 0.110 train acc: 0.941\n",
      "[151,    11] train loss: 0.095 train acc: 0.965\n",
      "[151,    12] train loss: 0.080 train acc: 0.965\n",
      "[151,    13] train loss: 0.092 train acc: 0.961\n",
      "[151,    14] train loss: 0.079 train acc: 0.977\n",
      "[151,    15] train loss: 0.104 train acc: 0.984\n",
      "[151] val loss: 0.017 val acc: 1.000\n",
      "[152,     1] train loss: 0.130 train acc: 0.961\n",
      "[152,     2] train loss: 0.096 train acc: 0.965\n",
      "[152,     3] train loss: 0.061 train acc: 0.973\n",
      "[152,     4] train loss: 0.090 train acc: 0.973\n",
      "[152,     5] train loss: 0.106 train acc: 0.941\n",
      "[152,     6] train loss: 0.125 train acc: 0.953\n",
      "[152,     7] train loss: 0.082 train acc: 0.965\n",
      "[152,     8] train loss: 0.106 train acc: 0.969\n",
      "[152,     9] train loss: 0.124 train acc: 0.949\n",
      "[152,    10] train loss: 0.132 train acc: 0.949\n",
      "[152,    11] train loss: 0.132 train acc: 0.961\n",
      "[152,    12] train loss: 0.128 train acc: 0.961\n",
      "[152,    13] train loss: 0.093 train acc: 0.953\n",
      "[152,    14] train loss: 0.155 train acc: 0.953\n",
      "[152,    15] train loss: 0.047 train acc: 0.984\n",
      "[152] val loss: 0.017 val acc: 1.000\n",
      "[153,     1] train loss: 0.129 train acc: 0.953\n",
      "[153,     2] train loss: 0.134 train acc: 0.934\n",
      "[153,     3] train loss: 0.089 train acc: 0.965\n",
      "[153,     4] train loss: 0.086 train acc: 0.961\n",
      "[153,     5] train loss: 0.146 train acc: 0.949\n",
      "[153,     6] train loss: 0.086 train acc: 0.965\n",
      "[153,     7] train loss: 0.080 train acc: 0.965\n",
      "[153,     8] train loss: 0.120 train acc: 0.945\n",
      "[153,     9] train loss: 0.076 train acc: 0.969\n",
      "[153,    10] train loss: 0.087 train acc: 0.965\n",
      "[153,    11] train loss: 0.112 train acc: 0.949\n",
      "[153,    12] train loss: 0.064 train acc: 0.977\n",
      "[153,    13] train loss: 0.086 train acc: 0.965\n",
      "[153,    14] train loss: 0.061 train acc: 0.980\n",
      "[153,    15] train loss: 0.114 train acc: 0.984\n",
      "[153] val loss: 0.014 val acc: 1.000\n",
      "[154,     1] train loss: 0.110 train acc: 0.969\n",
      "[154,     2] train loss: 0.080 train acc: 0.961\n",
      "[154,     3] train loss: 0.110 train acc: 0.965\n",
      "[154,     4] train loss: 0.128 train acc: 0.961\n",
      "[154,     5] train loss: 0.078 train acc: 0.965\n",
      "[154,     6] train loss: 0.125 train acc: 0.961\n",
      "[154,     7] train loss: 0.075 train acc: 0.973\n",
      "[154,     8] train loss: 0.117 train acc: 0.973\n",
      "[154,     9] train loss: 0.141 train acc: 0.934\n",
      "[154,    10] train loss: 0.098 train acc: 0.965\n",
      "[154,    11] train loss: 0.095 train acc: 0.961\n",
      "[154,    12] train loss: 0.147 train acc: 0.938\n",
      "[154,    13] train loss: 0.071 train acc: 0.969\n",
      "[154,    14] train loss: 0.072 train acc: 0.977\n",
      "[154,    15] train loss: 0.081 train acc: 0.984\n",
      "[154] val loss: 0.013 val acc: 1.000\n",
      "[155,     1] train loss: 0.091 train acc: 0.969\n",
      "[155,     2] train loss: 0.076 train acc: 0.961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155,     3] train loss: 0.065 train acc: 0.973\n",
      "[155,     4] train loss: 0.090 train acc: 0.973\n",
      "[155,     5] train loss: 0.085 train acc: 0.977\n",
      "[155,     6] train loss: 0.106 train acc: 0.965\n",
      "[155,     7] train loss: 0.146 train acc: 0.941\n",
      "[155,     8] train loss: 0.076 train acc: 0.977\n",
      "[155,     9] train loss: 0.117 train acc: 0.957\n",
      "[155,    10] train loss: 0.092 train acc: 0.957\n",
      "[155,    11] train loss: 0.087 train acc: 0.965\n",
      "[155,    12] train loss: 0.078 train acc: 0.965\n",
      "[155,    13] train loss: 0.095 train acc: 0.957\n",
      "[155,    14] train loss: 0.091 train acc: 0.961\n",
      "[155,    15] train loss: 0.089 train acc: 0.969\n",
      "[155] val loss: 0.012 val acc: 1.000\n",
      "[156,     1] train loss: 0.101 train acc: 0.965\n",
      "[156,     2] train loss: 0.116 train acc: 0.949\n",
      "[156,     3] train loss: 0.134 train acc: 0.941\n",
      "[156,     4] train loss: 0.080 train acc: 0.973\n",
      "[156,     5] train loss: 0.098 train acc: 0.965\n",
      "[156,     6] train loss: 0.119 train acc: 0.945\n",
      "[156,     7] train loss: 0.145 train acc: 0.949\n",
      "[156,     8] train loss: 0.115 train acc: 0.957\n",
      "[156,     9] train loss: 0.084 train acc: 0.969\n",
      "[156,    10] train loss: 0.129 train acc: 0.973\n",
      "[156,    11] train loss: 0.120 train acc: 0.938\n",
      "[156,    12] train loss: 0.117 train acc: 0.965\n",
      "[156,    13] train loss: 0.090 train acc: 0.961\n",
      "[156,    14] train loss: 0.080 train acc: 0.969\n",
      "[156,    15] train loss: 0.131 train acc: 0.953\n",
      "[156] val loss: 0.016 val acc: 1.000\n",
      "[157,     1] train loss: 0.073 train acc: 0.969\n",
      "[157,     2] train loss: 0.108 train acc: 0.957\n",
      "[157,     3] train loss: 0.086 train acc: 0.961\n",
      "[157,     4] train loss: 0.128 train acc: 0.953\n",
      "[157,     5] train loss: 0.082 train acc: 0.973\n",
      "[157,     6] train loss: 0.115 train acc: 0.945\n",
      "[157,     7] train loss: 0.071 train acc: 0.969\n",
      "[157,     8] train loss: 0.069 train acc: 0.973\n",
      "[157,     9] train loss: 0.122 train acc: 0.957\n",
      "[157,    10] train loss: 0.078 train acc: 0.969\n",
      "[157,    11] train loss: 0.098 train acc: 0.973\n",
      "[157,    12] train loss: 0.102 train acc: 0.949\n",
      "[157,    13] train loss: 0.101 train acc: 0.957\n",
      "[157,    14] train loss: 0.099 train acc: 0.957\n",
      "[157,    15] train loss: 0.151 train acc: 0.922\n",
      "[157] val loss: 0.014 val acc: 1.000\n",
      "[158,     1] train loss: 0.117 train acc: 0.957\n",
      "[158,     2] train loss: 0.064 train acc: 0.988\n",
      "[158,     3] train loss: 0.098 train acc: 0.969\n",
      "[158,     4] train loss: 0.077 train acc: 0.961\n",
      "[158,     5] train loss: 0.066 train acc: 0.980\n",
      "[158,     6] train loss: 0.060 train acc: 0.980\n",
      "[158,     7] train loss: 0.057 train acc: 0.984\n",
      "[158,     8] train loss: 0.065 train acc: 0.977\n",
      "[158,     9] train loss: 0.069 train acc: 0.965\n",
      "[158,    10] train loss: 0.101 train acc: 0.953\n",
      "[158,    11] train loss: 0.080 train acc: 0.969\n",
      "[158,    12] train loss: 0.087 train acc: 0.965\n",
      "[158,    13] train loss: 0.096 train acc: 0.957\n",
      "[158,    14] train loss: 0.118 train acc: 0.949\n",
      "[158,    15] train loss: 0.078 train acc: 0.953\n",
      "[158] val loss: 0.012 val acc: 1.000\n",
      "[159,     1] train loss: 0.121 train acc: 0.957\n",
      "[159,     2] train loss: 0.050 train acc: 0.980\n",
      "[159,     3] train loss: 0.097 train acc: 0.961\n",
      "[159,     4] train loss: 0.138 train acc: 0.953\n",
      "[159,     5] train loss: 0.105 train acc: 0.957\n",
      "[159,     6] train loss: 0.107 train acc: 0.957\n",
      "[159,     7] train loss: 0.086 train acc: 0.957\n",
      "[159,     8] train loss: 0.084 train acc: 0.961\n",
      "[159,     9] train loss: 0.140 train acc: 0.945\n",
      "[159,    10] train loss: 0.136 train acc: 0.949\n",
      "[159,    11] train loss: 0.100 train acc: 0.965\n",
      "[159,    12] train loss: 0.151 train acc: 0.941\n",
      "[159,    13] train loss: 0.147 train acc: 0.938\n",
      "[159,    14] train loss: 0.123 train acc: 0.953\n",
      "[159,    15] train loss: 0.089 train acc: 0.984\n",
      "[159] val loss: 0.014 val acc: 1.000\n",
      "[160,     1] train loss: 0.067 train acc: 0.969\n",
      "[160,     2] train loss: 0.134 train acc: 0.965\n",
      "[160,     3] train loss: 0.119 train acc: 0.961\n",
      "[160,     4] train loss: 0.084 train acc: 0.977\n",
      "[160,     5] train loss: 0.093 train acc: 0.961\n",
      "[160,     6] train loss: 0.059 train acc: 0.977\n",
      "[160,     7] train loss: 0.135 train acc: 0.922\n",
      "[160,     8] train loss: 0.063 train acc: 0.988\n",
      "[160,     9] train loss: 0.098 train acc: 0.961\n",
      "[160,    10] train loss: 0.075 train acc: 0.977\n",
      "[160,    11] train loss: 0.103 train acc: 0.957\n",
      "[160,    12] train loss: 0.111 train acc: 0.953\n",
      "[160,    13] train loss: 0.073 train acc: 0.977\n",
      "[160,    14] train loss: 0.067 train acc: 0.980\n",
      "[160,    15] train loss: 0.188 train acc: 0.906\n",
      "[160] val loss: 0.016 val acc: 1.000\n",
      "[161,     1] train loss: 0.091 train acc: 0.957\n",
      "[161,     2] train loss: 0.060 train acc: 0.977\n",
      "[161,     3] train loss: 0.104 train acc: 0.969\n",
      "[161,     4] train loss: 0.085 train acc: 0.969\n",
      "[161,     5] train loss: 0.067 train acc: 0.980\n",
      "[161,     6] train loss: 0.115 train acc: 0.949\n",
      "[161,     7] train loss: 0.082 train acc: 0.961\n",
      "[161,     8] train loss: 0.087 train acc: 0.969\n",
      "[161,     9] train loss: 0.091 train acc: 0.969\n",
      "[161,    10] train loss: 0.110 train acc: 0.961\n",
      "[161,    11] train loss: 0.086 train acc: 0.961\n",
      "[161,    12] train loss: 0.087 train acc: 0.965\n",
      "[161,    13] train loss: 0.087 train acc: 0.965\n",
      "[161,    14] train loss: 0.095 train acc: 0.969\n",
      "[161,    15] train loss: 0.111 train acc: 0.938\n",
      "[161] val loss: 0.013 val acc: 1.000\n",
      "[162,     1] train loss: 0.057 train acc: 0.984\n",
      "[162,     2] train loss: 0.111 train acc: 0.957\n",
      "[162,     3] train loss: 0.102 train acc: 0.961\n",
      "[162,     4] train loss: 0.149 train acc: 0.934\n",
      "[162,     5] train loss: 0.109 train acc: 0.961\n",
      "[162,     6] train loss: 0.064 train acc: 0.980\n",
      "[162,     7] train loss: 0.171 train acc: 0.938\n",
      "[162,     8] train loss: 0.106 train acc: 0.961\n",
      "[162,     9] train loss: 0.101 train acc: 0.957\n",
      "[162,    10] train loss: 0.101 train acc: 0.957\n",
      "[162,    11] train loss: 0.108 train acc: 0.969\n",
      "[162,    12] train loss: 0.159 train acc: 0.953\n",
      "[162,    13] train loss: 0.102 train acc: 0.957\n",
      "[162,    14] train loss: 0.099 train acc: 0.969\n",
      "[162,    15] train loss: 0.089 train acc: 0.969\n",
      "[162] val loss: 0.014 val acc: 1.000\n",
      "[163,     1] train loss: 0.121 train acc: 0.953\n",
      "[163,     2] train loss: 0.134 train acc: 0.945\n",
      "[163,     3] train loss: 0.112 train acc: 0.961\n",
      "[163,     4] train loss: 0.062 train acc: 0.977\n",
      "[163,     5] train loss: 0.093 train acc: 0.969\n",
      "[163,     6] train loss: 0.099 train acc: 0.965\n",
      "[163,     7] train loss: 0.095 train acc: 0.961\n",
      "[163,     8] train loss: 0.094 train acc: 0.973\n",
      "[163,     9] train loss: 0.099 train acc: 0.957\n",
      "[163,    10] train loss: 0.075 train acc: 0.973\n",
      "[163,    11] train loss: 0.068 train acc: 0.973\n",
      "[163,    12] train loss: 0.110 train acc: 0.977\n",
      "[163,    13] train loss: 0.075 train acc: 0.977\n",
      "[163,    14] train loss: 0.064 train acc: 0.969\n",
      "[163,    15] train loss: 0.203 train acc: 0.922\n",
      "[163] val loss: 0.015 val acc: 1.000\n",
      "[164,     1] train loss: 0.093 train acc: 0.969\n",
      "[164,     2] train loss: 0.102 train acc: 0.945\n",
      "[164,     3] train loss: 0.110 train acc: 0.965\n",
      "[164,     4] train loss: 0.095 train acc: 0.957\n",
      "[164,     5] train loss: 0.140 train acc: 0.957\n",
      "[164,     6] train loss: 0.089 train acc: 0.965\n",
      "[164,     7] train loss: 0.124 train acc: 0.949\n",
      "[164,     8] train loss: 0.101 train acc: 0.957\n",
      "[164,     9] train loss: 0.114 train acc: 0.957\n",
      "[164,    10] train loss: 0.093 train acc: 0.957\n",
      "[164,    11] train loss: 0.072 train acc: 0.980\n",
      "[164,    12] train loss: 0.082 train acc: 0.977\n",
      "[164,    13] train loss: 0.080 train acc: 0.965\n",
      "[164,    14] train loss: 0.140 train acc: 0.957\n",
      "[164,    15] train loss: 0.117 train acc: 0.938\n",
      "[164] val loss: 0.015 val acc: 1.000\n",
      "[165,     1] train loss: 0.095 train acc: 0.973\n",
      "[165,     2] train loss: 0.127 train acc: 0.945\n",
      "[165,     3] train loss: 0.136 train acc: 0.961\n",
      "[165,     4] train loss: 0.133 train acc: 0.945\n",
      "[165,     5] train loss: 0.135 train acc: 0.941\n",
      "[165,     6] train loss: 0.095 train acc: 0.965\n",
      "[165,     7] train loss: 0.059 train acc: 0.980\n",
      "[165,     8] train loss: 0.070 train acc: 0.980\n",
      "[165,     9] train loss: 0.103 train acc: 0.965\n",
      "[165,    10] train loss: 0.094 train acc: 0.980\n",
      "[165,    11] train loss: 0.083 train acc: 0.965\n",
      "[165,    12] train loss: 0.082 train acc: 0.961\n",
      "[165,    13] train loss: 0.095 train acc: 0.957\n",
      "[165,    14] train loss: 0.135 train acc: 0.961\n",
      "[165,    15] train loss: 0.167 train acc: 0.906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165] val loss: 0.018 val acc: 1.000\n",
      "[166,     1] train loss: 0.081 train acc: 0.973\n",
      "[166,     2] train loss: 0.065 train acc: 0.973\n",
      "[166,     3] train loss: 0.106 train acc: 0.973\n",
      "[166,     4] train loss: 0.170 train acc: 0.945\n",
      "[166,     5] train loss: 0.087 train acc: 0.957\n",
      "[166,     6] train loss: 0.092 train acc: 0.969\n",
      "[166,     7] train loss: 0.078 train acc: 0.969\n",
      "[166,     8] train loss: 0.102 train acc: 0.969\n",
      "[166,     9] train loss: 0.135 train acc: 0.949\n",
      "[166,    10] train loss: 0.136 train acc: 0.957\n",
      "[166,    11] train loss: 0.107 train acc: 0.949\n",
      "[166,    12] train loss: 0.089 train acc: 0.957\n",
      "[166,    13] train loss: 0.087 train acc: 0.980\n",
      "[166,    14] train loss: 0.073 train acc: 0.969\n",
      "[166,    15] train loss: 0.061 train acc: 0.969\n",
      "[166] val loss: 0.013 val acc: 1.000\n",
      "[167,     1] train loss: 0.070 train acc: 0.980\n",
      "[167,     2] train loss: 0.119 train acc: 0.957\n",
      "[167,     3] train loss: 0.062 train acc: 0.980\n",
      "[167,     4] train loss: 0.076 train acc: 0.965\n",
      "[167,     5] train loss: 0.043 train acc: 0.992\n",
      "[167,     6] train loss: 0.050 train acc: 0.992\n",
      "[167,     7] train loss: 0.079 train acc: 0.969\n",
      "[167,     8] train loss: 0.066 train acc: 0.977\n",
      "[167,     9] train loss: 0.065 train acc: 0.977\n",
      "[167,    10] train loss: 0.100 train acc: 0.965\n",
      "[167,    11] train loss: 0.102 train acc: 0.953\n",
      "[167,    12] train loss: 0.088 train acc: 0.957\n",
      "[167,    13] train loss: 0.112 train acc: 0.957\n",
      "[167,    14] train loss: 0.057 train acc: 0.977\n",
      "[167,    15] train loss: 0.052 train acc: 0.984\n",
      "[167] val loss: 0.011 val acc: 1.000\n",
      "[168,     1] train loss: 0.074 train acc: 0.980\n",
      "[168,     2] train loss: 0.105 train acc: 0.965\n",
      "[168,     3] train loss: 0.093 train acc: 0.977\n",
      "[168,     4] train loss: 0.099 train acc: 0.961\n",
      "[168,     5] train loss: 0.131 train acc: 0.953\n",
      "[168,     6] train loss: 0.125 train acc: 0.945\n",
      "[168,     7] train loss: 0.105 train acc: 0.957\n",
      "[168,     8] train loss: 0.111 train acc: 0.957\n",
      "[168,     9] train loss: 0.042 train acc: 0.988\n",
      "[168,    10] train loss: 0.093 train acc: 0.953\n",
      "[168,    11] train loss: 0.114 train acc: 0.949\n",
      "[168,    12] train loss: 0.152 train acc: 0.934\n",
      "[168,    13] train loss: 0.055 train acc: 0.980\n",
      "[168,    14] train loss: 0.116 train acc: 0.961\n",
      "[168,    15] train loss: 0.088 train acc: 0.953\n",
      "[168] val loss: 0.011 val acc: 1.000\n",
      "[169,     1] train loss: 0.080 train acc: 0.969\n",
      "[169,     2] train loss: 0.119 train acc: 0.945\n",
      "[169,     3] train loss: 0.106 train acc: 0.969\n",
      "[169,     4] train loss: 0.107 train acc: 0.969\n",
      "[169,     5] train loss: 0.159 train acc: 0.945\n",
      "[169,     6] train loss: 0.084 train acc: 0.977\n",
      "[169,     7] train loss: 0.108 train acc: 0.961\n",
      "[169,     8] train loss: 0.156 train acc: 0.941\n",
      "[169,     9] train loss: 0.091 train acc: 0.965\n",
      "[169,    10] train loss: 0.121 train acc: 0.949\n",
      "[169,    11] train loss: 0.113 train acc: 0.965\n",
      "[169,    12] train loss: 0.096 train acc: 0.965\n",
      "[169,    13] train loss: 0.053 train acc: 0.984\n",
      "[169,    14] train loss: 0.162 train acc: 0.941\n",
      "[169,    15] train loss: 0.055 train acc: 0.984\n",
      "[169] val loss: 0.014 val acc: 1.000\n",
      "[170,     1] train loss: 0.069 train acc: 0.973\n",
      "[170,     2] train loss: 0.051 train acc: 0.977\n",
      "[170,     3] train loss: 0.055 train acc: 0.984\n",
      "[170,     4] train loss: 0.078 train acc: 0.977\n",
      "[170,     5] train loss: 0.144 train acc: 0.941\n",
      "[170,     6] train loss: 0.079 train acc: 0.977\n",
      "[170,     7] train loss: 0.093 train acc: 0.969\n",
      "[170,     8] train loss: 0.102 train acc: 0.953\n",
      "[170,     9] train loss: 0.129 train acc: 0.965\n",
      "[170,    10] train loss: 0.132 train acc: 0.965\n",
      "[170,    11] train loss: 0.088 train acc: 0.965\n",
      "[170,    12] train loss: 0.086 train acc: 0.969\n",
      "[170,    13] train loss: 0.110 train acc: 0.961\n",
      "[170,    14] train loss: 0.079 train acc: 0.980\n",
      "[170,    15] train loss: 0.075 train acc: 0.953\n",
      "[170] val loss: 0.014 val acc: 1.000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MALDataset(Dataset):\n",
    "    def __init__(self, r_test_x, r_test_y):\n",
    "        self.r_test_x = r_test_x\n",
    "        self.r_test_y = r_test_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.r_test_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.from_numpy(np.array(r_test_x[idx]))\n",
    "        label = [[1,0],[0,1]]\n",
    "        label = torch.from_numpy(np.array(label[r_test_y[idx]], dtype='float32'))\n",
    "        return data, label\n",
    "\n",
    "ratio = 0.7\n",
    "\n",
    "trainset = MALDataset(r_test_x[:int(len(r_test_x)*ratio)], r_test_y[:int(len(r_test_y)*ratio)])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "valset = MALDataset(r_test_x[int(len(r_test_x)*ratio):], r_test_y[int(len(r_test_y)*ratio):])\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "# Define a Loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "#Train the network\n",
    "for epoch in range(170):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_tot_acc = 0\n",
    "    train_tot_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        outputs = outputs[:,1] > outputs[:,0]\n",
    "        labels = labels[:,1] > labels[:,0]\n",
    "        acc = np.sum(outputs.astype(\"int32\") == labels.astype(\"int32\"))/len(labels)\n",
    "#         print(outputs.astype('int32'), labels.astype(\"int32\"))\n",
    "        train_acc += acc\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        train_tot_loss += loss.item()\n",
    "        train_tot_acc += acc\n",
    "        if i % 1 == 0: \n",
    "            print('[%d, %5d] train loss: %.3f train acc: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1, train_acc/ 1))\n",
    "            running_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "    \n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    for i, data in enumerate(valloader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        outputs = outputs[:,1] > outputs[:,0]\n",
    "        labels = labels[:,1] > labels[:,0]\n",
    "        acc = np.sum(outputs.astype(\"int32\") == labels.astype(\"int32\"))/len(labels)\n",
    "        val_acc += acc\n",
    "    print('[%d] val loss: %.3f val acc: %.3f' %\n",
    "              (epoch + 1, val_loss / len(valloader), val_acc/len(valloader)))\n",
    "    train_accs.append(train_tot_acc/len(trainloader))\n",
    "    train_losses.append(train_tot_loss/len(trainloader))\n",
    "    val_accs.append(val_acc / len(valloader))\n",
    "    val_losses.append(val_loss / len(valloader))\n",
    "\n",
    "PATH = './Path/dlmal_e_net.pth'\n",
    "torch.save(net, PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1cd2a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABHfElEQVR4nO3dd3hc1bXw4d+a0aj3ZluSZcm94oIbLvRebCD0ErpD6JBGAkm4CeHmJiT5gBAMIQRMTDXNAVNsXCjuvRe5SrKs3rs0+/vjHMkjWbLHZVTX+zx6GJ0ys+Ygz5pdzl5ijEEppZTyhqO9A1BKKdV5aNJQSinlNU0aSimlvKZJQymllNc0aSillPKaJg2llFJe06ShFCAir4vI014eu09Ezvd1TEp1RJo0lFJKeU2ThlJdiIj4tXcMqmvTpKE6Dbtb6GcislFEykXkXyLSQ0Q+F5FSEVkgIlEex08TkS0iUiQii0VkiMe+0SKy1j7vXSCw2WtdLiLr7XOXishpXsZ4mYisE5ESEUkXkaea7Z9iP1+Rvf92e3uQiPxFRPaLSLGIfGdvO1tEMlq4Dufbj58SkTki8h8RKQFuF5HxIrLMfo0sEfm7iPh7nD9MROaLSIGIZIvIr0Skp4hUiEiMx3FjRCRXRFzevHfVPWjSUJ3ND4ALgIHAFcDnwK+AOKy/54cARGQg8DbwiL1vHvBfEfG3P0A/Bt4EooH37efFPnc08BrwIyAGeBmYKyIBXsRXDvwQiAQuA34sIlfaz9vHjvcFO6ZRwHr7vGeB04FJdkw/B9xeXpPpwBz7NWcD9cCjQCxwBnAecJ8dQxiwAPgCSAD6A18bYw4Bi4HrPJ73VuAdY0ytl3GobkCThupsXjDGZBtjMoFvgRXGmHXGmCrgI2C0fdz1wGfGmPn2h96zQBDWh/JEwAX8P2NMrTFmDrDK4zVmAC8bY1YYY+qNMW8A1fZ5R2WMWWyM2WSMcRtjNmIlrrPs3TcBC4wxb9uvm2+MWS8iDuBO4GFjTKb9mkuNMdVeXpNlxpiP7desNMasMcYsN8bUGWP2YSW9hhguBw4ZY/5ijKkyxpQaY1bY+94AbgEQESdwI1ZiVaqRJg3V2WR7PK5s4fdQ+3ECsL9hhzHGDaQDifa+TNN0tc79Ho/7AD+xu3eKRKQI6G2fd1QiMkFEFtndOsXAvVjf+LGfY3cLp8VidY+1tM8b6c1iGCgin4rIIbvL6hkvYgD4BBgqIqlYrbliY8zKE4xJdVGaNFRXdRDrwx8AERGsD8xMIAtItLc1SPZ4nA78wRgT6fETbIx524vXfQuYC/Q2xkQAM4GG10kH+rVwTh5Q1cq+ciDY4304sbq2PDVfqvolYDswwBgTjtV95xlD35YCt1tr72G1Nm5FWxmqBZo0VFf1HnCZiJxnD+T+BKuLaSmwDKgDHhIRl4hcDYz3OPefwL12q0FEJMQe4A7z4nXDgAJjTJWIjMfqkmowGzhfRK4TET8RiRGRUXYr6DXgryKSICJOETnDHkPZCQTar+8CngSONbYSBpQAZSIyGPixx75PgV4i8oiIBIhImIhM8Ng/C7gdmIYmDdUCTRqqSzLG7MD6xvwC1jf5K4ArjDE1xpga4GqsD8cCrPGPDz3OXQ3cA/wdKATS7GO9cR/wOxEpBX6DlbwanvcAcClWAivAGgQfae/+KbAJa2ylAPg/wGGMKbaf81WsVlI50GQ2VQt+ipWsSrES4LseMZRidT1dARwCdgHneOz/HmsAfq0xxrPLTikARIswKaU8ichC4C1jzKvtHYvqeDRpKKUaicg4YD7WmExpe8ejOh7tnlJKASAib2Ddw/GIJgzVGm1pKKWU8pq2NJRSSnmtyyxuFhsba1JSUto7DKWU6lTWrFmTZ4xpfu9Pq7pM0khJSWH16tXtHYZSSnUqInJcU6u1e0oppZTXNGkopZTymiYNpZRSXusyYxotqa2tJSMjg6qqqvYOpdMKDAwkKSkJl0vr8CilunjSyMjIICwsjJSUFJouaKq8YYwhPz+fjIwMUlNT2zscpVQH4LPuKRF5TURyRGRzK/tFRJ4XkTSxyneO8dh3m4jssn9uO9EYqqqqiImJ0YRxgkSEmJgYbakppRr5ckzjdeDio+y/BBhg/8zAqgGAiEQDvwUmYC1X/VvxqPt8vDRhnBy9fkopTz7rnjLGfCMiKUc5ZDowy66etlxEIkWkF3A2MN8YUwAgIvOxko83BXBUWzMGcrfD3m8hdSrEDzn1r5G7EzbPsV6rI/Pzh1E3Q3gCHFgOaV+3d0SdX3A0DLwYjBt2fgmVhe0dUccUngBj72iTl2rPMY1EmpapzLC3tbb9CCIyA6uVQnJyckuHqNbUVoK7HvxDoL4WaisgMAJEoK4G3HXgH3zkeVXFsO876DMJ8vfAZ49B1nprn8MPJj0EZ/3cevzZT2DLR9a+6L4w+HI4/TYIjYfyPFj7Bmz/DMQJF/0B4gbBniVQXWLFNfhy68PirWuhcB+Hi891VAa+ew76ngXbP7W3dfSYOzoDXzzu8btezxYlje0WSeOkGWNeAV4BGDt2bIf8GlpUVMRbb73Ffffdd1znXXrppbz11ltERkae+qCMgfw0KzGIE0y9tT0kDkJ7QP4uqK+ByD7gFwBlubBpCwy9Et65GfZ9ayUFdz2E9YRLn4WUKbD0Bfjur7DlQ4hKgT2LYcR11rfFzDWw6GlY9gKMvhXWz7a+NSaeDiUH4V8X2s9ZezjOEddZLZfCfXDrR9Dv3FN/LU6lgj3w6WOw43OY/Aic9YuWE6/yXuE+63oiMOgSiOpzrDOUj7Vn0sjEqtncIMnelonVReW5fXGbRXWKFRUV8Y9//OOIpFFXV4efX+uXf968ecf/YsYABsQBbjeUZoLDBYGR4Ao8fFxthZUwgmOtb/J+AVaSKM+1WhL1teAKhiJ7dYG6KvjgLlj1LziwFM79NVSXgtMFkx60WigAV/4DRt4Anz5qJYxLn4Xx9xx+3dydVstk2d8h+Qy4/G9WUqgusxJObQUMuhQiEmH927D4GUBg4CUdP2GA1Zq69SOoKYMAbyrDqmOKSoGJPz7mYarttGfSmAs8ICLvYA16FxtjskTkS+AZj8HvC4FfnuyL/c9/t7D1YMnJPk0TQxPC+e0Vw456zOOPP87u3bsZNWoULpeLwMBAoqKi2L59Ozt37uTKK68kPT2dqqoqHn74YWbMmAEcXkurrKyMSy65hClTprB06VISExP55JNPCAoKavpCxkDBXv7571m88s5/qamqoH+fBN58/vcEBwWRXRvKvY/8nD179oC7jpee/hmTLr+FWbPf4tlnn0VEOG1QCm/+v6esFkZgJJSkWy2RcJf1wb3zc5hwL5z509bfcOqZ8OOlUJwBMf2a7osbCLf9F3K2QtwQcNjzMAJC4Zxm/4vP+jlU5Fstkov+cOz/GR2FiCYM1aX5rJ6GiLyN1WKIBbKxZkS5AIwxM8WalvN3rEHuCuAOuzYzInIn8Cv7qf5gjPn3sV5v7NixpvmChdu2bWPIEGtgtr2Sxr59+7j88svZvHkzixcv5rLLLmPz5s2N9z0UFBQQHR1NZWUl48aNY8mSJcTExDRJGv3792f16tWMGjWK6667jmmXXsQtl0yEsF5W1w9ARQEU7Se/oJiY6CjAzZN/e50eiSk8eNPFXH/Po5xx9oU88uhj1B/aSll5JRlVgVx11VUsXbqU2NhYCvJyiQ4Lsj7Em1/Hgf2tbqmUM8HZht81aiq0i0cpHxKRNcaYsd4e78vZUzceY78B7m9l32vAa6cynmN9uLeV8ePHN7lR7vnnn+ejj6zB4vT0dHbt2EHMoHhrnMG4AUhNTWXUqFEAnD5yOPu2rYMLx0BpFgRFWl1RJZngCmZzdh5P3n0nRSVllFXWcNFFF0HkDBZ+t4JZr/wd6mtxuquJiE9k1uvvc+211xIbGwtAdOxRVkd2utqni0gThlIdSqceCO+MQkJCGh8vXryYBQsWsGzZMoKDgzn77LOpKs2H6iBrkDl3F7j9CQjwt06oKsZZXUSl20BEbyhOt1oYlYXW8THJ3H73xXz80UeMHDmS1994g8WLF1szkcRhdffk1VjPFRDe9m9eKdXp6YKFPhYWFkZpacvllouLi4mKiiI4OJjt27ezfPlya4DZ4bJmEjmcUJ4DddXWOEDBHmt7UAwEx4BfkJU4asogMhlcQZSWltIrIYHaujpmz57d+FrnnX8BL733FdTXUm8cFFfUcO655/L++++Tn58PWF1lSil1NNrS8LGYmBgmT57M8OHDCQoKokePHo37Lr74YmbOnMmQIUMYNGgQEydOsPrwgyKtlkFsf/CLtpOICwKDISQWyiusAdewnlC4F8ISGsc2fv/73zNhwgTi4uKYMGFCY8J67rnnmDFjBv/6z7s4HU5emjmTM844gyeeeIKzzjoLp9PJ6NGjef3119vhKimlOgufDYS3tWMNhHcodVVQsA+iU63prnU1UF9tbS/OgNhB3vfl19da4w0+1GGvo1LqpHWYgXB1FJVFUFdp/Tesh3U/RE2Ztc8ZAK6go53dlI8ThlJKedKk0VaMGxCrW6nanvpbVWx1K9WUWfdFOJwQYC/lcQz3338/33//fZNtDz/8MHfc0TZLCSiluidNGm3BGOtuaP9gCE+0xi3EAbXl1owmsMYnjqOF8eKLL/ooWKWUap3OnmoLtRVWd1RFPlTkAcZKEgCl2VaXlF/gUZ9CKaU6Ak0abaGyAGt1ToGSLKuVERJnTZ/Fbc+W0tU7lVIdnyYNXzNua8A7MMKaLosB/1ArcTQs9BcY2Y4BKqWU93RM41QxBmrKra4oU3+4JVFdZq0oGxRlrRxbUWC1LABC4o9/tpRSSrUjTRonyxhrDajyvMN1KcC6szsyxdonTggMt1oXPYfTWEjGFdh0yXIgNDSUsrKyNgtfKaWOhyaNk1WeA2XZVldTULS1zlNNmVU8JmerdUxUHythwOH/KqVUJ9R9ksbnj8OhTaf2OeMGwZhbrTGJqJTDg9lBUVaXVEU+j//pn/RO6cv991sL+j711FP4+fmxaNEiCgsLqa2t5emnn2b69OnHfLmysjKmT5/e4nmzZs06XBfjtNN48803yc7O5t5777VqaAAvvfQSkyZNOrXXQCnVrXSfpOELtRXg9LcWC2w++ykkDkLiuP6mW3jkkUcak8Z7773Hl19+yUMPPUR4eDh5eXlMnDiRadOmIceYQRUYGMhHH310xHlbt27l6aefPlwXw1548KGHHuKss87io48+or6+Xru9lFInrfskjUv+ePLP4XZDZb61wixA1iarW8rhbPWU0aNHk5OTw8GDB8nNzSUqKoqePXvy6KOP8s033+BwOMjMzCQ7O5uePXse9eWNMfzqV7864ryFCxc2rYsRbS1euHDhQmbNmgWA0+kkIiLi5K+BUqpb6z5J41SoLLQWFBSntdAgbmv67DFce+21zJkzh0OHDnH99dcze/ZscnNzWbNmDS6Xi5SUFKqqqo75PCd6nlJKnSo6Kns8PNeMalhg0D+k9eNt119/Pe+88w5z5szh2muvpbi4mPj4eFwuF4sWLWL//v1evXxr57VWF+O8887jpZdeAqC+vp7i4uLjebdKKXUETRreMsaaRgtW8qgus+6x8GKV2WHDhlFaWkpiYiK9evXi5ptvZvXq1YwYMYJZs2YxePBgr0Jo7bxhw4Y11sUYOXIkjz32GGDV0Fi0aBEjRozg9NNPZ+vWrSf23pVSyqb1NLxVUw55O62ZUZWF1ragaGs6bRen9TSU6hhW7yvgtKRI/P1O3ff9462noS0NbzW0MsISDt9r4cV4hlJKnQppOaVcM3MZH67NaNc4dCDcW9Ul1nIffv4QEA5VRV6NZ5yITZs2ceuttzbZFhAQwIoVK3zyekqpjm/FXmuscmtWSbvG0eWThjHmmPc/HJO73uqeCrXre4f2sJKHX8DJB9iCESNGsH79ep889/HqKt2XSnV2a/ZZ3eI7DpW2axxdunsqMDCQ/Pz8k//gq7Ontbrsut0NxZS6+HLmxhjy8/MJDNRaH0q1t9X7raSxM7u0Xb/MdemWRlJSEhkZGeTm5p7cE9XYFfYKnODMOjXBdRKBgYEkJSW1dxhKdXpZxZU8//UufnP5MIL8W78huCU5pVUcKKggKSqIjMJKcsuqiQ+zvsw9++UOyqrreGraMF+EfYQunTRcLhepqakn/0SLnoFv/gxPHPJZl5RSquPZk1tGSkwIDsfJ9yp8sv4gb69M55LhvThzYNxxndvQNXXj+GT+/OUOdh4qIz4skJo6N7NX7Gdy/9iTjs9bXbp76pTJT7PWl9KEoVS3sWJPPuf+ZQmzVx44Jc+32v7g33zw+G+yXb2/EH8/B1ePSQSsLiqARTtyKKyo5Qdj2q43QJOGN/J2QUz/9o5CKdVGjDH86csdAMxaus/rMYTiylpeXJTG1D8t5IG31jZ5vrUHrKSxJbOkyfYP1mSwK/vw4LbbfeRrrd5fyMikCHpFBBET4t+YND5cm0FsaABTB2hLo+MwBvJ3Q8yA9o5EKdVGFm7PYc3+QsanRrMrp4yV9nRXgHq3aTGJZJdUcc1LS/nzlzuornXzxeZDFFfWArAnr5yC8hpcTmGL3dJwuw1Pzd3CT97fwI9nr6W23s1nG7MY/8zXbD90OLFkFFawMaOIM/pZiWFgjzB2ZJdSWF7Dwu05XDkqAT9n232Ua9I4ltIsqC2HWG1pKNVd/HX+TvrEBPPqbWMJC/Rj9orDXVQPvb2Oa2Yuo7LmcKXOjRlFXPfyMjKLKnnr7gm8dMsY6tyGxTtygMNjEpeN6MW+/ApKqmr5vy+288ay/UwdEEtaThnPf72LJz7eRF5ZNc/arRyAd1amA3DdWKsLalDPMHYeKuUP87ZRW2+4ug27pkCTxrHl7bL+q91TSnUoNXVunzxvTmkVWw6WcPOEZMIDXfxgTBKfb84iu6SKvXnlfLYpizX7C/nZnA2s2V/I/W+tZdrfv6esqo7/3D2BSf1jGdU7ithQf+ZvzQZg9f4CIoNdTBuVAMCy3fm8vnQfV49OZNad4zmjbwwvLEyjsqae68YmsWBbDmv2F1BT5+adVemcOyiepChryv+AHqGU19TzwdoMbp+UwtCEcJ9ch9Z06dlTJ6XoAFSVWIPgoN1TSnUgC7dnc//sdcy89XSm9I/lqblbiA0N4OHzT/7f6dr9RQCc3seqS3Pn5FRmr9jPX77aQXigCz+HcMfkFP757V4+3ZhFiL+TB8/tz4wz+xIWaC1g6nQI5w3uwbxNWdTUuVm9v5DTk6MYnmjVtPnj59uprnNz99S+iAi/vnwo18xcys8uGsT143qzcHsuv/54C1MHxpJXVs3NE5Mb47t0eC/251dw1ehEhvRq24QBmjRaVlcNb0yD0kPQ5wzrpr7whPaOSill+2BtJpW19Tz41lqmDozjs41ZBPs7+dFZfQl0Hd89EM2tO1CIv9PB8ETrAzk5JpjbJ6Xw6nd7CXY5uWhYT3516RDiwgIICfBj+qhEQgOO/Cg9f2gP3l2dzs/mbGBPbjnXnJ5EfFgg8WEB7M0rZ3RyZGMrYWhCOGt/fUFj7L+9YiiPf7CRrUtKSIoK4qyB8Y3PGxXiz68ubb8FRLV7qoExUJ5nPV4xEwr3WqVcdy+EmH5d/u5vpTqLmjo3S3bkcvagOPycDj7bmMXUAbFU1NTz7a68xuOMMezMLmVzZjG5pdUtPldlTT3zNmU1Gdhee6CQYYnhBPgdTj4PnDuAqGB/ymvquXlCMiLCjDP7cfOEPi0mDIAp/WMJ8Xcyd8NBzh0cz/VjewM0tjZumdB0hWzPZHfFyATW//ZC3r/3DN68awLOU3CfyKmiLY2yXPjmT7B9HpRkQN+zIWMNDLwYJj8Mr18OsYPaO0qllG35nnzKquu4ZUIfel4YyPr0Iq4b25uxT8/ni82HuGCotUbc3A0Hefid9QDEhPiz8onzcTqEv83fybiUaKYMiOVvC3byyjd7mHnL6Vw8vCc1dW42ZhRzy8SmH+gRQS5+P304C7Zlc0a/GK/iDPJ38tH9kwlyOekdHdy4feqAWHZml3LZab2Oer7L6WBcSvRxXJm2oUnDFQgb34U+U2Dk9bB2FtRXw4VPQ+wAuP1T7ZpS6iS53YaP12dyyfBex72ERnPzt2YT5HIyZUAsgS5n4zf384f2YMG2bGrr3bicDj5el0liZBCXj+zFy0v2sDO7lPiwAJ77ehdRwS7evGsCbyzdB8DMJbu5aFgPtmWVUF3n5vQ+UUe87mWn9TrmB31zA3uEHbHtjsmp3HZGyim5y7w9aPdUQBj8NA1ufAvO+w08vBEeXGslDIA+kyAqpV1DVKqz+3LLIR57bwNzN2R6fc6fvtjOJc99S0F5TeM2YwwLtmUz1U4Yni4e1pPiylqW78mnuKKWb3flcdlpvbhpvDWIvPZAIavsqa+FFbVcO3MZbmO4Z2oq69OLWLWvkDX2ooBjko9MGqdSZ00YoEnD4ud/+LF/MET2br9YlOqCGu5z2Hrw6LUgGu6GfnfVAf6xeDfbskq4b/Yaauut6bVfb8shq7iKC4f1POLcMwfGERrgx0uLd/PFlizq3IbLRvQiOTqYmBB/1uwvZPW+Avz9HPzkgoFU1tZz4/hkHrtgENEh/jzx0SbeWLaPhIhAekboys6t8Wn3lIhcDDwHOIFXjTF/bLa/D/AaEAcUALcYYzLsffXAJvvQA8aYab6MVanuYOaS3YQG+B3RZ+9Le/PK+S7NGqA+WgGhPbllXP7CdwT4OSitqmPqgFiuHJXIT97fwC/mbOS304bx1H+3MCA+lGkjj+wyDnQ5eeKyIfzyw01szCgmKSqI05IiEBFGJ0ex7kAR4YF+jOodyX3n9CclNoRzBscT5O/kgXP689zXu0iKCuKGKfql8Wh8ljRExAm8CFwAZACrRGSuMWarx2HPArOMMW+IyLnA/wINJesqjTGjfBWfUt1NeXUdf5u/k97RwW2aNGYv34+fQzhvSDxL0/JbLYz294VpGAMXD7fGDR6/eDARwS4OFlXyl/k7Wbwzl4LyGt6ZMbHVGtk3jOvN92l5fLoxq3GWE8CYPpEs2JaNQ+C+s/vjdAhXeCSeO6ekcueUU7Aidjfgy5bGeCDNGLMHQETeAaYDnkljKPCY/XgR8LEP41GqW1u4PYfqOjd7csuorKk/6QFpb1TX1TNnbQYXDevJ5P6xfLklm4zCysbZRA3dTpmFlXy8PpO7pqTyxGVDmzzHg+cNoE9sCD+fs4HrxiYxsW/rs5dEhGeuHkFUsD+3nnE4MZ5uj1G4DYxN8e14RVfny6SRCKR7/J4BTGh2zAbgaqwurKuAMBGJMcbkA4EishqoA/5ojPm4+QuIyAxgBkBycnLz3UopD59ttAqIuQ3syC5lVO9Ir881xlBWXdd4x3NLquvqWbO/kDP6xjR+w1+4LYeiilquG9eb8EDr42ZrVklj0rjttZVszSqhZ3ggLqeDe87s2+JzTxuZwFn2mMWxhAe6+P2Vw5tsOy0pEj+H4DamxZlRynvtPRD+U+AsEVkHnAVkAg2rgPUxxowFbgL+n4j0a36yMeYVY8xYY8zYuLjjK2qiVHdSVl3Hoh05nDvYurP4WAPSntYeKOT6l5cz6nfzee27va0uE/72igPc9M8VLLIX6QPrzu34sACm9I9lUM8wRGCbPa6xal8BS3fnkxQVRFpOGbdPSmmsRteSiCDXCd/kFuTvZFhiBMMTI46a+NSx+bKlkQl4jigl2dsaGWMOYrU0EJFQ4AfGmCJ7X6b93z0ishgYDez2YbxKdVlfb8umus7Nj87sy6p9BY3Lcx/Ngq3Z/GNxGmsPFBEbGsC4lCh+9+lWdmaX8sxVI46YNrpwh1VW+c9f7uTsgfEUVtSweEcOd01JxekQgv39SI0JaUwaLy/ZTVSwi/d/NAm3MQSd5PIfx/LCDaMxtF9t7a7Cl0ljFTBARFKxksUNWK2GRiISCxQYY9zAL7FmUiEiUUCFMabaPmYy8CcfxqpUl/bxukx6hAcwLiWaob3CjzqLCax61jPeXE1ydDBPXjaEG8cnE+Ry8pf5O3hx0W4C/BzcPbUvr32/l8tP68XQXhEs35NPaqyVFD5en8mhkirq3E2X7h7SK5xNmcXsOFTKgm05PHL+gDYZWwFrDSl18nyWNIwxdSLyAPAl1pTb14wxW0Tkd8BqY8xc4Gzgf0XEAN8A99unDwFeFhE3VhfaH5vNulJKtcAYw7Uzl3H1mCRummCN8x0qrmLJzlx+fHY/HA5haEI476xMp95tWu3umbM6A7eBWXdOaPJh+9MLB1Fbb3jlmz28uXw/bgNL0/L5xSWDqKlz89srhvLHz7fz2HsbABieGM6gnofvih7SK4zPNmVx6fPfEuRy8sMzUnx3MZRP+PQ+DWPMPGBes22/8Xg8B5jTwnlLgRG+jE2priizqJLV+wvZcaiUC4f1IDY0gDlr0nEbuM5eMG9YQgSVtfvYm1dO//jQI57D7Ta8uzqdSf1ijvh2LiL88pLBiEBpVR0pMcE8M287f/hsG4EuBxP7xvDX60bx5ZZDJEUFMal/0zKkFw/vyYq9BYxMiuSKkQlEh/ijOhdde0qpTm5vXjn78so5Z3A8Ow5ZtaNL7Xsyfj99OO+uTueMvjH0iQkBYKhdg2FrVkmLSWPZnnwyCiv52UUtL9RpJQ5rae66ejdvr0xnd2455wyKI9DlZGhCeKuFgfrHh/HmXc0nUarOpL1nTymlTtJf5+/kR/9ZQ3VdPdvtpPGDMUm8vfIA0178jvSCSq4fd3hOSv/4UAJdDr63lxHfllXCo++up6jCWuPpP8v3ExHk4qIWlupozs/p4P5zrKqWZw+KP8bRqivQloZSndyWzGJq6txszixhx6FSEiOD+PXlQzDGkFtWzfRRCVw8/HAC8Pdz8IMxSby/JoOfXDSQ336yhZX7CiirruPy03rx+eZDPHzeAK+LGV01OhGw6l+rrk+ThlKdWFl1HXvyygFYs7+AHYdKGdQzjMhgf/56/ahWz7tnal/eWnmAB2avY+W+AsalRDF/azYLt+cwtk8UD57b3+sYnA7hmtOTjn2g6hK0e0qpTmybx9TZ5XsK2J1b1mS2UmtSYkO4ZHhPVu4roHd0EP+5ewIXDO1BRJCL524cjZ9TPxpUy7SloVQHVFlTz8LtOfQID2BQz7BW72LenGndpDelfyxLduZS7zYM9iJpANx7Vj++3JLNTy8cRICfk5dvOZ2K2nqvlupQ3Zf+dSjVBipr6gnwc3hdfOfvi3bx4iJrAYTe0UF887NzWlwZdnNmCbGhAVw6olfj8uPetDTAWo9p7a8vICLISkgOh2jCUMekbVClfKiu3s0/Fqcx8n++4s3l+706p6q2nrdWHODMgXHcf04/0gsqScspa/HYLQeLGZ4Y3rhyq59D6Bt75DTa1jQkDKW8pUlDKR966J11/OmLHdS63axPLwLgQH4FT3+6tXFZcLBqXVw7cynPf72LT9ZnUlhRy71n9eWGcdZd3d/brQhPVbX17MopY1hCOP3jQgkP9KNfXGirtSaUOhW0LaqUj+w4VMq8TYe47+x+bMgoYq89y2nuhkxe/W4vUwfGcdZAa3Xm/7dgJ6v2WTWsg1xOBvcMa1xivE9MMN+l5XPD+GSuf3kZt01K4eoxSew4VEq92zA8IQKHQ7j37H66gqvyOf1KopSPvL50LwF+Du6Z2peUmBD25VtJY5fd1fTFZqu+xbasEl77fh83jOvNnZNTqayt564pqY1jGJP6xbJiTz7/Wb6fDRnFjfW2V+0rAGB4YgRgVaS7tQ0r8qnuSVsaSvlAYXkNH67N5KrRiUSF+JMaG0JRRS2F5TXsyraSxldbsvn9dDdPfryZiCAXj18ymIggFzdNSKZfXEjjc03uH8PbKw/w7Fc7cDqENfsLySmp4uP1mQxLCG8saKRUW9CWhlI+8O7qdKrr3Nw+OQWAFHvdpz15ZezOLaN3dBD55TX84oNNrNlfyC8vGUxksD8iQv/40CYzpSb1sxb9q6p18+Rl1ppPLyxMY3NmCT8YozfVqbalSUOp4/T3hbu45qWlRz3mqy2HGJkUweCe1sJ9KbFW0vhmZx7VdW7umJSKv5+DD9ZmMD41+qh3VEeH+DM6OZJxKVHcPimFfnEhvLl8P06HMG1Uwql7Y0p5QZOGUsdpxd4CVu8vJL+susX9xZW1rE8vahzkButeC4fA/K3ZAIzsHcmZA2LxcwhPXzm8xXswPL1+x3j+fcd4RKRxHamzB8YRGxpwit6VUt7RpKHUcTpQUAHAxoyWS6Yu252H28BUj6QR4OckITKosWJe//hQnpo2jP/cPYGBPY59M15EkKvxxrvLT0vA6RBuHJ98sm9FqeOmSUOp41BX7yazsBKADRlFGGP45zd7+GLzIYyx6k9/syuP0AA/RvWObHJuqt1F1SM8gIggF0lRwUzsG3PcMQzpFc7qJ87n/KE9Tu7NKHUCdPaUUschq9iqew2wIb2I7YdK+cO8bQCMSY7kz9eO5JuduZzRLwZXs0X/UmJC+HZXHgPivVvm42iitOKdaifa0lDqODR0TfWODmJDRjEfr8/EzyH85vKh7MuvYNoL35FRWMmZA2KPOLdhMLylanlKdRaaNJQ6Dg1J44rTEigor2lcI+rOKanMfWAyfWJCcAic6TGe0SA11rqfYkAPTRqq89KkoVQLVu4t4MG31zHxma+ZveLwQoMHCirwc0hjKdTSqjqm29Nek6KC+fC+SXz+8JmN9bg9nd4nmvOH9OAcLYuqOjEd01Cqmaraeu6ZtRqnQ4gIcvHMZ9s4d3A8vSKCOFBQQVJUEEMTwvH3c+AQOH/I4QHpQJez1aXJI4JcvHrb2LZ6G0r5hLY0lGrmvxsOUlxZy4s3jeGNO8ZT5zb87r9bAWuF2uSYEFxOBxcN68kN45IJ0RoUqhvRpKG6HGMMv/zQWp6juXp75lODoooa7n1zDZlFlY3b/rPiAP3jQ5nYN5rkmGAeOm8An28+xLe7cjlQUEFydBAAL9w4mqemDfPtm1Gqg9Gkobqcg8VVvL3yAHPXZzbZ/uyXOxjwxDwm/3Ehs5btA2DBthy+2HKIt+xxi82ZxWxIL+LmCcmNd2nfPTWVxMgg/ue/WymurCVZFwhU3ZgmDdXl7MouBWB3bnmT7Qu359AnJoRAl4NXvtmDMYYVe/IBmLvhIMYYXvt+L4EuB1d7LAQY4Ofk0QsGNlbPS44+cpBbqe7Cq6QhIh+KyGUioklGdXgNH+67cw+XSK2oqWP7oRKuOK0Xd0xOJaOwkt255azYW0Cwv5P0gkreX5PBx+syuWVCnyPKoF41OrHx/gptaajuzNsk8A/gJmCXiPxRRAb5MCalTkpDvYqs4irKqusA2JRRjNvAqOTIxoUE3111gAMFFcw4sy/+fg5+9eEmglxO7jun/xHP6XQIT10xjHEpUfSN05aG6r68ShrGmAXGmJuBMcA+YIGILBWRO0RE60uqDmVXTikOe9HYvXYX1Tq7Pveo3lH0jg6mX1wIbyyzxjHOH9KD8wbHU+c23D21L9GtLNExZUAs7987iUCX0+fvQamOyuvuJhGJAW4H7gbWAc9hJZH5PolMqRNgjGFXThnjU6MBSMu1xjfWHyiiT0xwY0I4e1A8NXVuwgL9GNIrnLumpDJ1QCx3T01tt9iV6gy8HdP4CPgWCAauMMZMM8a8a4x5ENA1EVSHkVNaTWlVHecP6YHTIezOaWhpFDZZdfbsQVYX1biUaJwOYWxKNG/eNYGwQG04K3U03rY0njfGDDXG/K8xJstzhzFGb3FVba623s2cNRnU1LmbbG8YzxjaK5w+0cHszi0jq7iS7JJqRnskjXEp0SRFBXGxvRyIUso73iaNoSIS2fCLiESJyH2+CUl1Z//dcJCckqoW9zXUqwB4b3U6P31/A3M3HGxyzK4cqzuqf49Q+saFsju3jJV7CwAYlRzVeFygy8l3vziX68b1PtVvQakuzdukcY8xpqjhF2NMIXCPTyJS3VZxZS0Pvr2O//l06xH7jDHcM2s1P3t/A/Vuq/ARwJKduU2O25VTRkSQi7jQAPrFh7A3r5z/nbed1NgQhiWEt8n7UKor83bRHKeIiLG/6omIE9AqMOqUyiq2lvL4fFMW+/PLm6wUu/ZAIQu25QBQWFHLvvwKEiIC+XZXLvVug9OeLrUru5QB8aGICP3iQqmtNxSU1/DhbZOOKIqklDp+3v4r+gJ4V0TOE5HzgLftbUqdMlnFVreU28Cr3+5tsu+17/cRFujH1AGxLNiWTUpMML+4ZDBFFbVsyCgCoLiilnUHiji9j9UNNSIxAoAnLhvCcPuxUurkeNvS+AXwI+DH9u/zgVd9EpHqtrKKrKQxdUAs761OJyEyiH5xISTHBPPF5kPcOTmFe8/qx4w313DP1FQm9o3BIbB4Ry5jkqP4cush6tyGy07rBVi1tFf86jx6hAe259tSqkvxKmkYY9zAS/aPUj6RVVyJQ+CpacO449+r+L8vtjfucwj88IwUYkID+ODHkxq3j+odyZKduTx2wUA+25hF7+igxhYGoAlDqVPMq6QhIgOA/wWGAo3/Co0xfY9x3sVYNwE6gVeNMX9str8P8BoQBxQAtxhjMux9twFP2oc+bYx5w5tYVeeVVVxFfFgg/eJC+ebn51BaVcuOQ6V8uyuP2FB/erew5tPZg+L524KdvLc6ne/T8rhramrj6rRKqVPP2+6pfwO/Bf4GnAPcwTHGQ+zB8heBC4AMYJWIzDXGeE6NeRaYZYx5Q0TOxUpMt4pItP16YwEDrLHPPbJAgurwKmrqcDqEAL+jL7+RVVxJr8jDLYOwQBdjU6IZmxLd6jm3T05h0Y4cfj5nIwCXj0g4NUErpVrk7UB4kDHma0CMMfuNMU8Blx3jnPFAmjFmjzGmBngHmN7smKHAQvvxIo/9FwHzjTEFdqKYD1zsZayqg7n51RU89u6GYx6XVVxFQkTQcT13eKCLN++awNQBsYxIjGB4ok6rVcqXvG1pVNvLou8SkQeATI69fEgikO7xewYwodkxG4CrsbqwrgLC7DWuWjo3sfkLiMgMYAZAcnKyl29FtaXaejebMqzCRplFlSRGtpwUjDFkFVVxzqD4436N0AA/3rxrAvVuo11TSvmYty2Nh7HWnXoIOB24BbjtFLz+T4GzRGQdcBZWMqr39mRjzCvGmLHGmLFxcXGnIBx1onJKq6itdx+xfX9+OXVug9vAOysPtHp+cWUtlbX19Io48YHrhns1lFK+c8ykYY9NXG+MKTPGZBhj7jDG/MAYs/wYp2YCnms0JNnbGhljDhpjrjbGjAaesLcVeXOu6jjKqus458+LuXbmMrKbLQHSsBZUUlQQ76xKb0wsC7dnc/5fl1BRY9W7aLhHI6GVlohSqmM4ZtIwxtQDU07guVcBA0QkVUT8gRuAuZ4HiEisRzXAX2LNpAL4ErjQXuMqCrjQ3qY6oL255ZTX1LM+vYgrXviO9IKKxn27csoQgV9eMoTc0moWbM0GYNH2XNJyyli7vwg4fDd4z5NoaSilfM/b7ql1IjJXRG4Vkasbfo52gjGmDngA68N+G/CeMWaLiPxORKbZh50N7BCRnUAP4A/2uQXA77ESzyrgd/Y21QHtybNaE8/dMIrSqjr+9/Ntjft25ZRZq8kO70lYoB/fpeUBsDWrBICV+6z/rY0tjeMcCFdKtS1vB8IDgXzgXI9tBvjwaCcZY+YB85pt+43H4znAnFbOfY3DLQ/Vge3JLUcELhrWk315FfxtwU5W7i1gfGq0vRZUGE6HcFpSBBsyinC7DdvtpLHKXoE2q6gKp0OICwtoz7eilDoGb8u93tHCz52+Dk51DnvyykmMDCLQ5WTGmX3pFRHI7z7dQm29mz155QyItybajUyKZHtWKWm5ZZTX1BMV7GJdeiG19W6yiqvoERagg9lKdXDe3hH+b6yWRROaOBTA3rwy+sZZiSHI38nPLx7Eo+9uYObi3dTUuenXkDR6R1LnNsxZkwHA9eOSmblkN5szi+0b+7RrSqmOztsxjU+Bz+yfr4FwoMxXQanOwxjD3txy+sYeXsZ8+shEBvcM47mvdwE0tjQayq1+sCYDh8DNE6x7a15esofV+wvpH6eVg5Xq6LztnvrA42c2cB3WEh+qm8spraa8pp6+cYeThsMh/OTCQdS5rcZpfztp9AgPpGd4IPnlNfSNC6V3dDCpsSF8seUQydHBPH7J4HZ5D0op751oVZoBwPHfuqu6nN25VoMz1aOlAXD+kHjGJEfSOzqIsEBX4/aRva0VaIf0spb7uGR4T/rGhvDmXeOJCtG6Xkp1dN6OaZTSdEzjEFaNDdXN7c0rB2gc02ggIvzzh2Mpqaprsn1k70i+3JLNkF5hAPz0wkH87KJBuvyHUp2Et/U0wnwdiOqc9uSWE+hy0KuFuhUxoQHEhDadQjveXrG2YXzDobOllOpUvOqeEpGrRCTC4/dIEbnSZ1GpTmNvXjkpMSFef/iPTYnmy0fO5Iy+MT6OTCnlC96OafzWGFPc8Iu9PtRvfRKR6nCMOWK2daOd2aX0O85ZT4N6hml3lFKdlLdJo6XjvL2bXHVi1XX1TP7jQmav2H/EvtzSajIKKxsHt5VSXZ+3SWO1iPxVRPrZP38F1vgyMNUxHCyq4mBxFc9+uYPSqtom+9YesAopjkmOao/QlFLtwNuk8SBQA7yLVYGvCrjfV0GpjiOryFp9trCille/3dtk39oDhbicwvBEbWko1V14O3uqHHjcx7GodlZRU8eWgyWM86jJfdBefXZ4YjivfruHaaMSGscw1u0vYlhCBIGuo9f+Vkp1Hd7OnpovIpEev0eJiNa36GKe/zqN615eRk7p4UJKB+2Wxp+vGYmf08Flz3/LrGX7qK13szGzSLumlOpmvO2eirVnTAFgjClE7wjv1Mqr66iqPVxZ1+02zF2fiTGwObNxohxZxZXEhvozpFc4Xz16JhP7xvCbT7bwm0+2UFXrZkyfyHaIXinVXrxNGm4RSW74RURSaGHVW9V53PjP5fzmk82Nv6/eX9jYFbUpo6Rxe2ZRFb3swkg9wgP5123jOGtgHG/b9b5P76MtDaW6E2+nzT4BfCciSwABpgIzfBaV8qmaOjdbDpaQWViJ221wOIRP1mcS5HISE+rPJs+WRlFlk8UInQ7h+RtHc9U/vqemzt2YUJRS3YO3A+FfiMhYrESxDvgYqPRhXMqHDhSUU+825JfXsP1QKQN6hDJvUxYXDO2BQ2D5HquanjGGg0WVTO4f2+T8iCAXc+6dRFmzdaWUUl2ftwsW3g08DCQB64GJwDKaln9VnURazuFSKEt357E/v5zCilqmj0pgb145H68/SG5pNf5+Dspr6klsoThSdIg/0boqrVLdjrfdUw8D44DlxphzRGQw8IzvwlK+tDvXWpk2MTKI79LyKKyopU9MMGcPiic0wGplbM4spmeEtQhhr8gjFyNUSnVP3iaNKmNMlYggIgHGmO0iMsinkSmf2Z1TRkJEIOcOjmf2iv24DTx95XCcDmFYYgQisCmzGGPPdUjQMqxKKZu3s6cy7Ps0Pgbmi8gnwJGLEakO5fXv95JeUNH4e02dG7AKJ/WLD2Vy/1jcBmJC/Lnm9CQAQgP8SI0NYVNmMQeLrNlUCTrYrZSyeVvu9SpjTJEx5ing18C/gCt9GJc6Sfll1Tz13608+bE1rXbmkt2MfXo+BeU17M4tp19cKGf0jSHE38ndU/s2uat7THIUy3bnsz69CD+HEBcW0NrLKKW6meNeqdYYs8QXgahTq7CiBoAlO3N5b3U6f52/k5o6Ny8uSqOsuo5+cSFEBLv4/vFziQhyNTn3/nP6M3fDQeasySAxMginFkpSStlOtEa46uAKyq0VaUXg53M24u90MKRXOG8s3QfQuH5UZLD/EbUtUmND+PFZ/QBanDmllOq+NGl0UQXlVkvj5gnWjfyPXTCQe8/qS53bGtzuF3/0wkk/PrsfA3uEMjQh3LeBKqU6FS2k1EU1dE/dd3Z/bhiXzLCEcGrq3cSE+FNd5yb+GOMUgS4nnz00FT/tmlJKedCk0UU1tDSiQ/wbp8wG+Dn5xcWDSS+s8KrcqsupDVGlVFOaNLqowvIaglzOI2pdXDeudztFpJTqCvSrZBdVWFGry3wopU45TRpdVGFFDVEhrmMfqJRSx0GTRhfhdjctb1JQXkNUsLY0lFKnliaNLuCDNRmM+8MCiuwZU2C1NLR7Sil1qmnS6AIW7sghv7yGOWsyGrdpS0Mp5QuaNLqAtfsLAXhrxQGMMdTWuymtqtOkoZQ65TRpdHJZxZVkFVcxsncke/LKWbo7v/HGvmgdCFdKnWKaNDq4tJxSnpm3jeLK2hb3r91fBMATlw4hMtjF7BX7KbTXnYrSMQ2l1CmmSaODe/7rNF75Zg9Xvfg9u3PLjti/9kAhAX4ORvWO5NIRvfhmZx75ZdUARGv3lFLqFPNp0hCRi0Vkh4ikicjjLexPFpFFIrJORDaKyKX29hQRqRSR9fbPTF/G2VFV1dbz9bZsJqRGU1xZy0NvrzvimLUHCjktKQJ/PwenJ0dRVl3Hqn3WGIe2NJRSp5rPlhERESfwInABkAGsEpG5xpitHoc9CbxnjHlJRIYC84AUe99uY8woX8XXGSzekUt5TT0PnjuA73fn8eq3e6ird+PndPDxukwCXQ62ZJZwx+QUAEYnRwKwcHs2gE65VUqdcr5ce2o8kGaM2QMgIu8A0wHPpGGAhrW3I4CDPoyn0/lsUxbRIf5M7BvNwaJKausNmUWVuJwOHnl3feNxo5OjAKsORkSQiw0ZxQBEButAuFLq1PJl0kgE0j1+zwAmNDvmKeArEXkQCAHO99iXKiLrgBLgSWPMt81fQERmADMAkpOTT13kHUBljdU1NX1UIn5OB6lxIQDszSun4d7vJy8bQoDLyXlD4gEQEUb1jmTJzlxC/J0E+DlbeXallDox7T0QfiPwujEmCbgUeFNEHEAWkGyMGQ08BrwlIkdUAzLGvGKMGWuMGRsXF9emgfvagm3ZVNTUc/lpvQCrFQFW0th5qBSAa05P4taJfZosYT6qdySg4xlKKd/wZUsjE/BchzvJ3ubpLuBiAGPMMhEJBGKNMTlAtb19jYjsBgYCq30Yb7urrXfjFMHhEN5bnU5iZBBn9I0BICbEn7AAP/bmlVNWXUeP8AAiW5gd1TCuoeMZSilf8GVLYxUwQERSRcQfuAGY2+yYA8B5ACIyBAgEckUkzh5IR0T6AgOAPT6MtUP4wUtL+dF/1pBeUMG3u/K4dmwSDrtynoiQGhditTSySxnYI6zF52hsaeh0W6WUD/gsaRhj6oAHgC+BbVizpLaIyO9EZJp92E+Ae0RkA/A2cLsxxgBnAhtFZD0wB7jXGFPgq1jb2sGiSq55aSmHiqsat5VW1bIxo5j5W7O58/VViMC1Y5sWTEqNDWF3Thm7sssY1ErSiAz2Z2TvSPofowa4UkqdCJ9W7jPGzMOaRuu57Tcej7cCk1s47wPgA1/G1p5W7Stg9f5CvtxyiNsmpQCwLcsap0iKCmJXThlTB8SSaJdpbZAaG8In660JZgN7tpw0AN770UT8HO09XKWU6or0k6UdZBRWAvBdWl7jts2Z1jTZf98+jguG9uCR8wcccV7DYDjQaksDrFrgTsexa4ArpdTx0hrh7aAhaSzfk994s97mg8XEhQUwoEcY//zh2BbP80waA3po95NSqu1pS6MdZBRWAFBaVcfmgyUAbD1YwvCEI2YVN5FiJ43k6GCC/TXfK6XaniaNdpBZVMmE1GgAvk/Lo6q2nl05ZQxPjDjqeeGBLuLCAlqdOaWUUr6mX1fbmDGGzMJKzhscT0lVHd+n5TGlfyz1bsOwY7Q0AJ67fhRxYQFtEKlSSh1JWxptLK+shuo6N4mRQUzpH8OqfQW8uCgNgGEJR29pAEzqH8sAbWkopdqJtjTaWGaRNQieFBXMpaf1YkN6MV9tzSYiyEVSVNAxzlZKqfalSaONNQyCJ0YFER8WyNszJvLWiv34+zkQ0WmySqmOTZNGG3lvVTr94kPJtKfbJtqtCqdDuPWMlHaMTCmlvKdJow28s/IAj3+4if7xoUzsG014oB/hgVrrQinV+WjS8LHV+wr49Seb6REeQFpOGUUVNSRFBbd3WEopdUJ09pSPvbAwjdjQAP77wBQiglzkldU0dk0ppVRno0nDh4wxbM4sZkr/WOLDA7nm9CQAnSWllOq0NGn4UHZJNfnlNY13et88IRmnQ+gXp+tGKaU6Jx3TOAV++NpKJqRGc/85/Ztsb1i5tuFO775xocx/9Ewd01BKdVra0jhJ+/LK+WZnLvM2ZR2xb/PBYkRgSK/Dy4P0jQvF308vu1Kqc9JPr5O0YFs2ANsPlVJZU99k35aDJfSNDSEkQBt0SqmuQZPGSfpqazZ+DqHebdhkd0c12JJZ7NV6Ukop1Vlo0jgJBeU1rN5XwHXjrFre6w4UNu7LL6vmYHEVwxOPvXKtUkp1Fpo0TsKi7Tm4DdwwrjfJ0cGsTy9q3LfFLq6kLQ2lVFeiSeMkLNmZS3xYACMSIxjVO5J1B4oa923Nakga2tJQSnUdmjROwr78cgb3CkdEGJ0cyaGSKrKKrQUJ03LKiA8LIDLYv52jVEqpU0eTxknIKKxsvLt7dHIUAOvt1sbu3DK9iU8p1eVo0jhB5dV1FJTXNCaNIb3CcDmFDRnFGGPYnVNGv/iQdo5SKaVOLU0aJ8izAh9AgJ+TgT3C2JxZTF5ZDSVVddrSUEp1OZo0TlBDBT7PxQdHJEawKbOYtJwyAE0aSqkuR5PGCcoobGhpHE4awxMjKK6sZfHOHAD6x2vSUEp1LZo0TlBGYSX+fg5iQwIat42wV7P97/qDBPs76Rke2F7hKaWUT2jSOEEZhRUkRQbhcEjjtkE9w/BzCAeLq+gbF9Jkn1JKdQW6kp6Xvt6WzZdbDrE7t5wXbxpDRmHlERX4Al3WYPjWrBIdz1BKdUmaNLyQXVLFXW+sJjzQj5KqOj5Ym0FGYWWLS4SMSIzQpKGU6rK0e8oL2w+VAvDKD8cyLiWK91anN7lHw9PwJCuR6CC4Uqor0qThhV3ZVtIY2COMaSMT2J9/5HTbBucNjmdSvxjGp0a3aYxKKdUWNGl4YWd2KbGh/kSH+HPpiF447QHulsq2JkQG8dY9E4kNDThin1JKdXaaNJqprXfzfVoes1fsZ8WefAB2Zpc1djfFhAYwdUAsAL1baGkopVRXpgPhzfz7+708M287ADEh/qz41Xmk5ZRx9ZjExmMeOKc/8WEBxIVpa0Ip1b1oS6OZvXnlRAW7+P30YeSX1zBv8yHKqusY0COs8ZixKdH86ZqRiOh9GEqp7kWTRjOZRVX0jg5m+uhEXE7hH4vSABios6GUUsq3SUNELhaRHSKSJiKPt7A/WUQWicg6EdkoIpd67Pulfd4OEbnIl3F6OlhUSUJEEOGBLib2jWmcbjvQo6WhlFLdlc+Shog4gReBS4ChwI0iMrTZYU8C7xljRgM3AP+wzx1q/z4MuBj4h/18PmWMsZJGpDXAfcHQHgDEhgYQFaIV+JRSypctjfFAmjFmjzGmBngHmN7sGAM0FNGOAA7aj6cD7xhjqo0xe4E0+/l8qriyloqaehIirYUGzxtiJY0B2jWllFKAb2dPJQLpHr9nABOaHfMU8JWIPAiEAOd7nLu82bmJNCMiM4AZAMnJyScdcENhpUS7pZEYGcQ1pycxxi7lqpRS3V17D4TfCLxujEkCLgXeFBGvYzLGvGKMGWuMGRsXF3fSwRwsqgJo7J4CePbakdw04eQTklJKdQW+bGlkAr09fk+yt3m6C2vMAmPMMhEJBGK9PPeUO2i3NDyThlJKqcN82dJYBQwQkVQR8cca2J7b7JgDwHkAIjIECARy7eNuEJEAEUkFBgArfRgrYCUNfz8HMTrorZRSLfJZS8MYUyciDwBfAk7gNWPMFhH5HbDaGDMX+AnwTxF5FGtQ/HZjjAG2iMh7wFagDrjfGFPvq1gbZBZVkhARqMWTlFKqFT5dRsQYMw+Y12zbbzwebwUmt3LuH4A/+DK+5jyn2yqllDpSew+Edwjzt2ZTUVNHVnEVvSI0aSilVGu6/YKFu3PLmPHmaq4clUh2SRWJ9j0aSimljtTtWxr94kJ58Jz+fLQuE7fRmVNKKXU03T5pADx03gDGp1iV9jRpKKVU6zRpAH5OB8/fOJrbJ6UwNkXv/lZKqdZ0+zGNBj0jAnlq2rD2DkMppTo0bWkopZTymiYNpZRSXtOkoZRSymuaNJRSSnlNk4ZSSimvadJQSinlNU0aSimlvKZJQymllNfEKl/R+YlILrD/JJ4iFsg7ReG0FY25bWjMbUNjbhvNY+5jjPG6XnaXSRonS0RWG2PGtnccx0Njbhsac9vQmNvGycas3VNKKaW8pklDKaWU1zRpHPZKewdwAjTmtqExtw2NuW2cVMw6pqGUUspr2tJQSinlNU0aSimlvNbtk4aIXCwiO0QkTUQeb+94WiIivUVkkYhsFZEtIvKwvf0pEckUkfX2z6XtHasnEdknIpvs2Fbb26JFZL6I7LL/22FKJYrIII9ruV5ESkTkkY52nUXkNRHJEZHNHttavK5ied7++94oImM6UMx/FpHtdlwfiUikvT1FRCo9rvfMDhRzq38LIvJL+zrvEJGLOlDM73rEu09E1tvbT+w6G2O67Q/gBHYDfQF/YAMwtL3jaiHOXsAY+3EYsBMYCjwF/LS94ztK3PuA2Gbb/gQ8bj9+HPi/9o7zKH8bh4A+He06A2cCY4DNx7quwKXA54AAE4EVHSjmCwE/+/H/ecSc4nlcB7vOLf4t2P8eNwABQKr9ueLsCDE32/8X4Dcnc527e0tjPJBmjNljjKkB3gGmt3NMRzDGZBlj1tqPS4FtQGL7RnXCpgNv2I/fAK5sv1CO6jxgtzHmZFYZ8AljzDdAQbPNrV3X6cAsY1kORIpIrzYJ1ENLMRtjvjLG1Nm/LgeS2jquo2nlOrdmOvCOMabaGLMXSMP6fGlTR4tZRAS4Dnj7ZF6juyeNRCDd4/cMOviHsYikAKOBFfamB+zm/WsdqavHZoCvRGSNiMywt/UwxmTZjw8BPdontGO6gab/uDrydYbWr2tn+Ru/E6tF1CBVRNaJyBIRmdpeQbWipb+FznCdpwLZxphdHtuO+zp396TRqYhIKPAB8IgxpgR4CegHjAKysJqeHckUY8wY4BLgfhE503OnsdrIHW7Ot4j4A9OA9+1NHf06N9FRr2trROQJoA6YbW/KApKNMaOBx4C3RCS8veJrplP9LTRzI02/CJ3Qde7uSSMT6O3xe5K9rcMRERdWwphtjPkQwBiTbYypN8a4gX/SDs3hozHGZNr/zQE+woovu6F7xP5vTvtF2KpLgLXGmGzo+NfZ1tp17dB/4yJyO3A5cLOd7LC7ePLtx2uwxgcGtluQHo7yt9DRr7MfcDXwbsO2E73O3T1prAIGiEiq/e3yBmBuO8d0BLsv8l/ANmPMXz22e/ZNXwVsbn5uexGREBEJa3iMNei5Gev63mYfdhvwSftEeFRNvpF15OvsobXrOhf4oT2LaiJQ7NGN1a5E5GLg58A0Y0yFx/Y4EXHaj/sCA4A97RNlU0f5W5gL3CAiASKSihXzyraO7yjOB7YbYzIaNpzwdW7r0f2O9oM1u2QnVpZ9or3jaSXGKVjdDRuB9fbPpcCbwCZ7+1ygV3vH6hFzX6zZJBuALQ3XFogBvgZ2AQuA6PaOtVncIUA+EOGxrUNdZ6yElgXUYvWd39XadcWaNfWi/fe9CRjbgWJOwxoHaPibnmkf+wP7b2Y9sBa4ogPF3OrfAvCEfZ13AJd0lJjt7a8D9zY79oSusy4jopRSymvdvXtKKaXUcdCkoZRSymuaNJRSSnlNk4ZSSimvadJQSinlNU0aSnUAInK2iHza3nEodSyaNJRSSnlNk4ZSx0FEbhGRlXb9gZdFxCkiZSLyN7FqnXwtInH2saNEZLlHvYiGGhf9RWSBiGwQkbUi0s9++lARmWPXmJhtrwSgVIeiSUMpL4nIEOB6YLIxZhRQD9yMdRf5amPMMGAJ8Fv7lFnAL4wxp2HdRdywfTbwojFmJDAJ6w5esFYvfgSrNkNfYLKP35JSx82vvQNQqhM5DzgdWGU3AoKwFgZ0c3ghuP8AH4pIBBBpjFlib38DeN9ejyvRGPMRgDGmCsB+vpXGXhvIrq6WAnzn83el1HHQpKGU9wR4wxjzyyYbRX7d7LgTXZun2uNxPfrvU3VA2j2llPe+Bq4RkXhorMvdB+vf0TX2MTcB3xljioFCj8I2twJLjFV5MUNErrSfI0BEgtvyTSh1MvSbjFJeMsZsFZEnsaoROrBWEr0fKAfG2/tysMY9wFqifKadFPYAd9jbbwVeFpHf2c9xbRu+DaVOiq5yq9RJEpEyY0xoe8ehVFvQ7imllFJe05aGUkopr2lLQymllNc0aSillPKaJg2llFJe06ShlFLKa5o0lFJKee3/A6AEpMS9aAD4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(train_accs)\n",
    "plt.plot(val_accs)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fab2db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCELDCINTASKIYMQMLLCPDPA\n",
      "LTEMVLRLPYQIKKIADTSSRIPPP\n",
      "KCLPIRGVDGNGKSPSKSELHRLYL\n",
      "IPSAKNLDADIWKKFLSRPALPFIL\n",
      "YTHIRNILIVLTKILPWYPKVLNLG\n",
      "PRSDISEPDREQKRRKIDSHPSPSH\n",
      "VTTPEKWDVVTRKSVGDVALSQIVK\n",
      "WGVNLPAHAVVIKGTQIYAAKRGSF\n",
      "SGYCLPEPFFSMKLKDWVQKLMMTL\n",
      "KTETMNVVMETNKMLREEKERLEQN\n",
      "AEGSEFWSALLEKAYAKINGCYEAL\n",
      "FALKKIKRSSIRKGMVMVSPRLNPQ\n",
      "GTCNPQFVVCQLKVKIYSSNSGPTR\n",
      "GAAAEHWRRREGKAPGLLGRVPLLP\n",
      "EEESQCINANRRKLQRELDEATESN\n",
      "YTGDALRICIVTKEGIREETVPLRK\n",
      "QTGNLLNWGRLRKKCPSTHSEELRD\n",
      "QTGNLLNWGRLRKKCPSTHSEELRD Q 316\n",
      "FVIFPPRWGVADKTFRPPYYHRNCM\n",
      "HRNCMSEFMGLIKGHYEAKQGGFLP\n",
      "VLKEVVRQRSDLKVIVMSATLDAGK\n",
      "RQLDRIIAKLQSKEYSQYOOOOOOO\n",
      "RLKEIICEQAAIKQATKDKKITTVO\n",
      "NAAFALAADISSKSPVAVQGSKINL\n",
      "PETLRIVLSNMEKLCSIPIHGIRSV\n",
      "OOOMPLRLDIKRKLTARSDRVKSVD\n",
      "CTPNKPSRTSMSKMFVKGAPEGVID\n",
      "ESKKAKKPAVVAKSSILLDVKPWDD\n",
      "NSPTLPIYEPGLKEVVESCRGKNLF\n",
      "ATFTVMGLKSTHKHOOOOOOOOOOO\n",
      "FRERQVVSHLQDKYVFITGCDSGFG\n",
      "TIVHESERLEALKHALHCTILASAG\n",
      "RFKDIFQEIYDKKYKSQFEAQKICY\n",
      "VCVISETGKAKYKAOOOOOOOOOOO\n",
      "ISMRTQLVSNLKKEGSSHNWQHITD\n",
      "HLVELLAVCTEGKNVYTEIKCNSLL\n",
      "FKLCPMNRYSAQKQFWKAAKPGANS\n",
      "SCDRIKQSASGTKRRVFIVETMGGY\n",
      "KVVAFSPVTELKKETDFEHRMPREQ\n",
      "DMSKNVSQSQMAKLNQQMAKMMDPR\n",
      "QRELDTVTLEDIKEHVRQLEKAVSG\n",
      "QRELDTVTLEDIKEHVRQLEKAVSG Q 339\n",
      "OOMATSASSHLNKGIKQMYMSLPQG\n",
      "ASIRIPRTVGQEKKGYFEDRRPSAN\n",
      "VFITLKKYDGRTKPIPRKSSVEGLE\n",
      "FELTCYSLAPQIKVIAPWRMPEFYN\n",
      "MEKAGAHLKGGAKRVIISAPSADAP\n",
      "PAALVGGINLLLKPNTSVQFMKLGM\n",
      "VDWIRQEDGSVLKSINRSLVVGQKI\n",
      "STVLQQQYNRVGKVEHGSVALPAIM\n",
      "QSKLAQENMDLFKEQWEKQVRVLTD\n",
      "QSKLAQENMDLFKEQWEKQVRVLTD Q 348\n",
      "KTVFAEHISDECKRRFYKNWHKSKK\n",
      "NGDNEKMTALEAKICHQIEYYFGDF\n",
      "KMSYKAAMGEEYKAGCPPGNPTAGR\n",
      "LYSRKEGQRQENKNKNRYKNILPFD\n",
      "GTTNSCVAVMEGKQAKVLENAEGAR\n",
      "GDTKNSTFSELFKKEHPDRFIECYI\n",
      "VLDDGELLVQQTKNSDRTPLVSVLL\n",
      "EAKEQHPDMSVTKVVVHQETEISEE\n",
      "LGSRTRAIGNAAKSQIVTNVRNTIH\n",
      "SVTLGAHQSIGFKGILLYGTKAQRE\n",
      "NKIDLENRQVATKRAQAWCYSKNNI\n",
      "HGISLFLVENGMKGFIKGRKLHKMG\n",
      "TFKIRMEPDETVKVLKEKIEAEKGR\n",
      "LDDAEKKLNLAQKCFKNCYGENHQR\n",
      "ATIEVFLPPRLRKHDKKSLLETRLH\n",
      "LDNPARIPPCGWKCSKCDMRENLWL\n",
      "VIYNDQKVCASEKPPKDLGYIYFYQ\n",
      "VARGVQKILQDYKSLQDIIAILGMD\n",
      "OOMADEELEALRKQRLAELQAKHGD\n",
      "KDSVVAGFQWATKEGALCEENMRGV\n",
      "FDLEAEEYVPLPKGDVHKKKEIIQD\n",
      "ITTRKTPCGEGSKTWDRFQMRIHKR\n",
      "KTTFIRMLAGRLKPDEGGEVPVLNV\n",
      "EIAAGKCRRPAVKQFHDSKIKFPLP\n",
      "QYVVRKPLNKEGKKPRTKAPKIQRL\n",
      "QYVVRKPLNKEGKKPRTKAPKIQRL Q 373\n",
      "TRKFMTNRLLQRKQMVIDVLHPGKA\n",
      "PRKVVGCSCVVVKDYGKESQAKDVI\n",
      "ESQAKDVIEEYFKCKKOOOOOOOOO\n",
      "RIEINFPAEYPFKPPKITFKTKIYH\n",
      "ICNDPDIIAENIKQVKLGSPDYIDC\n",
      "MPPLRDGARVLMKLMPPDSTTIEKL\n",
      "ISLAVAKERAHQKRSSKRAPQMDWS\n",
      "PSPQKIPKVAGPKEASATPPSKKTP\n",
      "VHPAAKTLVDIAKSQDAEVGDGTTS\n",
      "LHPAARMLVELSKAQDIEAGDGTTS\n",
      "KIAILTCPFEPPKPKTKHKLDVMSV\n",
      "DLCNTNLMQVLVKMICEPMSLGFNI\n",
      "PKSLKHSPEDTEKYSCFALFAKFGK\n",
      "KWQTMIEAHVDVKTTDGYLLRLFCV\n",
      "TPDIKDNIVAQLKQLYRILQTQESW\n",
      "EQRFKLFKIACEKHQHLYRLAMTGA\n",
      "GYLTAEQFDEWVKPKDMLGPKOOOO\n",
      "YSFAFPFLEDSVKVVKNKVSLYKKV\n",
      "RASTCLAGRAGRKEAGWECGGARSF\n",
      "TPEHAHRAARGVKITYEDLPAIITI\n",
      "VRVKRMGGGFGGKETRSTLISTAVA\n",
      "KRERKKQKEKERKERLKKEGKLLTK\n",
      "GAKYTKGDSNIQKRWKELSKEDLSP\n",
      "KTMYGKDLIKDLKSELSGNMEELIL\n",
      "EEKAPEVSPAQPKPGVAAPPEVAPA\n",
      "GHTREYPQYRDNKPRTEHVPSGPLR\n",
      "YVKPDRRHPLYQKHQEALEMISNSK\n",
      "SILLVWKFTYVHKYLLADNRHYTFY\n",
      "QTSAEVYRILRQKGLLDKFPLFTAV\n",
      "QTSAEVYRILRQKGLLDKFPLFTAV Q 402\n",
      "ESCQISHNPDIVKEKENKGIFSFFQ\n",
      "YKIVPPKSLEMAKDWESEAMGRKDD\n",
      "OOOOOOOOOMKLKEIDRTAMQAWSP\n",
      "QGYWAQSHTLRGKVYQRLIRDIPCR\n",
      "QGYWAQSHTLRGKVYQRLIRDIPCR Q 406\n",
      "RLQITTREDINSKQVAPAKADLEPE\n",
      "LFYGLKFRRRLNKHPPETFVDALER\n",
      "PPKRRASPSPPPKRRVSHSPPPKQR\n",
      "DRGIIHHVCDEHKEFKDVKLFYRFR\n",
      "SEGITASNGRDPKVPGGPLKPSWWA\n",
      "LEEALRKELNCMKITSTVMDHIGGD\n",
      "VAEIIAGLIRGSKHWTFEKVEKLWE\n",
      "IIGDLLHFQGSHKHEFDSRWKSFNL\n",
      "QSAMNTSIVRDTKTCHFTVALQLGI\n",
      "QSAMNTSIVRDTKTCHFTVALQLGI Q 415\n",
      "QVIRMFKEEVMTKLSHFRECINHGD\n",
      "QVIRMFKEEVMTKLSHFRECINHGD Q 416\n",
      "KCPGRYFAVNEMKLLLIMLLTYFDL\n",
      "VDADEIIVLSQGKVAERGTHYGLLA\n",
      "EEEEEKCKAEGNKLQVDNASLPQAD\n",
      "KASAAQPDSVGVKAKVLENFLTKSR\n",
      "TKSRTELLEYFVKVIFDYNTAHNKV\n",
      "QQQQMTSSYGGYKEPAAPVSIQRSA\n",
      "QQQQMTSSYGGYKEPAAPVSIQRSA Q 422\n",
      "VSIQRSAPGGGGKRYRAVYDYSAAD\n",
      "PTPVQKHAIPIIKEKRDLMACAQTG\n",
      "TGMERDLVAIEAKLSDLQKEAEKLE\n",
      "GRMRIHCLENVDKALQFLKEQRVHL\n",
      "GKDLTSVMRLLSKHRAFEDEMSGRS\n",
      "KRQIVAGTNLFIKVDVGGDKCVHLR\n",
      "LLLLLAESRSEGKSMFAGVPTMRES\n",
      "GEETLTPSAPVNKGPKPKREKKEPG\n",
      "EWKKHIENQKAWKIKYYKGLGTSTA\n",
      "EFPREGWVEQDPKEILQSVYECIEK\n",
      "LTVLCSEKQKQEKQSKAKKKKKGVV\n",
      "HCPSAVVPVELQKLARGSLVGGGRQ\n",
      "SVGYAIFKVLNEKKLQEVDSLWKEF\n",
      "EAEIEETYANFIKHNDGKNIFYAAR\n",
      "SENDELSQDVASKGLGLVYELGNEQ\n",
      "LESLLGAFESLGKAWPRNPDTQRCY\n",
      "YFQAPVKIAVIFKQLRVLIDSVLRK\n",
      "KPYVENIWALLLKHCECAEEGTRNV\n",
      "AKPAKAANKTPPKSPGDPARAAKRL\n",
      "LAAFKKGIIGGVKVHVDRLQTLISG\n",
      "PPTPAPDDLAQLKNLRSEEQKKKNR\n",
      "TNPSGASTECFVKDNADGTYQVEYT\n",
      "MPVWEDEGDDDAKKQTPKQRLLGWI\n",
      "KVIVLFAGQHISKSPFEVNVDKAQG\n",
      "KQFMKQHVFPAEKEVAEYYAQSGNS\n",
      "EAFEGYDSIDSPKAITSLKYMLLCK\n",
      "MSKVVDSLYSKAKKLTOOOOOOOOO\n",
      "PPDDTPYEWDLDKKAWFPKITEDFI\n",
      "SDDIEEEEEADEKECEDADDKEEDN\n",
      "IEVMRRSHEAREKLLRLGIFRQVDV\n",
      "EMLGSWSFGDYFKELACKMALELLT\n",
      "MSPQVNAVFPESKTDLVLIVVAVEH\n",
      "SPEGGKSLAQALKQNTTLTVIWLTK\n",
      "RQGLDEMYSDVWKQLKEEKKVRLEL\n",
      "QNLLPDTITEEEKERIYGILGQAVC\n",
      "QNLLPDTITEEEKERIYGILGQAVC Q 457\n",
      "SYAVSVWKRVKAKLEGRDVDPNRRM\n",
      "PRKRFHLLSRLRKAVKHAEELERLC\n",
      "DVQELITQVRSEKCSLQAAAILDAN\n",
      "HREQMKNDSREKKEAEDNLRREKNL\n",
      "PGHGRRYARTDGKVFQFLNAKCESA\n",
      "LFADTHIPGSPFKAHVAPCFDASKV\n",
      "DEEDDDKDGEVPKSAHEKSOOOOOO\n",
      "TQTQEITEDIPMKTLNMKTVYVSVL\n",
      "GRVIRDRKTIPIKYPLKEIVVIHQD\n",
      "DPEALEDIRSLEKYIIEELNVRKVT\n",
      "QVQRCVEILSRAKRPLLVLGSQALL\n",
      "QVQRCVEILSRAKRPLLVLGSQALL Q 468\n",
      "HFPDILPREYGGKEFSMEDICQEWT\n",
      "NFEIKEVVVTLMKKAERKGSPFHEL\n",
      "TVQAAERAASSVKDLAQKSFRLLMD\n",
      "EHWTPCILNEAEKIIYKLVKLDSLS\n",
      "LSFFEHFHISPVKLHLSLSLGSGGE\n",
      "GKQTHEKWQDILKEVKFLQQLKHPN\n",
      "PTTEEEYLHFGEKADSENQARKYMN\n",
      "LKTLWGDYYINMKAKKIMKVDQAKG\n",
      "GNRVPPTLPSSPKSGSGQSLPKESV\n",
      "FRVGLSGPPGAGKSTFIECFGKMLT\n",
      "HELVVQARDDFRKELDSITPDITPG\n",
      "QRWGKDSEFEGGKATVNCSTIPIAE\n",
      "QRWGKDSEFEGGKATVNCSTIPIAE Q 480\n",
      "OOOOMNTNDEKMKIISEDFTGDGVD\n",
      "IMKYAEKLIQEGKAYVDDTPAEQMK\n",
      "KVTEAVECLLSLKAEYKEKTGKDYV\n",
      "KSPGHMVILNQTKGDHCRPSRRGRY\n",
      "LFVPEVSFELLVKRQIKRLEEPSLR\n",
      "EQSSTVNAKKLEKAEARLKAKQEKR\n",
      "AKQEKRSEKETLKTSNPLVLEEASA\n",
      "PPIRPHSGIVRSKSECPSDDMGSLH\n",
      "SYFSLLTSEQVTKQFPVMTQAISQI\n",
      "GSQKYPCRDPFFKMLNRSLSTFMNA\n",
      "VNATPQQMPQAEKEVENFLRNVGRS\n",
      "TLQSFGKAVDWAKSGKFTQQDIDEA\n",
      "LEEGDLILTGTPKGVGPIKENDEIE\n",
      "KSSHPEDLRAANKLIKEMVQEDQKR\n",
      "LAQGMQSPDEVAKQPTVKVCLWGTG\n",
      "IGMLTTLVEAAEKQPSQATFKGLRK\n",
      "ITKQTLDELNQGKRTVQEVTEMDSV\n",
      "SFFQATVEHLVSKGIALQLLEAQAA\n",
      "IVFMEKAGYPLEKPFDFRVKGVTSI\n",
      "LKKKQESSGESRKFHKKMEDDDEDS\n",
      "DEEDTKRVVRSAKDKRFEELTNLIR\n",
      "LTKQLEAVADRVKVLYESKAVGYSW\n",
      "LLKRLRKEQTKLKWMQSELNVEEVV\n",
      "EQVKSICKILAMKAVRNNRLGSALS\n",
      "LNICANLDAFTIKTVKENLGQLKHQ\n",
      "GHFYGHLDFDLEKTLFLFIAGRYEF\n",
      "SSSKSEKREPVRKRMLEKRGEDFKK\n",
      "TFMNFPEDTLEQKAGSVGRIMPHTE\n",
      "DEQGFCKIVGRSKDMIIRGGENIYP\n",
      "DKLVRQYNEEGRKAGPPPEKRFDSR\n",
      "YGRGREEYEGPNKKPRFOOOOOOOO\n",
      "EIIQSVYEDLVAKGAIVPAQEVPPP\n",
      "GGAGEVDLQILSKVQAQYPGICINN\n",
      "GANSPEMWSEAIKILKGEYAVRAIK\n",
      "SIPVIQIVYETLKDQQEGKKGKTTI\n",
      "YWNYDDLPSRLEKVDLEYFEEGVEF\n",
      "KWVLNTRFSGQSKARCIESLLQVIH\n",
      "YCSTKGALDMLTKMMALELGPHKIR\n",
      "QAALVLGSLEARKEOOOOOOOOOOO\n",
      "QAALVLGSLEARKEOOOOOOOOOOO Q 519\n",
      "CRGAVMDLGPMRKSYRGDREAFEET\n",
      "GTEKEKGQCGIGKSCLCNRFVRPSA\n",
      "FPISRDAFQALEKLSKKQLNYVQLE\n",
      "RAWKSFEKQMEEKILQKGRDIELEN\n",
      "EIKGCKMMIDVIKTQFPQLKKVIQF\n",
      "VYCVEKLGIPAEKVNPLGGAIALGH\n",
      "QERFQTEIQTVSKQFPCEPFKFLEP\n",
      "QERFQTEIQTVSKQFPCEPFKFLEP Q 526\n",
      "KEKYDTDFYVLDKYPLAVRPFYTMP\n",
      "RYGISSMIQSQEKPDRVLVRVKDLT\n",
      "KEAKELAFLEATKQQEAPPGGEIDO\n",
      "AVATAVGMNMWTKRAPPLVGRWVPF\n",
      "VSTEVDARLSFDKDAMVARARRLIE\n",
      "SNVLVKFTLSEIKRVMQMLLNGLYY\n",
      "EKKTENSKVRKVKYEETVFYGLQYI\n",
      "RYGLKDIDGRPVKDYFSLGYALSQV\n",
      "EEKRDKTTEENIKTEELSSEESDLE\n",
      "AFPKTIIQSPYEKLKMSSDDGIRML\n",
      "LSERIEGFVETLKRGGGQRSSEDMA\n",
      "YRTILAETEGMLKDLQKSVEEEERV\n",
      "MAAASAECQNYAKEVAGLRQLLLES\n",
      "STYGLYRTHLLSKLPIPDSQVLTIN\n",
      "EEVFLSGEFNSLKKVTKEEWDIIEG\n",
      "VCHIPMAEPFCPKTREVLIETAKKL\n",
      "ARIVKVQLPAYLKQLPVPDSITGFA\n",
      "QYYIGDVHPSDLKPKGDDKDPSKNN\n",
      "QYYIGDVHPSDLKPKGDDKDPSKNN Q 544\n",
      "QLRVQEAVDAMVKSVERENIRKMQG\n",
      "QLRVQEAVDAMVKSVERENIRKMQG Q 545\n",
      "IIGFCYDYLHILKQLPVLSDQQKAN\n",
      "VLFSHREPPLELKDTDAAVGDNIGY\n",
      "GDKTKVMVSISLKFYKELQAHGADE\n",
      "DEVSTKILMEFNKMNLPGEVTFLPL\n",
      "DIGGVTLLRAAAKNHARVTVVCEPE\n",
      "YIRKKNKWLIKIKEWVDKYDPGALV\n",
      "CYISEMELLLSEKGEFTIETEGKTF\n",
      "TAEERDQLIPGLKAAGWSELSERDA\n",
      "ILNEEDEIQPNGKIFKKVILGHYNW\n",
      "EALEVKENSGYLKAKQYMEEENYDK\n",
      "GVVDPRATAVLAKWQNSHSIKVILQ\n",
      "PYDDEEFEGYEDKPDTSSNKNKDPI\n",
      "YFYLGQVAILLFKSGOOOOOOOOOO\n",
      "LERLSAVEESTDKKVOOOOOOOOOO\n",
      "LCPLAGNSVHADKKFLDKHMPQFMK\n",
      "TWDEYFETFINGKVSWGSWFDHVKG\n",
      "ITPPPLCEAAWEKECVLKGCKLNRL\n",
      "VGRGLCPQGARAKATIPAALQAQES\n",
      "AKAGMRDVVLMEKSELTAGSTWHAA\n",
      "FRWLPVGPHIMGKAVKOOOOOOOOO\n",
      "TEIKWKSGKDLTKRSSQTQNKASRK\n",
      "SLEWNLCVVQTLKEYLESLQCLDSD\n",
      "ESYLRRVVYPLEKLLTSHKRLVMKD\n",
      "RIEDQIVKSPEDKREYRGLELANGI\n",
      "ITEKMATFEIDKKRFEIIKEAYMRS\n",
      "QSGVNRWIREIQKVTKLDRDPASGT\n",
      "QSGVNRWIREIQKVTKLDRDPASGT Q 571\n",
      "SMAEPLVAGPDEKGETSAESEHDAH\n",
      "VEDVVVSDECRGKQLGKLLLSTLTL\n",
      "ALVAYDKKMDTNKEDPELMLGRMRC\n",
      "GLREVVTEHLINKVREDVLNSLNNN\n",
      "DVYELCTPELQEKMVSFRSKFKDLE\n",
      "DKKVNQQPNANDKNSPPKEIKYEPF\n",
      "YKCDPAGYYCGFKATAAGVKQTEST\n",
      "PSTIELIPFIIEKAVRSSIYGRPGA\n",
      "GSDRSEEQVSGAKVIAQALKTQDVE\n",
      "YIGDTRSRTAEYKAWLLGLLRQHGC\n",
      "AIKDYELQLITYKAQLEPVASPAKK\n",
      "AQQKSLAQADAEKQKEEAEREARRR\n",
      "AKLEQLFQDEVAKAKQLREEQQRQQ\n",
      "OOMALSMPLNGLKEEDKEPLIELFV\n",
      "OOOOMGQLFSSPKSDENNDLPSSFT\n",
      "OOOOOOOOOOMLKKFDKKDEESGGG\n",
      "EQRPESTATAAVKQPEKVAATRQEI\n",
      "KTRSLISMEEINKQLTFTAEDSGKE\n",
      "OMSSRGGKKKSTKTSRSAKAGVIFP\n",
      "ANHRQLTFEEIAKSAKITVNKVELL\n",
      "DLSPYPTISHINKELLALEVFQVSH\n",
      "QNKIDLVKESQAKEQYEQILAFVQG\n",
      "QNKIDLVKESQAKEQYEQILAFVQG Q 593\n",
      "LQKWIDMCNEDIKNGIGNLLMYVKS\n",
      "EYERFSKYMPNVKVAVFFGGLSIKK\n",
      "LFSSDWKKNEDLKQMLESNKDSAKL\n",
      "EPIENEAARYDLKYIGLDGNIACFV\n",
      "FAMRPFFGYNFGKYLAHWLSMAHRP\n",
      "AIQPRLVAVSKTKPADMVIEAYGHG\n",
      "(600, 1, 25)\n",
      "[[[ 1  4 16 ...  7  7 20]]\n",
      "\n",
      " [[ 4 11  9 ... 14 10  7]]\n",
      "\n",
      " [[15 10 20 ...  2 11 11]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 7 15 10 ...  5 14 20]]\n",
      "\n",
      " [[14  1 13 ...  9  2 15]]\n",
      "\n",
      " [[ 1 10  6 ...  8  9  8]]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "r_test_x = []\n",
    "r_test_y = []\n",
    "posit_1 = 1;\n",
    "negat_0 = 0; #dinh nghĩa label\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'OARNDCQEGHILKMFPSTWYV'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "i = 0\n",
    "#-------------------------TEST DATASET----------------------------------------\n",
    "#for positive sequence\n",
    "def innertest1():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #rint(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded)\n",
    "    r_test_y.append(posit_1)\n",
    "for seq_record in SeqIO.parse(\"./Datasets/independent_data/mus_test.fasta\", \"fasta\"):\n",
    "\n",
    "    innertest1()\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#for negative sequence\n",
    "def innertest2():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    print(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "#             print(data, i)\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    if integer_encoded[0] == 6: print(data, int_to_char[6], len(r_test_x))\n",
    "    r_test_x.append(integer_encoded) \n",
    "    r_test_y.append(negat_0)\n",
    "\n",
    "for seq_record in SeqIO.parse(\"./Datasets/independent_data/mus_test_neg.fasta\", \"fasta\"):\n",
    "    innertest2()\n",
    "# Changing to array (matrix)    \n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "\n",
    "# Balancing test dataset\n",
    "# Testing Data Balancing by undersampling####################################\n",
    "# rus = RandomUnderSampler(random_state=7)\n",
    "# x_res3, y_res3 = rus.fit_resample(r_test_x, r_test_y)\n",
    "# #Shuffling\n",
    "# r_test_x, r_test_y = shuffle(x_res3, y_res3, random_state=7)\n",
    "# r_test_x = np.array(r_test_x)\n",
    "# r_test_y = np.array(r_test_y)\n",
    "\n",
    "r_test_x = np.expand_dims(r_test_x, 1)\n",
    "print(r_test_x.shape)\n",
    "print(r_test_x)\n",
    "print(r_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9beb718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4 16  7  1 20 17 19  6 12 11 20 12  8  9  1 19 16 20 17  8  1  7  7\n",
      "  20]]\n",
      "['A', 'D', 'S', 'E', 'A', 'V', 'T', 'Y', 'Q', 'K', 'L', 'V', 'K', 'G', 'H', 'A', 'Y', 'S', 'V', 'T', 'G', 'A', 'E', 'E', 'V']\n",
      "tensor([[0.3612, 0.6388]], grad_fn=<SoftmaxBackward>)\n",
      "[[ 1 10  6 15  2 11 20  1 20 16 12 17 12 15  1  4 13 20 10  7  1 19  8  9\n",
      "   8]]\n",
      "['A', 'I', 'Q', 'P', 'R', 'L', 'V', 'A', 'V', 'S', 'K', 'T', 'K', 'P', 'A', 'D', 'M', 'V', 'I', 'E', 'A', 'Y', 'G', 'H', 'G']\n",
      "tensor([[0.2958, 0.7042]], grad_fn=<SoftmaxBackward>)\n",
      "['Q', 'Q', 'E', 'A', 'Q', 'R', 'A', 'Q', 'F', 'L', 'V', 'E', 'K', 'A', 'K', 'Q', 'E', 'Q', 'R', 'Q', 'K', 'I', 'V', 'Q', 'A'] [[0.5384481  0.46155185]]\n",
      "['Y', 'L', 'E', 'Y', 'M', 'K', 'V', 'V', 'D', 'G', 'L', 'E', 'K', 'A', 'V', 'Y', 'Q', 'G', 'P', 'G', 'S', 'S', 'P', 'V', 'K'] [[0.51028466 0.4897154 ]]\n",
      "['S', 'D', 'S', 'V', 'L', 'R', 'V', 'L', 'S', 'Q', 'M', 'E', 'K', 'I', 'V', 'R', 'I', 'T', 'W', 'S', 'N', 'P', 'P', 'A', 'Q'] [[0.5891493  0.41085076]]\n",
      "['A', 'E', 'L', 'A', 'L', 'G', 'E', 'N', 'N', 'E', 'V', 'L', 'K', 'S', 'G', 'R', 'F', 'V', 'T', 'V', 'Q', 'T', 'I', 'S', 'G'] [[0.5853566  0.41464338]]\n",
      "['L', 'V', 'V', 'M', 'A', 'M', 'E', 'K', 'E', 'G', 'L', 'S', 'K', 'E', 'N', 'A', 'R', 'K', 'K', 'I', 'W', 'L', 'V', 'D', 'S'] [[0.54026043 0.45973948]]\n",
      "['E', 'I', 'T', 'T', 'A', 'K', 'G', 'K', 'K', 'T', 'P', 'A', 'K', 'V', 'V', 'P', 'M', 'K', 'A', 'K', 'S', 'V', 'A', 'E', 'E'] [[0.69149053 0.30850944]]\n",
      "['K', 'H', 'A', 'V', 'S', 'E', 'G', 'T', 'K', 'A', 'V', 'T', 'K', 'Y', 'T', 'S', 'S', 'K', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] [[0.5403206  0.45967945]]\n",
      "['G', 'W', 'L', 'G', 'R', 'G', 'G', 'S', 'M', 'L', 'G', 'T', 'K', 'R', 'T', 'L', 'P', 'K', 'P', 'H', 'L', 'E', 'A', 'I', 'V'] [[0.67347753 0.32652244]]\n",
      "['I', 'T', 'T', 'V', 'Q', 'Q', 'R', 'G', 'A', 'A', 'V', 'I', 'K', 'A', 'R', 'K', 'L', 'S', 'S', 'A', 'M', 'S', 'A', 'A', 'K'] [[0.504179 0.495821]]\n",
      "['A', 'Q', 'N', 'I', 'I', 'P', 'A', 'S', 'T', 'G', 'A', 'A', 'K', 'A', 'V', 'G', 'K', 'V', 'I', 'P', 'E', 'L', 'N', 'G', 'K'] [[0.56918037 0.43081963]]\n",
      "['L', 'V', 'S', 'K', 'K', 'V', 'N', 'V', 'V', 'E', 'Q', 'E', 'K', 'I', 'D', 'K', 'L', 'M', 'I', 'E', 'M', 'D', 'G', 'T', 'E'] [[0.6671187 0.3328812]]\n",
      "['K', 'Q', 'S', 'A', 'A', 'L', 'C', 'L', 'L', 'R', 'L', 'Y', 'K', 'A', 'S', 'P', 'D', 'L', 'V', 'P', 'M', 'G', 'E', 'W', 'T'] [[0.7747816  0.22521849]]\n",
      "['K', 'E', 'L', 'Q', 'E', 'R', 'L', 'K', 'N', 'Q', 'E', 'K', 'K', 'I', 'D', 'Y', 'F', 'E', 'R', 'A', 'K', 'R', 'L', 'E', 'E'] [[0.5226549  0.47734514]]\n",
      "['E', 'F', 'T', 'A', 'K', 'I', 'A', 'L', 'L', 'E', 'E', 'A', 'K', 'K', 'K', 'K', 'E', 'E', 'E', 'A', 'T', 'E', 'W', 'Q', 'H'] [[0.6325511  0.36744896]]\n",
      "['A', 'P', 'S', 'G', 'R', 'S', 'Q', 'S', 'S', 'L', 'H', 'R', 'K', 'G', 'L', 'M', 'A', 'A', 'D', 'R', 'R', 'S', 'R', 'I', 'L'] [[0.60595983 0.39404017]]\n",
      "['V', 'H', 'N', 'T', 'A', 'S', 'A', 'L', 'R', 'E', 'L', 'E', 'K', 'V', 'T', 'G', 'T', 'Q', 'F', 'P', 'E', 'A', 'P', 'L', 'P'] [[0.5352795  0.46472052]]\n",
      "['L', 'L', 'S', 'A', 'S', 'Q', 'A', 'R', 'L', 'R', 'D', 'D', 'K', 'E', 'L', 'R', 'Q', 'R', 'A', 'F', 'T', 'S', 'Q', 'A', 'S'] [[0.5931052  0.40689483]]\n",
      "['I', 'I', 'R', 'L', 'V', 'Q', 'A', 'F', 'Q', 'F', 'T', 'D', 'K', 'H', 'G', 'E', 'V', 'C', 'P', 'A', 'G', 'W', 'K', 'P', 'G'] [[0.6078704  0.39212957]]\n",
      "['V', 'L', 'D', 'P', 'F', 'T', 'I', 'K', 'P', 'L', 'D', 'R', 'K', 'L', 'I', 'L', 'D', 'S', 'A', 'R', 'A', 'T', 'K', 'G', 'R'] [[0.54694295 0.45305702]]\n",
      "['A', 'K', 'T', 'K', 'V', 'E', 'K', 'K', 'K', 'K', 'K', 'E', 'K', 'V', 'L', 'A', 'T', 'V', 'T', 'K', 'T', 'V', 'G', 'G', 'D'] [[0.62569875 0.37430128]]\n",
      "['L', 'Q', 'L', 'E', 'L', 'S', 'K', 'V', 'R', 'E', 'E', 'F', 'K', 'E', 'L', 'K', 'A', 'R', 'N', 'T', 'K', 'K', 'E', 'G', 'D'] [[0.6384128 0.3615872]]\n",
      "['V', 'D', 'E', 'E', 'G', 'K', 'F', 'V', 'R', 'L', 'R', 'N', 'K', 'S', 'N', 'E', 'D', 'Q', 'S', 'M', 'G', 'N', 'W', 'Q', 'I'] [[0.6392606  0.36073944]]\n",
      "['K', 'V', 'L', 'R', 'I', 'F', 'D', 'K', 'R', 'T', 'Q', 'Q', 'K', 'F', 'A', 'L', 'K', 'M', 'L', 'Q', 'D', 'C', 'P', 'K', 'A'] [[0.5406514  0.45934868]]\n",
      "['G', 'L', 'M', 'R', 'M', 'R', 'E', 'M', 'Y', 'S', 'A', 'S', 'K', 'P', 'L', 'K', 'G', 'A', 'R', 'I', 'A', 'G', 'C', 'L', 'H'] [[0.5275198  0.47248015]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'E', 'P', 'L', 'R', 'K', 'P', 'L', 'V', 'P', 'V', 'K', 'G', 'I', 'P', 'L', 'I', 'K'] [[0.7044053  0.29559466]]\n",
      "['D', 'Q', 'V', 'T', 'I', 'D', 'S', 'A', 'L', 'A', 'T', 'Q', 'K', 'Y', 'S', 'V', 'A', 'V', 'K', 'C', 'A', 'T', 'I', 'T', 'P'] [[0.6236183  0.37638167]]\n",
      "['L', 'H', 'K', 'R', 'R', 'R', 'A', 'L', 'G', 'L', 'H', 'G', 'K', 'K', 'S', 'G', 'K', 'P', 'P', 'L', 'Q', 'N', 'N', 'E', 'K'] [[0.55591893 0.44408104]]\n",
      "['F', 'N', 'C', 'K', 'K', 'E', 'S', 'S', 'N', 'L', 'S', 'H', 'K', 'I', 'F', 'S', 'Y', 'E', 'C', 'R', 'F', 'K', 'Q', 'H', 'V'] [[0.51125216 0.48874784]]\n",
      "['I', 'L', 'R', 'Y', 'L', 'T', 'T', 'A', 'V', 'I', 'T', 'N', 'K', 'D', 'V', 'R', 'K', 'R', 'R', 'Q', 'V', 'L', 'K', 'D', 'L'] [[0.52985424 0.47014576]]\n",
      "['Y', 'P', 'G', 'L', 'P', 'S', 'R', 'L', 'E', 'R', 'E', 'L', 'K', 'Q', 'L', 'Y', 'L', 'E', 'R', 'V', 'L', 'K', 'G', 'D', 'V'] [[0.5282084  0.47179165]]\n",
      "['T', 'K', 'K', 'N', 'K', 'E', 'E', 'A', 'A', 'E', 'Y', 'A', 'K', 'L', 'L', 'A', 'K', 'R', 'M', 'K', 'E', 'A', 'K', 'E', 'K'] [[0.59392023 0.4060798 ]]\n",
      "['Y', 'G', 'V', 'L', 'R', 'F', 'I', 'M', 'E', 'S', 'G', 'A', 'K', 'G', 'C', 'E', 'V', 'V', 'V', 'S', 'G', 'K', 'L', 'R', 'G'] [[0.508565   0.49143496]]\n",
      "['E', 'D', 'G', 'R', 'T', 'L', 'S', 'D', 'Y', 'N', 'I', 'Q', 'K', 'E', 'S', 'T', 'L', 'H', 'L', 'V', 'L', 'R', 'L', 'R', 'G'] [[0.7952351  0.20476495]]\n",
      "['P', 'A', 'Y', 'F', 'N', 'D', 'S', 'Q', 'R', 'Q', 'A', 'T', 'K', 'D', 'A', 'G', 'T', 'I', 'A', 'G', 'L', 'N', 'V', 'L', 'R'] [[0.5647898  0.43521026]]\n",
      "['E', 'R', 'W', 'K', 'T', 'M', 'S', 'A', 'K', 'E', 'K', 'G', 'K', 'F', 'E', 'D', 'M', 'A', 'K', 'A', 'D', 'K', 'A', 'R', 'Y'] [[0.527456 0.472544]]\n",
      "['T', 'N', 'K', 'A', 'K', 'T', 'L', 'A', 'T', 'W', 'A', 'T', 'K', 'E', 'L', 'R', 'K', 'L', 'K', 'N', 'Q', 'A', 'O', 'O', 'O'] [[0.51920456 0.48079544]]\n",
      "['K', 'H', 'L', 'T', 'S', 'A', 'Q', 'S', 'V', 'L', 'D', 'S', 'K', 'V', 'M', 'K', 'I', 'K', 'S', 'K', 'H', 'N', 'O', 'O', 'O'] [[0.6138     0.38619998]]\n",
      "['A', 'A', 'D', 'N', 'S', 'K', 'D', 'P', 'Y', 'D', 'P', 'H', 'K', 'V', 'Y', 'E', 'F', 'V', 'T', 'P', 'G', 'A', 'R', 'D', 'F'] [[0.56925046 0.43074954]]\n",
      "['M', 'T', 'A', 'L', 'S', 'S', 'K', 'L', 'I', 'S', 'Q', 'Q', 'K', 'V', 'F', 'F', 'A', 'K', 'M', 'V', 'V', 'D', 'A', 'V', 'M'] [[0.6362516  0.36374837]]\n",
      "['G', 'Y', 'L', 'L', 'R', 'L', 'F', 'C', 'V', 'G', 'F', 'T', 'K', 'K', 'R', 'N', 'N', 'Q', 'I', 'R', 'K', 'T', 'S', 'Y', 'A'] [[0.5789888 0.4210112]]\n",
      "['R', 'K', 'F', 'K', 'L', 'I', 'T', 'E', 'D', 'V', 'Q', 'G', 'K', 'N', 'C', 'L', 'T', 'N', 'F', 'H', 'G', 'M', 'D', 'L', 'T'] [[0.5753623 0.4246377]]\n",
      "['S', 'F', 'R', 'V', 'E', 'F', 'D', 'T', 'F', 'G', 'E', 'L', 'K', 'V', 'P', 'T', 'D', 'K', 'Y', 'Y', 'G', 'A', 'Q', 'T', 'V'] [[0.59250355 0.40749654]]\n",
      "['K', 'Y', 'I', 'Q', 'F', 'K', 'T', 'T', 'V', 'C', 'S', 'V', 'K', 'K', 'Q', 'P', 'D', 'F', 'S', 'T', 'S', 'G', 'Q', 'W', 'Q'] [[0.5038245 0.4961756]]\n",
      "['A', 'V', 'I', 'Y', 'A', 'G', 'V', 'R', 'G', 'Y', 'L', 'D', 'K', 'L', 'E', 'P', 'S', 'K', 'I', 'T', 'K', 'F', 'E', 'N', 'A'] [[0.5874605  0.41253954]]\n",
      "['K', 'Y', 'Q', 'V', 'D', 'P', 'D', 'A', 'C', 'F', 'S', 'A', 'K', 'V', 'N', 'N', 'S', 'S', 'L', 'I', 'G', 'L', 'G', 'Y', 'T'] [[0.76231784 0.23768213]]\n",
      "['N', 'E', 'E', 'L', 'A', 'G', 'V', 'V', 'A', 'E', 'V', 'Q', 'K', 'N', 'G', 'R', 'V', 'S', 'V', 'V', 'L', 'G', 'G', 'D', 'H'] [[0.5177186  0.48228142]]\n",
      "['E', 'S', 'A', 'T', 'Y', 'C', 'L', 'V', 'K', 'N', 'A', 'V', 'K', 'M', 'F', 'K', 'D', 'I', 'G', 'V', 'F', 'K', 'E', 'T', 'K'] [[0.5634165  0.43658346]]\n",
      "['F', 'D', 'T', 'N', 'I', 'Q', 'T', 'S', 'S', 'K', 'K', 'K', 'K', 'N', 'A', 'K', 'P', 'L', 'S', 'F', 'K', 'V', 'G', 'V', 'G'] [[0.71976864 0.28023133]]\n",
      "['T', 'L', 'D', 'G', 'A', 'A', 'C', 'L', 'L', 'N', 'S', 'N', 'K', 'Y', 'F', 'P', 'S', 'R', 'V', 'S', 'I', 'K', 'E', 'S', 'S'] [[0.51670676 0.48329324]]\n",
      "['S', 'E', 'L', 'S', 'R', 'R', 'R', 'I', 'R', 'S', 'I', 'N', 'K', 'L', 'I', 'R', 'I', 'G', 'R', 'N', 'E', 'C', 'V', 'V', 'V'] [[0.6167685  0.38323152]]\n",
      "['K', 'D', 'T', 'R', 'E', 'E', 'L', 'P', 'V', 'N', 'T', 'S', 'K', 'A', 'R', 'P', 'K', 'Q', 'E', 'K', 'A', 'C', 'S', 'L', 'K'] [[0.52571946 0.4742806 ]]\n",
      "['G', 'K', 'L', 'D', 'V', 'T', 'I', 'L', 'S', 'P', 'S', 'R', 'K', 'V', 'V', 'P', 'C', 'L', 'V', 'A', 'P', 'V', 'A', 'G', 'R'] [[0.5343349 0.4656651]]\n",
      "['E', 'T', 'T', 'D', 'D', 'S', 'L', 'R', 'E', 'H', 'F', 'E', 'K', 'W', 'G', 'T', 'L', 'T', 'D', 'C', 'V', 'V', 'M', 'R', 'D'] [[0.52095556 0.47904444]]\n",
      "['I', 'P', 'G', 'Y', 'P', 'Q', 'A', 'K', 'L', 'Y', 'P', 'G', 'K', 'S', 'L', 'M', 'R', 'R', 'L', 'L', 'T', 'S', 'G', 'I', 'V'] [[0.5233465  0.47665352]]\n",
      "['Q', 'L', 'P', 'G', 'L', 'E', 'E', 'D', 'R', 'T', 'F', 'Q', 'K', 'E', 'I', 'S', 'L', 'K', 'T', 'L', 'V', 'F', 'K', 'A', 'Y'] [[0.52196085 0.47803918]]\n",
      "['A', 'L', 'M', 'G', 'L', 'Y', 'N', 'G', 'Q', 'V', 'L', 'C', 'K', 'K', 'N', 'K', 'F', 'G', 'A', 'P', 'Q', 'K', 'N', 'V', 'Q'] [[0.5983487  0.40165135]]\n",
      "['I', 'N', 'E', 'K', 'K', 'R', 'Q', 'K', 'K', 'K', 'S', 'Y', 'K', 'N', 'S', 'G', 'V', 'I', 'S', 'V', 'K', 'H', 'C', 'E', 'I'] [[0.5643433 0.4356567]]\n",
      "['A', 'V', 'A', 'K', 'Y', 'L', 'R', 'F', 'N', 'C', 'P', 'T', 'K', 'S', 'T', 'N', 'M', 'M', 'G', 'H', 'R', 'V', 'D', 'Y', 'F'] [[0.5932539  0.40674612]]\n",
      "['F', 'R', 'V', 'V', 'D', 'S', 'I', 'E', 'D', 'E', 'V', 'Q', 'K', 'R', 'L', 'Q', 'L', 'V', 'Q', 'A', 'G', 'Q', 'A', 'E', 'K'] [[0.5170736  0.48292646]]\n",
      "['G', 'G', 'Y', 'G', 'S', 'Q', 'G', 'Y', 'K', 'Y', 'T', 'W', 'K', 'L', 'E', 'E', 'A', 'R', 'K', 'N', 'L', 'L', 'R', 'T', 'H'] [[0.68590164 0.31409833]]\n",
      "['I', 'M', 'A', 'T', 'E', 'D', 'R', 'Q', 'L', 'F', 'S', 'D', 'K', 'L', 'N', 'E', 'I', 'N', 'E', 'K', 'I', 'A', 'P', 'S', 'F'] [[0.59074956 0.4092504 ]]\n",
      "['A', 'L', 'G', 'G', 'L', 'G', 'S', 'G', 'I', 'C', 'P', 'N', 'K', 'E', 'T', 'L', 'I', 'D', 'L', 'G', 'T', 'K', 'A', 'F', 'A'] [[0.84356505 0.15643498]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'D', 'D', 'P', 'Y', 'K', 'K', 'A', 'V', 'A', 'M', 'S', 'K', 'R', 'Y', 'G', 'S', 'D', 'R', 'R', 'L', 'A', 'E', 'L', 'S'] [[0.5833433  0.41665664]]\n",
      "['P', 'S', 'I', 'R', 'E', 'Q', 'I', 'P', 'L', 'M', 'E', 'R', 'K', 'V', 'L', 'S', 'G', 'A', 'L', 'S', 'P', 'G', 'R', 'A', 'A'] [[0.51536596 0.484634  ]]\n",
      "['M', 'K', 'D', 'S', 'K', 'L', 'P', 'V', 'A', 'Q', 'S', 'Q', 'K', 'T', 'K', 'G', 'D', 'T', 'P', 'A', 'S', 'A', 'A', 'S', 'T'] [[0.5612773  0.43872264]]\n",
      "['F', 'Q', 'K', 'T', 'G', 'Q', 'Y', 'F', 'V', 'S', 'I', 'S', 'K', 'K', 'L', 'M', 'G', 'L', 'S', 'I', 'L', 'Q', 'F', 'S', 'Y'] [[0.5906147  0.40938535]]\n",
      "['E', 'E', 'G', 'P', 'K', 'Y', 'K', 'S', 'K', 'V', 'S', 'L', 'K', 'G', 'N', 'R', 'E', 'S', 'D', 'G', 'F', 'R', 'E', 'E', 'K'] [[0.56856686 0.4314332 ]]\n",
      "['H', 'T', 'G', 'Y', 'A', 'H', 'W', 'K', 'G', 'Q', 'V', 'L', 'K', 'S', 'Q', 'E', 'L', 'H', 'E', 'L', 'Y', 'E', 'G', 'L', 'K'] [[0.6412739  0.35872605]]\n",
      "['G', 'E', 'F', 'K', 'D', 'M', 'D', 'A', 'T', 'S', 'E', 'L', 'K', 'N', 'K', 'T', 'F', 'D', 'T', 'L', 'R', 'N', 'H', 'P', 'S'] [[0.69246864 0.30753133]]\n",
      "['S', 'I', 'R', 'T', 'S', 'E', 'L', 'R', 'L', 'N', 'M', 'Q', 'K', 'S', 'M', 'Q', 'N', 'H', 'A', 'A', 'V', 'F', 'R', 'V', 'G'] [[0.58442277 0.41557723]]\n",
      "['G', 'L', 'P', 'I', 'S', 'T', 'P', 'C', 'T', 'T', 'V', 'N', 'K', 'V', 'C', 'A', 'S', 'G', 'M', 'K', 'A', 'I', 'M', 'M', 'A'] [[0.72310203 0.27689797]]\n",
      "['K', 'G', 'G', 'Q', 'T', 'R', 'E', 'H', 'A', 'M', 'L', 'A', 'K', 'T', 'A', 'G', 'V', 'K', 'H', 'L', 'I', 'V', 'L', 'I', 'N'] [[0.6286475 0.3713525]]\n",
      "['Q', 'P', 'E', 'E', 'G', 'A', 'T', 'Y', 'E', 'G', 'I', 'Q', 'K', 'K', 'E', 'T', 'A', 'M', 'I', 'N', 'W', 'D', 'Q', 'P', 'A'] [[0.64898586 0.3510142 ]]\n",
      "['S', 'L', 'F', 'G', 'Q', 'E', 'V', 'Y', 'C', 'Q', 'L', 'R', 'K', 'E', 'G', 'H', 'E', 'V', 'V', 'G', 'V', 'F', 'T', 'I', 'P'] [[0.5541174 0.4458826]]\n",
      "['A', 'E', 'A', 'I', 'H', 'N', 'W', 'I', 'R', 'G', 'N', 'D', 'K', 'V', 'P', 'G', 'A', 'W', 'T', 'E', 'A', 'C', 'G', 'Q', 'K'] [[0.56906605 0.43093398]]\n",
      "['V', 'Y', 'K', 'L', 'A', 'E', 'S', 'G', 'S', 'C', 'L', 'G', 'K', 'K', 'V', 'A', 'G', 'A', 'L', 'Q', 'T', 'I', 'E', 'T', 'A'] [[0.5409164  0.45908362]]\n",
      "['S', 'K', 'I', 'V', 'E', 'I', 'P', 'F', 'N', 'S', 'T', 'N', 'K', 'Y', 'Q', 'L', 'S', 'I', 'H', 'K', 'N', 'P', 'N', 'A', 'S'] [[0.57643026 0.42356974]]\n",
      "['L', 'D', 'L', 'N', 'A', 'A', 'E', 'S', 'G', 'V', 'Q', 'H', 'K', 'P', 'S', 'A', 'P', 'Q', 'G', 'G', 'R', 'L', 'T', 'S', 'E'] [[0.5283784 0.4716216]]\n",
      "['G', 'K', 'S', 'C', 'L', 'L', 'H', 'Q', 'F', 'T', 'E', 'K', 'K', 'F', 'M', 'A', 'D', 'C', 'P', 'H', 'T', 'I', 'G', 'V', 'E'] [[0.77955735 0.22044267]]\n",
      "['E', 'F', 'G', 'T', 'R', 'I', 'I', 'E', 'V', 'S', 'G', 'Q', 'K', 'I', 'K', 'L', 'Q', 'I', 'W', 'D', 'T', 'A', 'G', 'Q', 'E'] [[0.5697005  0.43029952]]\n",
      "['V', 'V', 'H', 'A', 'F', 'D', 'M', 'E', 'D', 'L', 'G', 'D', 'K', 'A', 'V', 'Y', 'C', 'R', 'C', 'W', 'R', 'S', 'K', 'K', 'F'] [[0.557095   0.44290504]]\n",
      "['D', 'A', 'R', 'A', 'F', 'A', 'A', 'C', 'L', 'E', 'A', 'I', 'K', 'L', 'P', 'K', 'N', 'T', 'P', 'E', 'E', 'R', 'D', 'R', 'R'] [[0.5224807 0.4775193]]\n",
      "['D', 'K', 'V', 'A', 'E', 'E', 'W', 'A', 'Q', 'G', 'T', 'F', 'K', 'L', 'H', 'P', 'N', 'D', 'E', 'D', 'I', 'H', 'T', 'A', 'N'] [[0.73814076 0.2618593 ]]\n",
      "['G', 'S', 'D', 'L', 'C', 'D', 'R', 'V', 'S', 'E', 'M', 'Q', 'K', 'L', 'D', 'A', 'Q', 'V', 'K', 'E', 'L', 'V', 'L', 'K', 'S'] [[0.8448307  0.15516928]]\n",
      "['K', 'L', 'N', 'G', 'G', 'L', 'G', 'T', 'S', 'M', 'G', 'C', 'K', 'G', 'P', 'K', 'S', 'L', 'I', 'G', 'V', 'R', 'N', 'E', 'N'] [[0.6326514  0.36734864]]\n",
      "['T', 'T', 'V', 'D', 'P', 'D', 'T', 'G', 'I', 'I', 'D', 'R', 'K', 'E', 'P', 'L', 'E', 'T', 'L', 'K', 'S', 'Y', 'R', 'L', 'C'] [[0.55825025 0.44174978]]\n",
      "['I', 'M', 'S', 'E', 'D', 'R', 'A', 'L', 'A', 'M', 'G', 'Y', 'K', 'P', 'K', 'A', 'Y', 'L', 'R', 'D', 'F', 'I', 'Y', 'V', 'S'] [[0.6167566  0.38324332]]\n",
      "['S', 'F', 'T', 'V', 'T', 'E', 'L', 'R', 'T', 'M', 'I', 'Y', 'K', 'D', 'V', 'P', 'E', 'N', 'L', 'H', 'K', 'M', 'A', 'E', 'H'] [[0.6011004 0.3988997]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'S', 'N', 'M', 'E', 'K', 'H', 'L', 'F', 'N', 'L', 'K', 'F', 'A', 'A', 'K', 'E', 'L'] [[0.50550044 0.49449953]]\n",
      "['L', 'H', 'S', 'K', 'L', 'Q', 'S', 'S', 'E', 'V', 'E', 'V', 'K', 'S', 'K', 'C', 'E', 'E', 'L', 'S', 'S', 'L', 'H', 'G', 'Q'] [[0.5732573  0.42674267]]\n",
      "['A', 'E', 'I', 'S', 'T', 'L', 'A', 'K', 'I', 'S', 'S', 'V', 'K', 'L', 'R', 'L', 'L', 'C', 'S', 'Q', 'V', 'L', 'K', 'E', 'L'] [[0.58616376 0.41383627]]\n",
      "['Y', 'E', 'Y', 'H', 'P', 'V', 'C', 'A', 'D', 'L', 'Q', 'T', 'K', 'I', 'L', 'Q', 'C', 'Y', 'R', 'Q', 'N', 'T', 'Q', 'Q', 'T'] [[0.7831861  0.21681394]]\n",
      "['V', 'V', 'D', 'K', 'S', 'L', 'F', 'S', 'N', 'I', 'V', 'T', 'K', 'N', 'K', 'D', 'L', 'P', 'E', 'S', 'A', 'L', 'R', 'D', 'L'] [[0.5519576 0.4480424]]\n",
      "['G', 'S', 'L', 'E', 'E', 'V', 'P', 'N', 'V', 'G', 'V', 'N', 'K', 'N', 'I', 'F', 'L', 'K', 'D', 'Q', 'D', 'I', 'F', 'I', 'Q'] [[0.69646376 0.30353624]]\n",
      "['Q', 'Q', 'E', 'F', 'V', 'R', 'A', 'L', 'A', 'A', 'F', 'L', 'K', 'K', 'S', 'G', 'K', 'L', 'K', 'V', 'P', 'E', 'W', 'V', 'D'] [[0.5170055  0.48299453]]\n",
      "['Q', 'S', 'V', 'E', 'D', 'I', 'L', 'K', 'D', 'H', 'W', 'Q', 'K', 'F', 'G', 'R', 'N', 'F', 'F', 'T', 'R', 'Y', 'D', 'Y', 'E'] [[0.5332297  0.46677032]]\n",
      "['E', 'E', 'V', 'E', 'A', 'E', 'G', 'A', 'N', 'K', 'M', 'M', 'K', 'D', 'L', 'E', 'A', 'L', 'M', 'L', 'D', 'R', 'S', 'F', 'V'] [[0.75299793 0.2470021 ]]\n",
      "['E', 'V', 'T', 'V', 'G', 'H', 'E', 'Q', 'E', 'E', 'G', 'G', 'K', 'W', 'P', 'Y', 'A', 'G', 'T', 'A', 'E', 'A', 'I', 'K', 'A'] [[0.7706831  0.22931688]]\n",
      "['L', 'K', 'Q', 'R', 'E', 'A', 'I', 'L', 'K', 'L', 'I', 'L', 'K', 'N', 'E', 'N', 'V', 'D', 'R', 'H', 'V', 'D', 'L', 'L', 'E'] [[0.5252131  0.47478697]]\n",
      "['M', 'D', 'E', 'C', 'E', 'Q', 'A', 'L', 'A', 'A', 'E', 'P', 'K', 'A', 'K', 'D', 'P', 'F', 'A', 'H', 'L', 'P', 'K', 'S', 'T'] [[0.54717517 0.45282483]]\n",
      "['S', 'R', 'Q', 'A', 'L', 'K', 'V', 'L', 'S', 'G', 'T', 'L', 'K', 'L', 'D', 'S', 'K', 'E', 'A', 'L', 'N', 'I', 'G', 'L', 'T'] [[0.50140357 0.49859646]]\n",
      "['P', 'D', 'L', 'G', 'Q', 'L', 'S', 'K', 'V', 'T', 'K', 'V', 'K', 'T', 'D', 'R', 'P', 'L', 'P', 'E', 'N', 'P', 'Y', 'H', 'S'] [[0.7786718  0.22132821]]\n",
      "['R', 'F', 'K', 'V', 'F', 'A', 'D', 'Y', 'E', 'A', 'Y', 'V', 'K', 'C', 'Q', 'E', 'K', 'V', 'S', 'Q', 'L', 'Y', 'M', 'N', 'Q'] [[0.7138116  0.28618845]]\n",
      "['D', 'R', 'E', 'V', 'N', 'A', 'V', 'D', 'S', 'E', 'H', 'E', 'K', 'N', 'V', 'M', 'N', 'D', 'A', 'W', 'R', 'L', 'F', 'Q', 'L'] [[0.58549    0.41450995]]\n",
      "['V', 'Y', 'G', 'N', 'E', 'T', 'E', 'I', 'G', 'E', 'A', 'L', 'K', 'E', 'S', 'V', 'G', 'S', 'G', 'K', 'A', 'V', 'P', 'R', 'E'] [[0.6721439 0.3278561]]\n",
      "['I', 'L', 'T', 'T', 'Q', 'R', 'E', 'P', 'K', 'E', 'M', 'Q', 'K', 'P', 'Q', 'W', 'Y', 'R', 'H', 'T', 'F', 'E', 'E', 'A', 'E'] [[0.6007924  0.39920756]]\n",
      "['F', 'L', 'Y', 'P', 'A', 'N', 'K', 'K', 'S', 'P', 'S', 'G', 'K', 'L', 'R', 'L', 'L', 'Y', 'E', 'C', 'N', 'P', 'I', 'A', 'Y'] [[0.5771757  0.42282432]]\n",
      "['P', 'G', 'K', 'N', 'I', 'Y', 'Y', 'K', 'S', 'D', 'L', 'T', 'K', 'D', 'I', 'T', 'T', 'S', 'V', 'L', 'T', 'V', 'N', 'N', 'K'] [[0.7190695 0.2809305]]\n",
      "['L', 'R', 'Q', 'F', 'R', 'T', 'G', 'K', 'V', 'T', 'V', 'E', 'K', 'V', 'I', 'K', 'I', 'V', 'I', 'T', 'I', 'V', 'E', 'E', 'V'] [[0.71093404 0.28906593]]\n",
      "['R', 'C', 'P', 'K', 'R', 'L', 'Q', 'E', 'L', 'L', 'P', 'T', 'K', 'A', 'A', 'P', 'R', 'P', 'S', 'A', 'S', 'C', 'V', 'Y', 'K'] [[0.53857505 0.46142495]]\n",
      "['N', 'L', 'F', 'E', 'W', 'A', 'K', 'K', 'S', 'P', 'L', 'N', 'K', 'T', 'E', 'V', 'H', 'E', 'S', 'Y', 'Y', 'K', 'H', 'L', 'K'] [[0.57586884 0.42413116]]\n",
      "['A', 'Q', 'K', 'I', 'R', 'K', 'K', 'R', 'L', 'G', 'N', 'R', 'K', 'S', 'V', 'V', 'F', 'T', 'S', 'A', 'R', 'A', 'T', 'T', 'E'] [[0.58910656 0.41089347]]\n",
      "['K', 'C', 'L', 'P', 'I', 'R', 'G', 'V', 'D', 'G', 'N', 'G', 'K', 'S', 'P', 'S', 'K', 'S', 'E', 'L', 'H', 'R', 'L', 'Y', 'L'] [[0.45638692 0.54361314]]\n",
      "['I', 'P', 'S', 'A', 'K', 'N', 'L', 'D', 'A', 'D', 'I', 'W', 'K', 'K', 'F', 'L', 'S', 'R', 'P', 'A', 'L', 'P', 'F', 'I', 'L'] [[0.44014135 0.5598586 ]]\n",
      "['W', 'G', 'V', 'N', 'L', 'P', 'A', 'H', 'A', 'V', 'V', 'I', 'K', 'G', 'T', 'Q', 'I', 'Y', 'A', 'A', 'K', 'R', 'G', 'S', 'F'] [[0.34731886 0.6526812 ]]\n",
      "['S', 'G', 'Y', 'C', 'L', 'P', 'E', 'P', 'F', 'F', 'S', 'M', 'K', 'L', 'K', 'D', 'W', 'V', 'Q', 'K', 'L', 'M', 'M', 'T', 'L'] [[0.48209 0.51791]]\n",
      "['A', 'E', 'G', 'S', 'E', 'F', 'W', 'S', 'A', 'L', 'L', 'E', 'K', 'A', 'Y', 'A', 'K', 'I', 'N', 'G', 'C', 'Y', 'E', 'A', 'L'] [[0.48527965 0.5147204 ]]\n",
      "['F', 'A', 'L', 'K', 'K', 'I', 'K', 'R', 'S', 'S', 'I', 'R', 'K', 'G', 'M', 'V', 'M', 'V', 'S', 'P', 'R', 'L', 'N', 'P', 'Q'] [[0.32227698 0.677723  ]]\n",
      "['F', 'V', 'I', 'F', 'P', 'P', 'R', 'W', 'G', 'V', 'A', 'D', 'K', 'T', 'F', 'R', 'P', 'P', 'Y', 'Y', 'H', 'R', 'N', 'C', 'M'] [[0.47482994 0.5251701 ]]\n",
      "['R', 'L', 'K', 'E', 'I', 'I', 'C', 'E', 'Q', 'A', 'A', 'I', 'K', 'Q', 'A', 'T', 'K', 'D', 'K', 'K', 'I', 'T', 'T', 'V', 'O'] [[0.4667565 0.5332435]]\n",
      "['O', 'O', 'O', 'M', 'P', 'L', 'R', 'L', 'D', 'I', 'K', 'R', 'K', 'L', 'T', 'A', 'R', 'S', 'D', 'R', 'V', 'K', 'S', 'V', 'D'] [[0.24644153 0.75355846]]\n",
      "['C', 'T', 'P', 'N', 'K', 'P', 'S', 'R', 'T', 'S', 'M', 'S', 'K', 'M', 'F', 'V', 'K', 'G', 'A', 'P', 'E', 'G', 'V', 'I', 'D'] [[0.45945776 0.5405423 ]]\n",
      "['E', 'S', 'K', 'K', 'A', 'K', 'K', 'P', 'A', 'V', 'V', 'A', 'K', 'S', 'S', 'I', 'L', 'L', 'D', 'V', 'K', 'P', 'W', 'D', 'D'] [[0.33455554 0.66544443]]\n",
      "['R', 'F', 'K', 'D', 'I', 'F', 'Q', 'E', 'I', 'Y', 'D', 'K', 'K', 'Y', 'K', 'S', 'Q', 'F', 'E', 'A', 'Q', 'K', 'I', 'C', 'Y'] [[0.40940937 0.5905906 ]]\n",
      "['I', 'S', 'M', 'R', 'T', 'Q', 'L', 'V', 'S', 'N', 'L', 'K', 'K', 'E', 'G', 'S', 'S', 'H', 'N', 'W', 'Q', 'H', 'I', 'T', 'D'] [[0.24350342 0.75649655]]\n",
      "['F', 'K', 'L', 'C', 'P', 'M', 'N', 'R', 'Y', 'S', 'A', 'Q', 'K', 'Q', 'F', 'W', 'K', 'A', 'A', 'K', 'P', 'G', 'A', 'N', 'S'] [[0.4081258 0.5918742]]\n",
      "['S', 'C', 'D', 'R', 'I', 'K', 'Q', 'S', 'A', 'S', 'G', 'T', 'K', 'R', 'R', 'V', 'F', 'I', 'V', 'E', 'T', 'M', 'G', 'G', 'Y'] [[0.37617567 0.6238243 ]]\n",
      "['K', 'V', 'V', 'A', 'F', 'S', 'P', 'V', 'T', 'E', 'L', 'K', 'K', 'E', 'T', 'D', 'F', 'E', 'H', 'R', 'M', 'P', 'R', 'E', 'Q'] [[0.39438805 0.605612  ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'E', 'K', 'A', 'G', 'A', 'H', 'L', 'K', 'G', 'G', 'A', 'K', 'R', 'V', 'I', 'I', 'S', 'A', 'P', 'S', 'A', 'D', 'A', 'P'] [[0.3720511 0.6279489]]\n",
      "['K', 'M', 'S', 'Y', 'K', 'A', 'A', 'M', 'G', 'E', 'E', 'Y', 'K', 'A', 'G', 'C', 'P', 'P', 'G', 'N', 'P', 'T', 'A', 'G', 'R'] [[0.30334875 0.6966513 ]]\n",
      "['G', 'D', 'T', 'K', 'N', 'S', 'T', 'F', 'S', 'E', 'L', 'F', 'K', 'K', 'E', 'H', 'P', 'D', 'R', 'F', 'I', 'E', 'C', 'Y', 'I'] [[0.4452848  0.55471516]]\n",
      "['L', 'G', 'S', 'R', 'T', 'R', 'A', 'I', 'G', 'N', 'A', 'A', 'K', 'S', 'Q', 'I', 'V', 'T', 'N', 'V', 'R', 'N', 'T', 'I', 'H'] [[0.44166172 0.55833834]]\n",
      "['N', 'K', 'I', 'D', 'L', 'E', 'N', 'R', 'Q', 'V', 'A', 'T', 'K', 'R', 'A', 'Q', 'A', 'W', 'C', 'Y', 'S', 'K', 'N', 'N', 'I'] [[0.3307566  0.66924334]]\n",
      "['H', 'G', 'I', 'S', 'L', 'F', 'L', 'V', 'E', 'N', 'G', 'M', 'K', 'G', 'F', 'I', 'K', 'G', 'R', 'K', 'L', 'H', 'K', 'M', 'G'] [[0.46873057 0.5312695 ]]\n",
      "['L', 'D', 'D', 'A', 'E', 'K', 'K', 'L', 'N', 'L', 'A', 'Q', 'K', 'C', 'F', 'K', 'N', 'C', 'Y', 'G', 'E', 'N', 'H', 'Q', 'R'] [[0.47975186 0.52024806]]\n",
      "['L', 'D', 'N', 'P', 'A', 'R', 'I', 'P', 'P', 'C', 'G', 'W', 'K', 'C', 'S', 'K', 'C', 'D', 'M', 'R', 'E', 'N', 'L', 'W', 'L'] [[0.4540564 0.5459436]]\n",
      "['V', 'I', 'Y', 'N', 'D', 'Q', 'K', 'V', 'C', 'A', 'S', 'E', 'K', 'P', 'P', 'K', 'D', 'L', 'G', 'Y', 'I', 'Y', 'F', 'Y', 'Q'] [[0.34358844 0.6564115 ]]\n",
      "['I', 'T', 'T', 'R', 'K', 'T', 'P', 'C', 'G', 'E', 'G', 'S', 'K', 'T', 'W', 'D', 'R', 'F', 'Q', 'M', 'R', 'I', 'H', 'K', 'R'] [[0.4265664  0.57343364]]\n",
      "['E', 'I', 'A', 'A', 'G', 'K', 'C', 'R', 'R', 'P', 'A', 'V', 'K', 'Q', 'F', 'H', 'D', 'S', 'K', 'I', 'K', 'F', 'P', 'L', 'P'] [[0.41661707 0.58338296]]\n",
      "['T', 'R', 'K', 'F', 'M', 'T', 'N', 'R', 'L', 'L', 'Q', 'R', 'K', 'Q', 'M', 'V', 'I', 'D', 'V', 'L', 'H', 'P', 'G', 'K', 'A'] [[0.32829162 0.67170846]]\n",
      "['P', 'R', 'K', 'V', 'V', 'G', 'C', 'S', 'C', 'V', 'V', 'V', 'K', 'D', 'Y', 'G', 'K', 'E', 'S', 'Q', 'A', 'K', 'D', 'V', 'I'] [[0.36779743 0.6322025 ]]\n",
      "['E', 'S', 'Q', 'A', 'K', 'D', 'V', 'I', 'E', 'E', 'Y', 'F', 'K', 'C', 'K', 'K', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] [[0.4066589 0.5933411]]\n",
      "['P', 'S', 'P', 'Q', 'K', 'I', 'P', 'K', 'V', 'A', 'G', 'P', 'K', 'E', 'A', 'S', 'A', 'T', 'P', 'P', 'S', 'K', 'K', 'T', 'P'] [[0.3230982 0.6769018]]\n",
      "['E', 'Q', 'R', 'F', 'K', 'L', 'F', 'K', 'I', 'A', 'C', 'E', 'K', 'H', 'Q', 'H', 'L', 'Y', 'R', 'L', 'A', 'M', 'T', 'G', 'A'] [[0.43078604 0.5692139 ]]\n",
      "['Y', 'S', 'F', 'A', 'F', 'P', 'F', 'L', 'E', 'D', 'S', 'V', 'K', 'V', 'V', 'K', 'N', 'K', 'V', 'S', 'L', 'Y', 'K', 'K', 'V'] [[0.44180176 0.5581982 ]]\n",
      "['R', 'A', 'S', 'T', 'C', 'L', 'A', 'G', 'R', 'A', 'G', 'R', 'K', 'E', 'A', 'G', 'W', 'E', 'C', 'G', 'G', 'A', 'R', 'S', 'F'] [[0.29999793 0.7000021 ]]\n",
      "['T', 'P', 'E', 'H', 'A', 'H', 'R', 'A', 'A', 'R', 'G', 'V', 'K', 'I', 'T', 'Y', 'E', 'D', 'L', 'P', 'A', 'I', 'I', 'T', 'I'] [[0.44897258 0.5510274 ]]\n",
      "['V', 'R', 'V', 'K', 'R', 'M', 'G', 'G', 'G', 'F', 'G', 'G', 'K', 'E', 'T', 'R', 'S', 'T', 'L', 'I', 'S', 'T', 'A', 'V', 'A'] [[0.29014084 0.70985913]]\n",
      "['G', 'A', 'K', 'Y', 'T', 'K', 'G', 'D', 'S', 'N', 'I', 'Q', 'K', 'R', 'W', 'K', 'E', 'L', 'S', 'K', 'E', 'D', 'L', 'S', 'P'] [[0.38319156 0.61680853]]\n",
      "['E', 'E', 'K', 'A', 'P', 'E', 'V', 'S', 'P', 'A', 'Q', 'P', 'K', 'P', 'G', 'V', 'A', 'A', 'P', 'P', 'E', 'V', 'A', 'P', 'A'] [[0.4278622  0.57213783]]\n",
      "['Q', 'T', 'S', 'A', 'E', 'V', 'Y', 'R', 'I', 'L', 'R', 'Q', 'K', 'G', 'L', 'L', 'D', 'K', 'F', 'P', 'L', 'F', 'T', 'A', 'V'] [[0.26147357 0.73852646]]\n",
      "['Y', 'K', 'I', 'V', 'P', 'P', 'K', 'S', 'L', 'E', 'M', 'A', 'K', 'D', 'W', 'E', 'S', 'E', 'A', 'M', 'G', 'R', 'K', 'D', 'D'] [[0.47519612 0.5248039 ]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'K', 'L', 'K', 'E', 'I', 'D', 'R', 'T', 'A', 'M', 'Q', 'A', 'W', 'S', 'P'] [[0.48053616 0.5194639 ]]\n",
      "['Q', 'G', 'Y', 'W', 'A', 'Q', 'S', 'H', 'T', 'L', 'R', 'G', 'K', 'V', 'Y', 'Q', 'R', 'L', 'I', 'R', 'D', 'I', 'P', 'C', 'R'] [[0.46384633 0.5361537 ]]\n",
      "['L', 'F', 'Y', 'G', 'L', 'K', 'F', 'R', 'R', 'R', 'L', 'N', 'K', 'H', 'P', 'P', 'E', 'T', 'F', 'V', 'D', 'A', 'L', 'E', 'R'] [[0.41232386 0.58767617]]\n",
      "['P', 'P', 'K', 'R', 'R', 'A', 'S', 'P', 'S', 'P', 'P', 'P', 'K', 'R', 'R', 'V', 'S', 'H', 'S', 'P', 'P', 'P', 'K', 'Q', 'R'] [[0.37189978 0.6281003 ]]\n",
      "['V', 'A', 'E', 'I', 'I', 'A', 'G', 'L', 'I', 'R', 'G', 'S', 'K', 'H', 'W', 'T', 'F', 'E', 'K', 'V', 'E', 'K', 'L', 'W', 'E'] [[0.4740594 0.5259406]]\n",
      "['V', 'D', 'A', 'D', 'E', 'I', 'I', 'V', 'L', 'S', 'Q', 'G', 'K', 'V', 'A', 'E', 'R', 'G', 'T', 'H', 'Y', 'G', 'L', 'L', 'A'] [[0.47433013 0.52566993]]\n",
      "['K', 'A', 'S', 'A', 'A', 'Q', 'P', 'D', 'S', 'V', 'G', 'V', 'K', 'A', 'K', 'V', 'L', 'E', 'N', 'F', 'L', 'T', 'K', 'S', 'R'] [[0.45573825 0.54426175]]\n",
      "['V', 'S', 'I', 'Q', 'R', 'S', 'A', 'P', 'G', 'G', 'G', 'G', 'K', 'R', 'Y', 'R', 'A', 'V', 'Y', 'D', 'Y', 'S', 'A', 'A', 'D'] [[0.34388542 0.6561145 ]]\n",
      "['G', 'K', 'D', 'L', 'T', 'S', 'V', 'M', 'R', 'L', 'L', 'S', 'K', 'H', 'R', 'A', 'F', 'E', 'D', 'E', 'M', 'S', 'G', 'R', 'S'] [[0.48085245 0.5191475 ]]\n",
      "['K', 'R', 'Q', 'I', 'V', 'A', 'G', 'T', 'N', 'L', 'F', 'I', 'K', 'V', 'D', 'V', 'G', 'G', 'D', 'K', 'C', 'V', 'H', 'L', 'R'] [[0.48470044 0.51529956]]\n",
      "['H', 'C', 'P', 'S', 'A', 'V', 'V', 'P', 'V', 'E', 'L', 'Q', 'K', 'L', 'A', 'R', 'G', 'S', 'L', 'V', 'G', 'G', 'G', 'R', 'Q'] [[0.488736  0.5112639]]\n",
      "['S', 'V', 'G', 'Y', 'A', 'I', 'F', 'K', 'V', 'L', 'N', 'E', 'K', 'K', 'L', 'Q', 'E', 'V', 'D', 'S', 'L', 'W', 'K', 'E', 'F'] [[0.49090415 0.50909585]]\n",
      "['A', 'K', 'P', 'A', 'K', 'A', 'A', 'N', 'K', 'T', 'P', 'P', 'K', 'S', 'P', 'G', 'D', 'P', 'A', 'R', 'A', 'A', 'K', 'R', 'L'] [[0.40675867 0.59324133]]\n",
      "['L', 'A', 'A', 'F', 'K', 'K', 'G', 'I', 'I', 'G', 'G', 'V', 'K', 'V', 'H', 'V', 'D', 'R', 'L', 'Q', 'T', 'L', 'I', 'S', 'G'] [[0.4134612  0.58653885]]\n",
      "['K', 'Q', 'F', 'M', 'K', 'Q', 'H', 'V', 'F', 'P', 'A', 'E', 'K', 'E', 'V', 'A', 'E', 'Y', 'Y', 'A', 'Q', 'S', 'G', 'N', 'S'] [[0.4897483 0.5102517]]\n",
      "['M', 'S', 'K', 'V', 'V', 'D', 'S', 'L', 'Y', 'S', 'K', 'A', 'K', 'K', 'L', 'T', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] [[0.31384972 0.6861503 ]]\n",
      "['S', 'Y', 'A', 'V', 'S', 'V', 'W', 'K', 'R', 'V', 'K', 'A', 'K', 'L', 'E', 'G', 'R', 'D', 'V', 'D', 'P', 'N', 'R', 'R', 'M'] [[0.1169503  0.88304967]]\n",
      "['P', 'G', 'H', 'G', 'R', 'R', 'Y', 'A', 'R', 'T', 'D', 'G', 'K', 'V', 'F', 'Q', 'F', 'L', 'N', 'A', 'K', 'C', 'E', 'S', 'A'] [[0.30370554 0.6962945 ]]\n",
      "['G', 'R', 'V', 'I', 'R', 'D', 'R', 'K', 'T', 'I', 'P', 'I', 'K', 'Y', 'P', 'L', 'K', 'E', 'I', 'V', 'V', 'I', 'H', 'Q', 'D'] [[0.48206174 0.5179382 ]]\n",
      "['T', 'V', 'Q', 'A', 'A', 'E', 'R', 'A', 'A', 'S', 'S', 'V', 'K', 'D', 'L', 'A', 'Q', 'K', 'S', 'F', 'R', 'L', 'L', 'M', 'D'] [[0.4500194  0.54998064]]\n",
      "['F', 'R', 'V', 'G', 'L', 'S', 'G', 'P', 'P', 'G', 'A', 'G', 'K', 'S', 'T', 'F', 'I', 'E', 'C', 'F', 'G', 'K', 'M', 'L', 'T'] [[0.3029552 0.6970448]]\n",
      "['H', 'E', 'L', 'V', 'V', 'Q', 'A', 'R', 'D', 'D', 'F', 'R', 'K', 'E', 'L', 'D', 'S', 'I', 'T', 'P', 'D', 'I', 'T', 'P', 'G'] [[0.42892435 0.5710757 ]]\n",
      "['O', 'O', 'O', 'O', 'M', 'N', 'T', 'N', 'D', 'E', 'K', 'M', 'K', 'I', 'I', 'S', 'E', 'D', 'F', 'T', 'G', 'D', 'G', 'V', 'D'] [[0.22057174 0.7794283 ]]\n",
      "['I', 'M', 'K', 'Y', 'A', 'E', 'K', 'L', 'I', 'Q', 'E', 'G', 'K', 'A', 'Y', 'V', 'D', 'D', 'T', 'P', 'A', 'E', 'Q', 'M', 'K'] [[0.42936844 0.57063156]]\n",
      "['K', 'S', 'P', 'G', 'H', 'M', 'V', 'I', 'L', 'N', 'Q', 'T', 'K', 'G', 'D', 'H', 'C', 'R', 'P', 'S', 'R', 'R', 'G', 'R', 'Y'] [[0.42623892 0.5737611 ]]\n",
      "['E', 'Q', 'S', 'S', 'T', 'V', 'N', 'A', 'K', 'K', 'L', 'E', 'K', 'A', 'E', 'A', 'R', 'L', 'K', 'A', 'K', 'Q', 'E', 'K', 'R'] [[0.44132158 0.55867845]]\n",
      "['P', 'P', 'I', 'R', 'P', 'H', 'S', 'G', 'I', 'V', 'R', 'S', 'K', 'S', 'E', 'C', 'P', 'S', 'D', 'D', 'M', 'G', 'S', 'L', 'H'] [[0.3713138 0.6286862]]\n",
      "['L', 'E', 'E', 'G', 'D', 'L', 'I', 'L', 'T', 'G', 'T', 'P', 'K', 'G', 'V', 'G', 'P', 'I', 'K', 'E', 'N', 'D', 'E', 'I', 'E'] [[0.45758113 0.54241884]]\n",
      "['I', 'T', 'K', 'Q', 'T', 'L', 'D', 'E', 'L', 'N', 'Q', 'G', 'K', 'R', 'T', 'V', 'Q', 'E', 'V', 'T', 'E', 'M', 'D', 'S', 'V'] [[0.4012364  0.59876364]]\n",
      "['I', 'V', 'F', 'M', 'E', 'K', 'A', 'G', 'Y', 'P', 'L', 'E', 'K', 'P', 'F', 'D', 'F', 'R', 'V', 'K', 'G', 'V', 'T', 'S', 'I'] [[0.3801097 0.6198903]]\n",
      "['L', 'T', 'K', 'Q', 'L', 'E', 'A', 'V', 'A', 'D', 'R', 'V', 'K', 'V', 'L', 'Y', 'E', 'S', 'K', 'A', 'V', 'G', 'Y', 'S', 'W'] [[0.41738588 0.5826141 ]]\n",
      "['L', 'L', 'K', 'R', 'L', 'R', 'K', 'E', 'Q', 'T', 'K', 'L', 'K', 'W', 'M', 'Q', 'S', 'E', 'L', 'N', 'V', 'E', 'E', 'V', 'V'] [[0.47465938 0.52534056]]\n",
      "['Y', 'G', 'R', 'G', 'R', 'E', 'E', 'Y', 'E', 'G', 'P', 'N', 'K', 'K', 'P', 'R', 'F', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] [[0.4712809  0.52871907]]\n",
      "['S', 'I', 'P', 'V', 'I', 'Q', 'I', 'V', 'Y', 'E', 'T', 'L', 'K', 'D', 'Q', 'Q', 'E', 'G', 'K', 'K', 'G', 'K', 'T', 'T', 'I'] [[0.42416102 0.575839  ]]\n",
      "['K', 'W', 'V', 'L', 'N', 'T', 'R', 'F', 'S', 'G', 'Q', 'S', 'K', 'A', 'R', 'C', 'I', 'E', 'S', 'L', 'L', 'Q', 'V', 'I', 'H'] [[0.43856668 0.56143326]]\n",
      "['C', 'R', 'G', 'A', 'V', 'M', 'D', 'L', 'G', 'P', 'M', 'R', 'K', 'S', 'Y', 'R', 'G', 'D', 'R', 'E', 'A', 'F', 'E', 'E', 'T'] [[0.44087067 0.5591293 ]]\n",
      "['G', 'T', 'E', 'K', 'E', 'K', 'G', 'Q', 'C', 'G', 'I', 'G', 'K', 'S', 'C', 'L', 'C', 'N', 'R', 'F', 'V', 'R', 'P', 'S', 'A'] [[0.3620012  0.63799876]]\n",
      "['R', 'A', 'W', 'K', 'S', 'F', 'E', 'K', 'Q', 'M', 'E', 'E', 'K', 'I', 'L', 'Q', 'K', 'G', 'R', 'D', 'I', 'E', 'L', 'E', 'N'] [[0.47602624 0.52397376]]\n",
      "['E', 'I', 'K', 'G', 'C', 'K', 'M', 'M', 'I', 'D', 'V', 'I', 'K', 'T', 'Q', 'F', 'P', 'Q', 'L', 'K', 'K', 'V', 'I', 'Q', 'F'] [[0.43761656 0.56238353]]\n",
      "['K', 'E', 'K', 'Y', 'D', 'T', 'D', 'F', 'Y', 'V', 'L', 'D', 'K', 'Y', 'P', 'L', 'A', 'V', 'R', 'P', 'F', 'Y', 'T', 'M', 'P'] [[0.46069583 0.5393042 ]]\n",
      "['R', 'Y', 'G', 'I', 'S', 'S', 'M', 'I', 'Q', 'S', 'Q', 'E', 'K', 'P', 'D', 'R', 'V', 'L', 'V', 'R', 'V', 'K', 'D', 'L', 'T'] [[0.34581625 0.65418375]]\n",
      "['V', 'S', 'T', 'E', 'V', 'D', 'A', 'R', 'L', 'S', 'F', 'D', 'K', 'D', 'A', 'M', 'V', 'A', 'R', 'A', 'R', 'R', 'L', 'I', 'E'] [[0.49170175 0.5082983 ]]\n",
      "['E', 'K', 'K', 'T', 'E', 'N', 'S', 'K', 'V', 'R', 'K', 'V', 'K', 'Y', 'E', 'E', 'T', 'V', 'F', 'Y', 'G', 'L', 'Q', 'Y', 'I'] [[0.35010183 0.6498982 ]]\n",
      "['R', 'Y', 'G', 'L', 'K', 'D', 'I', 'D', 'G', 'R', 'P', 'V', 'K', 'D', 'Y', 'F', 'S', 'L', 'G', 'Y', 'A', 'L', 'S', 'Q', 'V'] [[0.33380628 0.6661937 ]]\n",
      "['M', 'A', 'A', 'A', 'S', 'A', 'E', 'C', 'Q', 'N', 'Y', 'A', 'K', 'E', 'V', 'A', 'G', 'L', 'R', 'Q', 'L', 'L', 'L', 'E', 'S'] [[0.36405012 0.63594985]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'E', 'V', 'F', 'L', 'S', 'G', 'E', 'F', 'N', 'S', 'L', 'K', 'K', 'V', 'T', 'K', 'E', 'E', 'W', 'D', 'I', 'I', 'E', 'G'] [[0.4736019  0.52639806]]\n",
      "['G', 'D', 'K', 'T', 'K', 'V', 'M', 'V', 'S', 'I', 'S', 'L', 'K', 'F', 'Y', 'K', 'E', 'L', 'Q', 'A', 'H', 'G', 'A', 'D', 'E'] [[0.3358825  0.66411746]]\n",
      "['D', 'I', 'G', 'G', 'V', 'T', 'L', 'L', 'R', 'A', 'A', 'A', 'K', 'N', 'H', 'A', 'R', 'V', 'T', 'V', 'V', 'C', 'E', 'P', 'E'] [[0.38962308 0.61037695]]\n",
      "['Y', 'I', 'R', 'K', 'K', 'N', 'K', 'W', 'L', 'I', 'K', 'I', 'K', 'E', 'W', 'V', 'D', 'K', 'Y', 'D', 'P', 'G', 'A', 'L', 'V'] [[0.45273405 0.54726595]]\n",
      "['E', 'A', 'L', 'E', 'V', 'K', 'E', 'N', 'S', 'G', 'Y', 'L', 'K', 'A', 'K', 'Q', 'Y', 'M', 'E', 'E', 'E', 'N', 'Y', 'D', 'K'] [[0.48175594 0.5182441 ]]\n",
      "['L', 'E', 'R', 'L', 'S', 'A', 'V', 'E', 'E', 'S', 'T', 'D', 'K', 'K', 'V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] [[0.3852851 0.6147149]]\n",
      "['V', 'G', 'R', 'G', 'L', 'C', 'P', 'Q', 'G', 'A', 'R', 'A', 'K', 'A', 'T', 'I', 'P', 'A', 'A', 'L', 'Q', 'A', 'Q', 'E', 'S'] [[0.39977154 0.6002284 ]]\n",
      "['F', 'R', 'W', 'L', 'P', 'V', 'G', 'P', 'H', 'I', 'M', 'G', 'K', 'A', 'V', 'K', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] [[0.45121273 0.54878724]]\n",
      "['I', 'T', 'E', 'K', 'M', 'A', 'T', 'F', 'E', 'I', 'D', 'K', 'K', 'R', 'F', 'E', 'I', 'I', 'K', 'E', 'A', 'Y', 'M', 'R', 'S'] [[0.4138705 0.5861294]]\n",
      "['Q', 'S', 'G', 'V', 'N', 'R', 'W', 'I', 'R', 'E', 'I', 'Q', 'K', 'V', 'T', 'K', 'L', 'D', 'R', 'D', 'P', 'A', 'S', 'G', 'T'] [[0.486002 0.513998]]\n",
      "['V', 'E', 'D', 'V', 'V', 'V', 'S', 'D', 'E', 'C', 'R', 'G', 'K', 'Q', 'L', 'G', 'K', 'L', 'L', 'L', 'S', 'T', 'L', 'T', 'L'] [[0.49179274 0.50820726]]\n",
      "['D', 'K', 'K', 'V', 'N', 'Q', 'Q', 'P', 'N', 'A', 'N', 'D', 'K', 'N', 'S', 'P', 'P', 'K', 'E', 'I', 'K', 'Y', 'E', 'P', 'F'] [[0.45850915 0.54149085]]\n",
      "['Y', 'K', 'C', 'D', 'P', 'A', 'G', 'Y', 'Y', 'C', 'G', 'F', 'K', 'A', 'T', 'A', 'A', 'G', 'V', 'K', 'Q', 'T', 'E', 'S', 'T'] [[0.381611 0.618389]]\n",
      "['G', 'S', 'D', 'R', 'S', 'E', 'E', 'Q', 'V', 'S', 'G', 'A', 'K', 'V', 'I', 'A', 'Q', 'A', 'L', 'K', 'T', 'Q', 'D', 'V', 'E'] [[0.49568662 0.5043134 ]]\n",
      "['O', 'O', 'O', 'O', 'M', 'G', 'Q', 'L', 'F', 'S', 'S', 'P', 'K', 'S', 'D', 'E', 'N', 'N', 'D', 'L', 'P', 'S', 'S', 'F', 'T'] [[0.4476438  0.55235624]]\n",
      "['E', 'Q', 'R', 'P', 'E', 'S', 'T', 'A', 'T', 'A', 'A', 'V', 'K', 'Q', 'P', 'E', 'K', 'V', 'A', 'A', 'T', 'R', 'Q', 'E', 'I'] [[0.42964876 0.5703513 ]]\n",
      "['E', 'P', 'I', 'E', 'N', 'E', 'A', 'A', 'R', 'Y', 'D', 'L', 'K', 'Y', 'I', 'G', 'L', 'D', 'G', 'N', 'I', 'A', 'C', 'F', 'V'] [[0.48963308 0.51036686]]\n",
      "['A', 'I', 'Q', 'P', 'R', 'L', 'V', 'A', 'V', 'S', 'K', 'T', 'K', 'P', 'A', 'D', 'M', 'V', 'I', 'E', 'A', 'Y', 'G', 'H', 'G'] [[0.295766 0.704234]]\n",
      "acc: 64.16666666666667\n",
      "sn: 64.60481099656357\n",
      "sp: 63.75404530744336\n",
      "mcc: 0.28346091946043106\n"
     ]
    }
   ],
   "source": [
    "PATH = './Path/dlmal_m_net.pth'\n",
    "net = torch.load(PATH, map_location=\"cpu\")\n",
    "net.eval()\n",
    "\n",
    "print(r_test_x[0])\n",
    "print([int_to_char[i] for i in r_test_x[0][0]])\n",
    "y = net(torch.from_numpy(np.array([r_test_x[0]])))\n",
    "print(F.softmax(y, dim=1))\n",
    "\n",
    "print(r_test_x[-1])\n",
    "print([int_to_char[i] for i in r_test_x[-1][0]])\n",
    "y = net(torch.from_numpy(np.array([r_test_x[-1]])))\n",
    "print(F.softmax(y, dim=1))\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for i in range(len(r_test_x)):\n",
    "    y = net(torch.from_numpy(np.array([r_test_x[i]])))\n",
    "    y = F.softmax(y, dim=1)\n",
    "    y = y.detach().cpu().numpy()\n",
    "    if (y[0][0] > y[0][1] and r_test_y[i] == 0):\n",
    "        TN += 1\n",
    "    elif (y[0][0] < y[0][1] and r_test_y[i] == 1):\n",
    "        TP += 1\n",
    "    elif r_test_y[i] == 0:\n",
    "        FN += 1\n",
    "        print([int_to_char[i] for i in r_test_x[i][0]], y)\n",
    "    else:\n",
    "        FP +=1\n",
    "        print([int_to_char[i] for i in r_test_x[i][0]], y)\n",
    "       \n",
    "print(\"acc:\", (TP+TN)/(TP+TN+FP+FN)*100)\n",
    "print(\"sn:\", (TP)/(TP+FN)*100)\n",
    "print(\"sp:\", TN/(TN+FP)*100)\n",
    "print(\"mcc:\", (TP*TN-FP*FN)/((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10271e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
