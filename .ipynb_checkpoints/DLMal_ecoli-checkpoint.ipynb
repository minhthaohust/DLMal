{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd05ee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2906, 1, 25)\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "r_test_x = []\n",
    "r_test_y = []\n",
    "posit_1 = 1;\n",
    "negat_0 = 0;\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'OARNDCQEGHILKMFPSTWYV'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "i = 0\n",
    "#-------------------------TEST DATASET----------------------------------------\n",
    "#for positive sequence\n",
    "def innertest1():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #rint(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            print(data, i)\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded)\n",
    "    r_test_y.append(posit_1)\n",
    "for seq_record in SeqIO.parse(\"./Datasets/training_data/ecoli_train.fasta\", \"fasta\"):\n",
    "\n",
    "    innertest1()\n",
    "    i += 1\n",
    "    \n",
    "#print(len(r_test_x))\n",
    "\n",
    "#for negative sequence\n",
    "def innertest2():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #print(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded) \n",
    "    r_test_y.append(negat_0)\n",
    "\n",
    "for seq_record in SeqIO.parse(\"./Datasets/training_data/ecoli_train_neg.fasta\", \"fasta\"):\n",
    "    innertest2()\n",
    "# Changing to array (matrix)    \n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "\n",
    "# Balancing test dataset\n",
    "# Testing Data Balancing by undersampling####################################\n",
    "# trộn dữ liệu\n",
    "rus = RandomUnderSampler(random_state=7)\n",
    "x_res3, y_res3 = rus.fit_resample(r_test_x, r_test_y)\n",
    "#Shuffling\n",
    "r_test_x, r_test_y = shuffle(x_res3, y_res3, random_state=7)\n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "#print(r_test_y.shape)\n",
    "r_test_x = np.expand_dims(r_test_x, 1)\n",
    "print(r_test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8858f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3481, -0.1081]], grad_fn=<AddmmBackward>)\n",
      "[1 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DLMal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLMal, self).__init__()\n",
    "        self.embedding = nn.Embedding(25,21)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (15, 3))\n",
    "        self.dropout1 = nn.Dropout(0.6)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(4096, 768)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(768, 256)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.maxpool(x)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout4(x)\n",
    "        return self.fc3(x)\n",
    "net = DLMal()\n",
    "y = net(torch.from_numpy(np.array([r_test_x[0]])))#convert array numpy to tensor\n",
    "print(y)\n",
    "print(r_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3189f2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] train loss: 0.708 train acc: 0.477\n",
      "[1,     2] train loss: 0.731 train acc: 0.512\n",
      "[1,     3] train loss: 0.786 train acc: 0.465\n",
      "[1,     4] train loss: 1.054 train acc: 0.500\n",
      "[1,     5] train loss: 0.708 train acc: 0.535\n",
      "[1,     6] train loss: 0.735 train acc: 0.492\n",
      "[1,     7] train loss: 0.732 train acc: 0.477\n",
      "[1,     8] train loss: 0.699 train acc: 0.533\n",
      "[1] val loss: 0.693 val acc: 0.484\n",
      "[2,     1] train loss: 0.694 train acc: 0.531\n",
      "[2,     2] train loss: 0.696 train acc: 0.523\n",
      "[2,     3] train loss: 0.698 train acc: 0.457\n",
      "[2,     4] train loss: 0.693 train acc: 0.480\n",
      "[2,     5] train loss: 0.695 train acc: 0.465\n",
      "[2,     6] train loss: 0.693 train acc: 0.531\n",
      "[2,     7] train loss: 0.693 train acc: 0.473\n",
      "[2,     8] train loss: 0.694 train acc: 0.430\n",
      "[2] val loss: 0.693 val acc: 0.517\n",
      "[3,     1] train loss: 0.693 train acc: 0.500\n",
      "[3,     2] train loss: 0.693 train acc: 0.504\n",
      "[3,     3] train loss: 0.693 train acc: 0.523\n",
      "[3,     4] train loss: 0.693 train acc: 0.457\n",
      "[3,     5] train loss: 0.693 train acc: 0.523\n",
      "[3,     6] train loss: 0.693 train acc: 0.535\n",
      "[3,     7] train loss: 0.693 train acc: 0.535\n",
      "[3,     8] train loss: 0.693 train acc: 0.488\n",
      "[3] val loss: 0.693 val acc: 0.515\n",
      "[4,     1] train loss: 0.693 train acc: 0.488\n",
      "[4,     2] train loss: 0.692 train acc: 0.555\n",
      "[4,     3] train loss: 0.693 train acc: 0.508\n",
      "[4,     4] train loss: 0.692 train acc: 0.547\n",
      "[4,     5] train loss: 0.693 train acc: 0.500\n",
      "[4,     6] train loss: 0.694 train acc: 0.480\n",
      "[4,     7] train loss: 0.694 train acc: 0.480\n",
      "[4,     8] train loss: 0.694 train acc: 0.446\n",
      "[4] val loss: 0.693 val acc: 0.518\n",
      "[5,     1] train loss: 0.694 train acc: 0.477\n",
      "[5,     2] train loss: 0.693 train acc: 0.508\n",
      "[5,     3] train loss: 0.693 train acc: 0.488\n",
      "[5,     4] train loss: 0.693 train acc: 0.535\n",
      "[5,     5] train loss: 0.693 train acc: 0.492\n",
      "[5,     6] train loss: 0.693 train acc: 0.496\n",
      "[5,     7] train loss: 0.693 train acc: 0.496\n",
      "[5,     8] train loss: 0.692 train acc: 0.537\n",
      "[5] val loss: 0.693 val acc: 0.517\n",
      "[6,     1] train loss: 0.693 train acc: 0.523\n",
      "[6,     2] train loss: 0.692 train acc: 0.547\n",
      "[6,     3] train loss: 0.693 train acc: 0.523\n",
      "[6,     4] train loss: 0.693 train acc: 0.480\n",
      "[6,     5] train loss: 0.694 train acc: 0.465\n",
      "[6,     6] train loss: 0.693 train acc: 0.504\n",
      "[6,     7] train loss: 0.693 train acc: 0.488\n",
      "[6,     8] train loss: 0.693 train acc: 0.500\n",
      "[6] val loss: 0.693 val acc: 0.518\n",
      "[7,     1] train loss: 0.693 train acc: 0.496\n",
      "[7,     2] train loss: 0.693 train acc: 0.500\n",
      "[7,     3] train loss: 0.694 train acc: 0.488\n",
      "[7,     4] train loss: 0.693 train acc: 0.488\n",
      "[7,     5] train loss: 0.693 train acc: 0.523\n",
      "[7,     6] train loss: 0.693 train acc: 0.520\n",
      "[7,     7] train loss: 0.693 train acc: 0.488\n",
      "[7,     8] train loss: 0.693 train acc: 0.537\n",
      "[7] val loss: 0.693 val acc: 0.515\n",
      "[8,     1] train loss: 0.693 train acc: 0.488\n",
      "[8,     2] train loss: 0.693 train acc: 0.496\n",
      "[8,     3] train loss: 0.692 train acc: 0.547\n",
      "[8,     4] train loss: 0.694 train acc: 0.449\n",
      "[8,     5] train loss: 0.693 train acc: 0.480\n",
      "[8,     6] train loss: 0.693 train acc: 0.535\n",
      "[8,     7] train loss: 0.693 train acc: 0.512\n",
      "[8,     8] train loss: 0.692 train acc: 0.541\n",
      "[8] val loss: 0.693 val acc: 0.519\n",
      "[9,     1] train loss: 0.692 train acc: 0.535\n",
      "[9,     2] train loss: 0.694 train acc: 0.441\n",
      "[9,     3] train loss: 0.693 train acc: 0.551\n",
      "[9,     4] train loss: 0.693 train acc: 0.512\n",
      "[9,     5] train loss: 0.692 train acc: 0.578\n",
      "[9,     6] train loss: 0.694 train acc: 0.469\n",
      "[9,     7] train loss: 0.694 train acc: 0.480\n",
      "[9,     8] train loss: 0.693 train acc: 0.504\n",
      "[9] val loss: 0.693 val acc: 0.518\n",
      "[10,     1] train loss: 0.693 train acc: 0.512\n",
      "[10,     2] train loss: 0.693 train acc: 0.488\n",
      "[10,     3] train loss: 0.693 train acc: 0.516\n",
      "[10,     4] train loss: 0.692 train acc: 0.531\n",
      "[10,     5] train loss: 0.693 train acc: 0.516\n",
      "[10,     6] train loss: 0.693 train acc: 0.480\n",
      "[10,     7] train loss: 0.693 train acc: 0.531\n",
      "[10,     8] train loss: 0.694 train acc: 0.488\n",
      "[10] val loss: 0.693 val acc: 0.517\n",
      "[11,     1] train loss: 0.693 train acc: 0.500\n",
      "[11,     2] train loss: 0.693 train acc: 0.520\n",
      "[11,     3] train loss: 0.694 train acc: 0.492\n",
      "[11,     4] train loss: 0.693 train acc: 0.496\n",
      "[11,     5] train loss: 0.693 train acc: 0.512\n",
      "[11,     6] train loss: 0.694 train acc: 0.504\n",
      "[11,     7] train loss: 0.693 train acc: 0.504\n",
      "[11,     8] train loss: 0.693 train acc: 0.529\n",
      "[11] val loss: 0.693 val acc: 0.518\n",
      "[12,     1] train loss: 0.693 train acc: 0.508\n",
      "[12,     2] train loss: 0.694 train acc: 0.453\n",
      "[12,     3] train loss: 0.692 train acc: 0.535\n",
      "[12,     4] train loss: 0.694 train acc: 0.477\n",
      "[12,     5] train loss: 0.692 train acc: 0.531\n",
      "[12,     6] train loss: 0.694 train acc: 0.480\n",
      "[12,     7] train loss: 0.691 train acc: 0.551\n",
      "[12,     8] train loss: 0.693 train acc: 0.492\n",
      "[12] val loss: 0.693 val acc: 0.518\n",
      "[13,     1] train loss: 0.694 train acc: 0.492\n",
      "[13,     2] train loss: 0.694 train acc: 0.488\n",
      "[13,     3] train loss: 0.694 train acc: 0.469\n",
      "[13,     4] train loss: 0.693 train acc: 0.508\n",
      "[13,     5] train loss: 0.693 train acc: 0.500\n",
      "[13,     6] train loss: 0.692 train acc: 0.551\n",
      "[13,     7] train loss: 0.693 train acc: 0.527\n",
      "[13,     8] train loss: 0.692 train acc: 0.533\n",
      "[13] val loss: 0.693 val acc: 0.517\n",
      "[14,     1] train loss: 0.693 train acc: 0.516\n",
      "[14,     2] train loss: 0.693 train acc: 0.508\n",
      "[14,     3] train loss: 0.692 train acc: 0.551\n",
      "[14,     4] train loss: 0.694 train acc: 0.480\n",
      "[14,     5] train loss: 0.694 train acc: 0.449\n",
      "[14,     6] train loss: 0.692 train acc: 0.531\n",
      "[14,     7] train loss: 0.692 train acc: 0.539\n",
      "[14,     8] train loss: 0.693 train acc: 0.463\n",
      "[14] val loss: 0.693 val acc: 0.518\n",
      "[15,     1] train loss: 0.693 train acc: 0.508\n",
      "[15,     2] train loss: 0.693 train acc: 0.500\n",
      "[15,     3] train loss: 0.693 train acc: 0.480\n",
      "[15,     4] train loss: 0.693 train acc: 0.523\n",
      "[15,     5] train loss: 0.694 train acc: 0.484\n",
      "[15,     6] train loss: 0.694 train acc: 0.457\n",
      "[15,     7] train loss: 0.693 train acc: 0.523\n",
      "[15,     8] train loss: 0.692 train acc: 0.583\n",
      "[15] val loss: 0.693 val acc: 0.514\n",
      "[16,     1] train loss: 0.694 train acc: 0.488\n",
      "[16,     2] train loss: 0.693 train acc: 0.500\n",
      "[16,     3] train loss: 0.693 train acc: 0.523\n",
      "[16,     4] train loss: 0.693 train acc: 0.516\n",
      "[16,     5] train loss: 0.693 train acc: 0.508\n",
      "[16,     6] train loss: 0.694 train acc: 0.477\n",
      "[16,     7] train loss: 0.691 train acc: 0.574\n",
      "[16,     8] train loss: 0.694 train acc: 0.517\n",
      "[16] val loss: 0.693 val acc: 0.515\n",
      "[17,     1] train loss: 0.693 train acc: 0.512\n",
      "[17,     2] train loss: 0.692 train acc: 0.512\n",
      "[17,     3] train loss: 0.694 train acc: 0.504\n",
      "[17,     4] train loss: 0.692 train acc: 0.512\n",
      "[17,     5] train loss: 0.691 train acc: 0.570\n",
      "[17,     6] train loss: 0.693 train acc: 0.500\n",
      "[17,     7] train loss: 0.693 train acc: 0.512\n",
      "[17,     8] train loss: 0.689 train acc: 0.554\n",
      "[17] val loss: 0.692 val acc: 0.518\n",
      "[18,     1] train loss: 0.693 train acc: 0.504\n",
      "[18,     2] train loss: 0.693 train acc: 0.488\n",
      "[18,     3] train loss: 0.688 train acc: 0.520\n",
      "[18,     4] train loss: 0.693 train acc: 0.520\n",
      "[18,     5] train loss: 0.695 train acc: 0.488\n",
      "[18,     6] train loss: 0.690 train acc: 0.512\n",
      "[18,     7] train loss: 0.693 train acc: 0.516\n",
      "[18,     8] train loss: 0.691 train acc: 0.541\n",
      "[18] val loss: 0.691 val acc: 0.580\n",
      "[19,     1] train loss: 0.697 train acc: 0.500\n",
      "[19,     2] train loss: 0.693 train acc: 0.551\n",
      "[19,     3] train loss: 0.692 train acc: 0.523\n",
      "[19,     4] train loss: 0.695 train acc: 0.496\n",
      "[19,     5] train loss: 0.688 train acc: 0.539\n",
      "[19,     6] train loss: 0.691 train acc: 0.500\n",
      "[19,     7] train loss: 0.690 train acc: 0.531\n",
      "[19,     8] train loss: 0.688 train acc: 0.537\n",
      "[19] val loss: 0.688 val acc: 0.595\n",
      "[20,     1] train loss: 0.687 train acc: 0.523\n",
      "[20,     2] train loss: 0.685 train acc: 0.551\n",
      "[20,     3] train loss: 0.692 train acc: 0.496\n",
      "[20,     4] train loss: 0.688 train acc: 0.512\n",
      "[20,     5] train loss: 0.688 train acc: 0.512\n",
      "[20,     6] train loss: 0.687 train acc: 0.531\n",
      "[20,     7] train loss: 0.687 train acc: 0.535\n",
      "[20,     8] train loss: 0.693 train acc: 0.492\n",
      "[20] val loss: 0.682 val acc: 0.560\n",
      "[21,     1] train loss: 0.685 train acc: 0.543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21,     2] train loss: 0.685 train acc: 0.512\n",
      "[21,     3] train loss: 0.692 train acc: 0.512\n",
      "[21,     4] train loss: 0.685 train acc: 0.535\n",
      "[21,     5] train loss: 0.675 train acc: 0.617\n",
      "[21,     6] train loss: 0.695 train acc: 0.516\n",
      "[21,     7] train loss: 0.673 train acc: 0.578\n",
      "[21,     8] train loss: 0.674 train acc: 0.603\n",
      "[21] val loss: 0.680 val acc: 0.588\n",
      "[22,     1] train loss: 0.691 train acc: 0.508\n",
      "[22,     2] train loss: 0.687 train acc: 0.543\n",
      "[22,     3] train loss: 0.672 train acc: 0.590\n",
      "[22,     4] train loss: 0.697 train acc: 0.531\n",
      "[22,     5] train loss: 0.681 train acc: 0.531\n",
      "[22,     6] train loss: 0.674 train acc: 0.562\n",
      "[22,     7] train loss: 0.680 train acc: 0.535\n",
      "[22,     8] train loss: 0.685 train acc: 0.558\n",
      "[22] val loss: 0.678 val acc: 0.597\n",
      "[23,     1] train loss: 0.676 train acc: 0.555\n",
      "[23,     2] train loss: 0.691 train acc: 0.492\n",
      "[23,     3] train loss: 0.676 train acc: 0.523\n",
      "[23,     4] train loss: 0.697 train acc: 0.535\n",
      "[23,     5] train loss: 0.675 train acc: 0.605\n",
      "[23,     6] train loss: 0.664 train acc: 0.594\n",
      "[23,     7] train loss: 0.683 train acc: 0.562\n",
      "[23,     8] train loss: 0.674 train acc: 0.558\n",
      "[23] val loss: 0.674 val acc: 0.631\n",
      "[24,     1] train loss: 0.675 train acc: 0.578\n",
      "[24,     2] train loss: 0.656 train acc: 0.641\n",
      "[24,     3] train loss: 0.680 train acc: 0.547\n",
      "[24,     4] train loss: 0.686 train acc: 0.523\n",
      "[24,     5] train loss: 0.681 train acc: 0.594\n",
      "[24,     6] train loss: 0.653 train acc: 0.578\n",
      "[24,     7] train loss: 0.674 train acc: 0.555\n",
      "[24,     8] train loss: 0.656 train acc: 0.624\n",
      "[24] val loss: 0.661 val acc: 0.622\n",
      "[25,     1] train loss: 0.685 train acc: 0.570\n",
      "[25,     2] train loss: 0.669 train acc: 0.582\n",
      "[25,     3] train loss: 0.644 train acc: 0.617\n",
      "[25,     4] train loss: 0.704 train acc: 0.547\n",
      "[25,     5] train loss: 0.678 train acc: 0.578\n",
      "[25,     6] train loss: 0.661 train acc: 0.594\n",
      "[25,     7] train loss: 0.662 train acc: 0.539\n",
      "[25,     8] train loss: 0.672 train acc: 0.607\n",
      "[25] val loss: 0.664 val acc: 0.654\n",
      "[26,     1] train loss: 0.673 train acc: 0.539\n",
      "[26,     2] train loss: 0.680 train acc: 0.555\n",
      "[26,     3] train loss: 0.652 train acc: 0.609\n",
      "[26,     4] train loss: 0.657 train acc: 0.656\n",
      "[26,     5] train loss: 0.681 train acc: 0.547\n",
      "[26,     6] train loss: 0.674 train acc: 0.570\n",
      "[26,     7] train loss: 0.671 train acc: 0.566\n",
      "[26,     8] train loss: 0.684 train acc: 0.533\n",
      "[26] val loss: 0.662 val acc: 0.645\n",
      "[27,     1] train loss: 0.649 train acc: 0.617\n",
      "[27,     2] train loss: 0.672 train acc: 0.566\n",
      "[27,     3] train loss: 0.659 train acc: 0.633\n",
      "[27,     4] train loss: 0.677 train acc: 0.574\n",
      "[27,     5] train loss: 0.652 train acc: 0.641\n",
      "[27,     6] train loss: 0.671 train acc: 0.547\n",
      "[27,     7] train loss: 0.655 train acc: 0.629\n",
      "[27,     8] train loss: 0.672 train acc: 0.574\n",
      "[27] val loss: 0.650 val acc: 0.654\n",
      "[28,     1] train loss: 0.659 train acc: 0.617\n",
      "[28,     2] train loss: 0.649 train acc: 0.613\n",
      "[28,     3] train loss: 0.652 train acc: 0.578\n",
      "[28,     4] train loss: 0.652 train acc: 0.598\n",
      "[28,     5] train loss: 0.666 train acc: 0.586\n",
      "[28,     6] train loss: 0.671 train acc: 0.594\n",
      "[28,     7] train loss: 0.660 train acc: 0.574\n",
      "[28,     8] train loss: 0.649 train acc: 0.599\n",
      "[28] val loss: 0.648 val acc: 0.647\n",
      "[29,     1] train loss: 0.702 train acc: 0.539\n",
      "[29,     2] train loss: 0.669 train acc: 0.582\n",
      "[29,     3] train loss: 0.668 train acc: 0.578\n",
      "[29,     4] train loss: 0.672 train acc: 0.574\n",
      "[29,     5] train loss: 0.636 train acc: 0.637\n",
      "[29,     6] train loss: 0.651 train acc: 0.594\n",
      "[29,     7] train loss: 0.665 train acc: 0.598\n",
      "[29,     8] train loss: 0.674 train acc: 0.537\n",
      "[29] val loss: 0.647 val acc: 0.675\n",
      "[30,     1] train loss: 0.670 train acc: 0.539\n",
      "[30,     2] train loss: 0.672 train acc: 0.605\n",
      "[30,     3] train loss: 0.640 train acc: 0.621\n",
      "[30,     4] train loss: 0.653 train acc: 0.621\n",
      "[30,     5] train loss: 0.650 train acc: 0.582\n",
      "[30,     6] train loss: 0.648 train acc: 0.625\n",
      "[30,     7] train loss: 0.666 train acc: 0.602\n",
      "[30,     8] train loss: 0.671 train acc: 0.583\n",
      "[30] val loss: 0.636 val acc: 0.678\n",
      "[31,     1] train loss: 0.641 train acc: 0.633\n",
      "[31,     2] train loss: 0.656 train acc: 0.633\n",
      "[31,     3] train loss: 0.646 train acc: 0.605\n",
      "[31,     4] train loss: 0.645 train acc: 0.664\n",
      "[31,     5] train loss: 0.649 train acc: 0.625\n",
      "[31,     6] train loss: 0.672 train acc: 0.590\n",
      "[31,     7] train loss: 0.634 train acc: 0.625\n",
      "[31,     8] train loss: 0.678 train acc: 0.558\n",
      "[31] val loss: 0.640 val acc: 0.679\n",
      "[32,     1] train loss: 0.681 train acc: 0.613\n",
      "[32,     2] train loss: 0.664 train acc: 0.629\n",
      "[32,     3] train loss: 0.654 train acc: 0.582\n",
      "[32,     4] train loss: 0.662 train acc: 0.625\n",
      "[32,     5] train loss: 0.667 train acc: 0.586\n",
      "[32,     6] train loss: 0.672 train acc: 0.598\n",
      "[32,     7] train loss: 0.652 train acc: 0.641\n",
      "[32,     8] train loss: 0.648 train acc: 0.603\n",
      "[32] val loss: 0.643 val acc: 0.690\n",
      "[33,     1] train loss: 0.652 train acc: 0.609\n",
      "[33,     2] train loss: 0.652 train acc: 0.621\n",
      "[33,     3] train loss: 0.660 train acc: 0.605\n",
      "[33,     4] train loss: 0.650 train acc: 0.637\n",
      "[33,     5] train loss: 0.631 train acc: 0.637\n",
      "[33,     6] train loss: 0.643 train acc: 0.660\n",
      "[33,     7] train loss: 0.631 train acc: 0.629\n",
      "[33,     8] train loss: 0.655 train acc: 0.603\n",
      "[33] val loss: 0.624 val acc: 0.695\n",
      "[34,     1] train loss: 0.657 train acc: 0.617\n",
      "[34,     2] train loss: 0.649 train acc: 0.602\n",
      "[34,     3] train loss: 0.661 train acc: 0.586\n",
      "[34,     4] train loss: 0.631 train acc: 0.660\n",
      "[34,     5] train loss: 0.630 train acc: 0.652\n",
      "[34,     6] train loss: 0.637 train acc: 0.656\n",
      "[34,     7] train loss: 0.636 train acc: 0.637\n",
      "[34,     8] train loss: 0.645 train acc: 0.595\n",
      "[34] val loss: 0.623 val acc: 0.694\n",
      "[35,     1] train loss: 0.661 train acc: 0.578\n",
      "[35,     2] train loss: 0.673 train acc: 0.598\n",
      "[35,     3] train loss: 0.649 train acc: 0.617\n",
      "[35,     4] train loss: 0.656 train acc: 0.660\n",
      "[35,     5] train loss: 0.643 train acc: 0.641\n",
      "[35,     6] train loss: 0.628 train acc: 0.672\n",
      "[35,     7] train loss: 0.636 train acc: 0.613\n",
      "[35,     8] train loss: 0.635 train acc: 0.669\n",
      "[35] val loss: 0.630 val acc: 0.709\n",
      "[36,     1] train loss: 0.654 train acc: 0.574\n",
      "[36,     2] train loss: 0.690 train acc: 0.547\n",
      "[36,     3] train loss: 0.651 train acc: 0.586\n",
      "[36,     4] train loss: 0.627 train acc: 0.617\n",
      "[36,     5] train loss: 0.657 train acc: 0.641\n",
      "[36,     6] train loss: 0.649 train acc: 0.633\n",
      "[36,     7] train loss: 0.649 train acc: 0.586\n",
      "[36,     8] train loss: 0.646 train acc: 0.607\n",
      "[36] val loss: 0.623 val acc: 0.713\n",
      "[37,     1] train loss: 0.643 train acc: 0.594\n",
      "[37,     2] train loss: 0.611 train acc: 0.699\n",
      "[37,     3] train loss: 0.622 train acc: 0.617\n",
      "[37,     4] train loss: 0.632 train acc: 0.629\n",
      "[37,     5] train loss: 0.659 train acc: 0.559\n",
      "[37,     6] train loss: 0.626 train acc: 0.621\n",
      "[37,     7] train loss: 0.619 train acc: 0.652\n",
      "[37,     8] train loss: 0.621 train acc: 0.661\n",
      "[37] val loss: 0.608 val acc: 0.705\n",
      "[38,     1] train loss: 0.622 train acc: 0.652\n",
      "[38,     2] train loss: 0.667 train acc: 0.625\n",
      "[38,     3] train loss: 0.649 train acc: 0.652\n",
      "[38,     4] train loss: 0.626 train acc: 0.602\n",
      "[38,     5] train loss: 0.669 train acc: 0.609\n",
      "[38,     6] train loss: 0.611 train acc: 0.688\n",
      "[38,     7] train loss: 0.658 train acc: 0.609\n",
      "[38,     8] train loss: 0.637 train acc: 0.674\n",
      "[38] val loss: 0.617 val acc: 0.724\n",
      "[39,     1] train loss: 0.624 train acc: 0.645\n",
      "[39,     2] train loss: 0.648 train acc: 0.602\n",
      "[39,     3] train loss: 0.645 train acc: 0.609\n",
      "[39,     4] train loss: 0.656 train acc: 0.609\n",
      "[39,     5] train loss: 0.659 train acc: 0.629\n",
      "[39,     6] train loss: 0.631 train acc: 0.648\n",
      "[39,     7] train loss: 0.660 train acc: 0.609\n",
      "[39,     8] train loss: 0.629 train acc: 0.665\n",
      "[39] val loss: 0.614 val acc: 0.733\n",
      "[40,     1] train loss: 0.631 train acc: 0.641\n",
      "[40,     2] train loss: 0.623 train acc: 0.660\n",
      "[40,     3] train loss: 0.601 train acc: 0.680\n",
      "[40,     4] train loss: 0.613 train acc: 0.645\n",
      "[40,     5] train loss: 0.620 train acc: 0.676\n",
      "[40,     6] train loss: 0.648 train acc: 0.617\n",
      "[40,     7] train loss: 0.678 train acc: 0.566\n",
      "[40,     8] train loss: 0.641 train acc: 0.636\n",
      "[40] val loss: 0.597 val acc: 0.736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41,     1] train loss: 0.608 train acc: 0.691\n",
      "[41,     2] train loss: 0.619 train acc: 0.648\n",
      "[41,     3] train loss: 0.620 train acc: 0.641\n",
      "[41,     4] train loss: 0.646 train acc: 0.602\n",
      "[41,     5] train loss: 0.595 train acc: 0.680\n",
      "[41,     6] train loss: 0.651 train acc: 0.625\n",
      "[41,     7] train loss: 0.611 train acc: 0.668\n",
      "[41,     8] train loss: 0.652 train acc: 0.612\n",
      "[41] val loss: 0.596 val acc: 0.742\n",
      "[42,     1] train loss: 0.630 train acc: 0.609\n",
      "[42,     2] train loss: 0.634 train acc: 0.660\n",
      "[42,     3] train loss: 0.628 train acc: 0.625\n",
      "[42,     4] train loss: 0.606 train acc: 0.664\n",
      "[42,     5] train loss: 0.604 train acc: 0.641\n",
      "[42,     6] train loss: 0.618 train acc: 0.629\n",
      "[42,     7] train loss: 0.624 train acc: 0.617\n",
      "[42,     8] train loss: 0.618 train acc: 0.682\n",
      "[42] val loss: 0.584 val acc: 0.747\n",
      "[43,     1] train loss: 0.572 train acc: 0.719\n",
      "[43,     2] train loss: 0.604 train acc: 0.645\n",
      "[43,     3] train loss: 0.666 train acc: 0.637\n",
      "[43,     4] train loss: 0.657 train acc: 0.602\n",
      "[43,     5] train loss: 0.589 train acc: 0.703\n",
      "[43,     6] train loss: 0.611 train acc: 0.637\n",
      "[43,     7] train loss: 0.650 train acc: 0.641\n",
      "[43,     8] train loss: 0.613 train acc: 0.649\n",
      "[43] val loss: 0.591 val acc: 0.742\n",
      "[44,     1] train loss: 0.636 train acc: 0.645\n",
      "[44,     2] train loss: 0.663 train acc: 0.590\n",
      "[44,     3] train loss: 0.649 train acc: 0.645\n",
      "[44,     4] train loss: 0.611 train acc: 0.668\n",
      "[44,     5] train loss: 0.591 train acc: 0.707\n",
      "[44,     6] train loss: 0.630 train acc: 0.621\n",
      "[44,     7] train loss: 0.616 train acc: 0.645\n",
      "[44,     8] train loss: 0.637 train acc: 0.599\n",
      "[44] val loss: 0.592 val acc: 0.761\n",
      "[45,     1] train loss: 0.600 train acc: 0.684\n",
      "[45,     2] train loss: 0.625 train acc: 0.625\n",
      "[45,     3] train loss: 0.635 train acc: 0.660\n",
      "[45,     4] train loss: 0.595 train acc: 0.664\n",
      "[45,     5] train loss: 0.622 train acc: 0.664\n",
      "[45,     6] train loss: 0.626 train acc: 0.645\n",
      "[45,     7] train loss: 0.622 train acc: 0.637\n",
      "[45,     8] train loss: 0.628 train acc: 0.665\n",
      "[45] val loss: 0.578 val acc: 0.757\n",
      "[46,     1] train loss: 0.635 train acc: 0.672\n",
      "[46,     2] train loss: 0.626 train acc: 0.629\n",
      "[46,     3] train loss: 0.604 train acc: 0.652\n",
      "[46,     4] train loss: 0.608 train acc: 0.672\n",
      "[46,     5] train loss: 0.639 train acc: 0.633\n",
      "[46,     6] train loss: 0.598 train acc: 0.691\n",
      "[46,     7] train loss: 0.607 train acc: 0.660\n",
      "[46,     8] train loss: 0.585 train acc: 0.707\n",
      "[46] val loss: 0.589 val acc: 0.739\n",
      "[47,     1] train loss: 0.603 train acc: 0.684\n",
      "[47,     2] train loss: 0.611 train acc: 0.625\n",
      "[47,     3] train loss: 0.606 train acc: 0.688\n",
      "[47,     4] train loss: 0.616 train acc: 0.652\n",
      "[47,     5] train loss: 0.612 train acc: 0.672\n",
      "[47,     6] train loss: 0.596 train acc: 0.688\n",
      "[47,     7] train loss: 0.613 train acc: 0.633\n",
      "[47,     8] train loss: 0.603 train acc: 0.661\n",
      "[47] val loss: 0.569 val acc: 0.781\n",
      "[48,     1] train loss: 0.562 train acc: 0.715\n",
      "[48,     2] train loss: 0.582 train acc: 0.684\n",
      "[48,     3] train loss: 0.568 train acc: 0.699\n",
      "[48,     4] train loss: 0.636 train acc: 0.621\n",
      "[48,     5] train loss: 0.601 train acc: 0.668\n",
      "[48,     6] train loss: 0.614 train acc: 0.664\n",
      "[48,     7] train loss: 0.575 train acc: 0.680\n",
      "[48,     8] train loss: 0.601 train acc: 0.698\n",
      "[48] val loss: 0.555 val acc: 0.780\n",
      "[49,     1] train loss: 0.614 train acc: 0.629\n",
      "[49,     2] train loss: 0.603 train acc: 0.660\n",
      "[49,     3] train loss: 0.572 train acc: 0.719\n",
      "[49,     4] train loss: 0.608 train acc: 0.672\n",
      "[49,     5] train loss: 0.622 train acc: 0.664\n",
      "[49,     6] train loss: 0.592 train acc: 0.680\n",
      "[49,     7] train loss: 0.594 train acc: 0.664\n",
      "[49,     8] train loss: 0.602 train acc: 0.636\n",
      "[49] val loss: 0.561 val acc: 0.806\n",
      "[50,     1] train loss: 0.580 train acc: 0.688\n",
      "[50,     2] train loss: 0.559 train acc: 0.711\n",
      "[50,     3] train loss: 0.605 train acc: 0.684\n",
      "[50,     4] train loss: 0.567 train acc: 0.699\n",
      "[50,     5] train loss: 0.608 train acc: 0.676\n",
      "[50,     6] train loss: 0.602 train acc: 0.680\n",
      "[50,     7] train loss: 0.564 train acc: 0.707\n",
      "[50,     8] train loss: 0.573 train acc: 0.678\n",
      "[50] val loss: 0.539 val acc: 0.798\n",
      "[51,     1] train loss: 0.546 train acc: 0.688\n",
      "[51,     2] train loss: 0.572 train acc: 0.691\n",
      "[51,     3] train loss: 0.602 train acc: 0.664\n",
      "[51,     4] train loss: 0.621 train acc: 0.637\n",
      "[51,     5] train loss: 0.594 train acc: 0.668\n",
      "[51,     6] train loss: 0.621 train acc: 0.617\n",
      "[51,     7] train loss: 0.600 train acc: 0.672\n",
      "[51,     8] train loss: 0.581 train acc: 0.698\n",
      "[51] val loss: 0.557 val acc: 0.777\n",
      "[52,     1] train loss: 0.603 train acc: 0.719\n",
      "[52,     2] train loss: 0.565 train acc: 0.715\n",
      "[52,     3] train loss: 0.619 train acc: 0.664\n",
      "[52,     4] train loss: 0.561 train acc: 0.727\n",
      "[52,     5] train loss: 0.592 train acc: 0.680\n",
      "[52,     6] train loss: 0.595 train acc: 0.691\n",
      "[52,     7] train loss: 0.591 train acc: 0.641\n",
      "[52,     8] train loss: 0.567 train acc: 0.711\n",
      "[52] val loss: 0.537 val acc: 0.807\n",
      "[53,     1] train loss: 0.568 train acc: 0.699\n",
      "[53,     2] train loss: 0.590 train acc: 0.688\n",
      "[53,     3] train loss: 0.562 train acc: 0.703\n",
      "[53,     4] train loss: 0.628 train acc: 0.668\n",
      "[53,     5] train loss: 0.593 train acc: 0.668\n",
      "[53,     6] train loss: 0.628 train acc: 0.645\n",
      "[53,     7] train loss: 0.570 train acc: 0.699\n",
      "[53,     8] train loss: 0.580 train acc: 0.674\n",
      "[53] val loss: 0.543 val acc: 0.811\n",
      "[54,     1] train loss: 0.567 train acc: 0.691\n",
      "[54,     2] train loss: 0.596 train acc: 0.637\n",
      "[54,     3] train loss: 0.550 train acc: 0.703\n",
      "[54,     4] train loss: 0.568 train acc: 0.691\n",
      "[54,     5] train loss: 0.597 train acc: 0.664\n",
      "[54,     6] train loss: 0.570 train acc: 0.652\n",
      "[54,     7] train loss: 0.593 train acc: 0.672\n",
      "[54,     8] train loss: 0.581 train acc: 0.698\n",
      "[54] val loss: 0.528 val acc: 0.803\n",
      "[55,     1] train loss: 0.601 train acc: 0.668\n",
      "[55,     2] train loss: 0.580 train acc: 0.680\n",
      "[55,     3] train loss: 0.529 train acc: 0.727\n",
      "[55,     4] train loss: 0.544 train acc: 0.699\n",
      "[55,     5] train loss: 0.564 train acc: 0.742\n",
      "[55,     6] train loss: 0.606 train acc: 0.656\n",
      "[55,     7] train loss: 0.583 train acc: 0.660\n",
      "[55,     8] train loss: 0.582 train acc: 0.674\n",
      "[55] val loss: 0.515 val acc: 0.800\n",
      "[56,     1] train loss: 0.608 train acc: 0.684\n",
      "[56,     2] train loss: 0.585 train acc: 0.680\n",
      "[56,     3] train loss: 0.563 train acc: 0.680\n",
      "[56,     4] train loss: 0.568 train acc: 0.676\n",
      "[56,     5] train loss: 0.557 train acc: 0.730\n",
      "[56,     6] train loss: 0.518 train acc: 0.742\n",
      "[56,     7] train loss: 0.550 train acc: 0.727\n",
      "[56,     8] train loss: 0.560 train acc: 0.711\n",
      "[56] val loss: 0.518 val acc: 0.814\n",
      "[57,     1] train loss: 0.569 train acc: 0.707\n",
      "[57,     2] train loss: 0.520 train acc: 0.758\n",
      "[57,     3] train loss: 0.596 train acc: 0.648\n",
      "[57,     4] train loss: 0.576 train acc: 0.699\n",
      "[57,     5] train loss: 0.574 train acc: 0.711\n",
      "[57,     6] train loss: 0.593 train acc: 0.703\n",
      "[57,     7] train loss: 0.583 train acc: 0.711\n",
      "[57,     8] train loss: 0.611 train acc: 0.649\n",
      "[57] val loss: 0.513 val acc: 0.824\n",
      "[58,     1] train loss: 0.563 train acc: 0.711\n",
      "[58,     2] train loss: 0.504 train acc: 0.773\n",
      "[58,     3] train loss: 0.582 train acc: 0.676\n",
      "[58,     4] train loss: 0.545 train acc: 0.742\n",
      "[58,     5] train loss: 0.582 train acc: 0.664\n",
      "[58,     6] train loss: 0.545 train acc: 0.730\n",
      "[58,     7] train loss: 0.563 train acc: 0.691\n",
      "[58,     8] train loss: 0.572 train acc: 0.682\n",
      "[58] val loss: 0.514 val acc: 0.838\n",
      "[59,     1] train loss: 0.550 train acc: 0.711\n",
      "[59,     2] train loss: 0.545 train acc: 0.688\n",
      "[59,     3] train loss: 0.628 train acc: 0.680\n",
      "[59,     4] train loss: 0.565 train acc: 0.707\n",
      "[59,     5] train loss: 0.547 train acc: 0.699\n",
      "[59,     6] train loss: 0.547 train acc: 0.711\n",
      "[59,     7] train loss: 0.599 train acc: 0.672\n",
      "[59,     8] train loss: 0.604 train acc: 0.645\n",
      "[59] val loss: 0.510 val acc: 0.852\n",
      "[60,     1] train loss: 0.529 train acc: 0.738\n",
      "[60,     2] train loss: 0.603 train acc: 0.672\n",
      "[60,     3] train loss: 0.620 train acc: 0.664\n",
      "[60,     4] train loss: 0.580 train acc: 0.703\n",
      "[60,     5] train loss: 0.561 train acc: 0.680\n",
      "[60,     6] train loss: 0.551 train acc: 0.711\n",
      "[60,     7] train loss: 0.573 train acc: 0.707\n",
      "[60,     8] train loss: 0.568 train acc: 0.711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60] val loss: 0.532 val acc: 0.815\n",
      "[61,     1] train loss: 0.538 train acc: 0.684\n",
      "[61,     2] train loss: 0.598 train acc: 0.672\n",
      "[61,     3] train loss: 0.594 train acc: 0.668\n",
      "[61,     4] train loss: 0.572 train acc: 0.668\n",
      "[61,     5] train loss: 0.589 train acc: 0.672\n",
      "[61,     6] train loss: 0.517 train acc: 0.742\n",
      "[61,     7] train loss: 0.494 train acc: 0.742\n",
      "[61,     8] train loss: 0.560 train acc: 0.682\n",
      "[61] val loss: 0.497 val acc: 0.856\n",
      "[62,     1] train loss: 0.556 train acc: 0.699\n",
      "[62,     2] train loss: 0.599 train acc: 0.645\n",
      "[62,     3] train loss: 0.578 train acc: 0.668\n",
      "[62,     4] train loss: 0.545 train acc: 0.719\n",
      "[62,     5] train loss: 0.562 train acc: 0.715\n",
      "[62,     6] train loss: 0.510 train acc: 0.730\n",
      "[62,     7] train loss: 0.523 train acc: 0.770\n",
      "[62,     8] train loss: 0.530 train acc: 0.752\n",
      "[62] val loss: 0.488 val acc: 0.842\n",
      "[63,     1] train loss: 0.541 train acc: 0.723\n",
      "[63,     2] train loss: 0.574 train acc: 0.719\n",
      "[63,     3] train loss: 0.483 train acc: 0.766\n",
      "[63,     4] train loss: 0.561 train acc: 0.730\n",
      "[63,     5] train loss: 0.576 train acc: 0.699\n",
      "[63,     6] train loss: 0.516 train acc: 0.750\n",
      "[63,     7] train loss: 0.526 train acc: 0.727\n",
      "[63,     8] train loss: 0.533 train acc: 0.744\n",
      "[63] val loss: 0.480 val acc: 0.865\n",
      "[64,     1] train loss: 0.526 train acc: 0.742\n",
      "[64,     2] train loss: 0.548 train acc: 0.684\n",
      "[64,     3] train loss: 0.566 train acc: 0.711\n",
      "[64,     4] train loss: 0.513 train acc: 0.711\n",
      "[64,     5] train loss: 0.549 train acc: 0.715\n",
      "[64,     6] train loss: 0.541 train acc: 0.727\n",
      "[64,     7] train loss: 0.569 train acc: 0.711\n",
      "[64,     8] train loss: 0.521 train acc: 0.715\n",
      "[64] val loss: 0.470 val acc: 0.874\n",
      "[65,     1] train loss: 0.520 train acc: 0.730\n",
      "[65,     2] train loss: 0.520 train acc: 0.734\n",
      "[65,     3] train loss: 0.540 train acc: 0.746\n",
      "[65,     4] train loss: 0.519 train acc: 0.727\n",
      "[65,     5] train loss: 0.507 train acc: 0.719\n",
      "[65,     6] train loss: 0.613 train acc: 0.656\n",
      "[65,     7] train loss: 0.516 train acc: 0.754\n",
      "[65,     8] train loss: 0.497 train acc: 0.773\n",
      "[65] val loss: 0.468 val acc: 0.873\n",
      "[66,     1] train loss: 0.573 train acc: 0.680\n",
      "[66,     2] train loss: 0.532 train acc: 0.727\n",
      "[66,     3] train loss: 0.516 train acc: 0.699\n",
      "[66,     4] train loss: 0.539 train acc: 0.719\n",
      "[66,     5] train loss: 0.511 train acc: 0.730\n",
      "[66,     6] train loss: 0.535 train acc: 0.734\n",
      "[66,     7] train loss: 0.514 train acc: 0.762\n",
      "[66,     8] train loss: 0.609 train acc: 0.690\n",
      "[66] val loss: 0.464 val acc: 0.887\n",
      "[67,     1] train loss: 0.512 train acc: 0.730\n",
      "[67,     2] train loss: 0.545 train acc: 0.723\n",
      "[67,     3] train loss: 0.546 train acc: 0.711\n",
      "[67,     4] train loss: 0.527 train acc: 0.730\n",
      "[67,     5] train loss: 0.541 train acc: 0.738\n",
      "[67,     6] train loss: 0.568 train acc: 0.711\n",
      "[67,     7] train loss: 0.522 train acc: 0.750\n",
      "[67,     8] train loss: 0.523 train acc: 0.748\n",
      "[67] val loss: 0.475 val acc: 0.875\n",
      "[68,     1] train loss: 0.484 train acc: 0.762\n",
      "[68,     2] train loss: 0.504 train acc: 0.746\n",
      "[68,     3] train loss: 0.502 train acc: 0.742\n",
      "[68,     4] train loss: 0.549 train acc: 0.711\n",
      "[68,     5] train loss: 0.513 train acc: 0.711\n",
      "[68,     6] train loss: 0.589 train acc: 0.664\n",
      "[68,     7] train loss: 0.569 train acc: 0.723\n",
      "[68,     8] train loss: 0.553 train acc: 0.744\n",
      "[68] val loss: 0.455 val acc: 0.890\n",
      "[69,     1] train loss: 0.543 train acc: 0.691\n",
      "[69,     2] train loss: 0.500 train acc: 0.758\n",
      "[69,     3] train loss: 0.538 train acc: 0.711\n",
      "[69,     4] train loss: 0.529 train acc: 0.754\n",
      "[69,     5] train loss: 0.514 train acc: 0.754\n",
      "[69,     6] train loss: 0.510 train acc: 0.766\n",
      "[69,     7] train loss: 0.493 train acc: 0.738\n",
      "[69,     8] train loss: 0.522 train acc: 0.752\n",
      "[69] val loss: 0.452 val acc: 0.906\n",
      "[70,     1] train loss: 0.499 train acc: 0.742\n",
      "[70,     2] train loss: 0.530 train acc: 0.742\n",
      "[70,     3] train loss: 0.474 train acc: 0.777\n",
      "[70,     4] train loss: 0.461 train acc: 0.785\n",
      "[70,     5] train loss: 0.586 train acc: 0.680\n",
      "[70,     6] train loss: 0.472 train acc: 0.773\n",
      "[70,     7] train loss: 0.543 train acc: 0.727\n",
      "[70,     8] train loss: 0.469 train acc: 0.764\n",
      "[70] val loss: 0.423 val acc: 0.905\n",
      "[71,     1] train loss: 0.531 train acc: 0.699\n",
      "[71,     2] train loss: 0.558 train acc: 0.727\n",
      "[71,     3] train loss: 0.558 train acc: 0.691\n",
      "[71,     4] train loss: 0.547 train acc: 0.734\n",
      "[71,     5] train loss: 0.547 train acc: 0.707\n",
      "[71,     6] train loss: 0.511 train acc: 0.734\n",
      "[71,     7] train loss: 0.495 train acc: 0.723\n",
      "[71,     8] train loss: 0.482 train acc: 0.781\n",
      "[71] val loss: 0.455 val acc: 0.916\n",
      "[72,     1] train loss: 0.507 train acc: 0.734\n",
      "[72,     2] train loss: 0.477 train acc: 0.758\n",
      "[72,     3] train loss: 0.512 train acc: 0.738\n",
      "[72,     4] train loss: 0.503 train acc: 0.711\n",
      "[72,     5] train loss: 0.532 train acc: 0.746\n",
      "[72,     6] train loss: 0.455 train acc: 0.785\n",
      "[72,     7] train loss: 0.503 train acc: 0.738\n",
      "[72,     8] train loss: 0.531 train acc: 0.711\n",
      "[72] val loss: 0.419 val acc: 0.922\n",
      "[73,     1] train loss: 0.512 train acc: 0.723\n",
      "[73,     2] train loss: 0.458 train acc: 0.781\n",
      "[73,     3] train loss: 0.507 train acc: 0.762\n",
      "[73,     4] train loss: 0.494 train acc: 0.770\n",
      "[73,     5] train loss: 0.499 train acc: 0.758\n",
      "[73,     6] train loss: 0.502 train acc: 0.734\n",
      "[73,     7] train loss: 0.527 train acc: 0.707\n",
      "[73,     8] train loss: 0.492 train acc: 0.756\n",
      "[73] val loss: 0.414 val acc: 0.930\n",
      "[74,     1] train loss: 0.492 train acc: 0.730\n",
      "[74,     2] train loss: 0.495 train acc: 0.770\n",
      "[74,     3] train loss: 0.464 train acc: 0.758\n",
      "[74,     4] train loss: 0.491 train acc: 0.762\n",
      "[74,     5] train loss: 0.537 train acc: 0.770\n",
      "[74,     6] train loss: 0.528 train acc: 0.742\n",
      "[74,     7] train loss: 0.445 train acc: 0.781\n",
      "[74,     8] train loss: 0.482 train acc: 0.756\n",
      "[74] val loss: 0.411 val acc: 0.920\n",
      "[75,     1] train loss: 0.443 train acc: 0.781\n",
      "[75,     2] train loss: 0.506 train acc: 0.750\n",
      "[75,     3] train loss: 0.464 train acc: 0.793\n",
      "[75,     4] train loss: 0.519 train acc: 0.695\n",
      "[75,     5] train loss: 0.466 train acc: 0.742\n",
      "[75,     6] train loss: 0.469 train acc: 0.762\n",
      "[75,     7] train loss: 0.483 train acc: 0.789\n",
      "[75,     8] train loss: 0.475 train acc: 0.756\n",
      "[75] val loss: 0.386 val acc: 0.941\n",
      "[76,     1] train loss: 0.483 train acc: 0.750\n",
      "[76,     2] train loss: 0.472 train acc: 0.785\n",
      "[76,     3] train loss: 0.470 train acc: 0.789\n",
      "[76,     4] train loss: 0.506 train acc: 0.750\n",
      "[76,     5] train loss: 0.454 train acc: 0.797\n",
      "[76,     6] train loss: 0.441 train acc: 0.793\n",
      "[76,     7] train loss: 0.473 train acc: 0.742\n",
      "[76,     8] train loss: 0.526 train acc: 0.764\n",
      "[76] val loss: 0.384 val acc: 0.939\n",
      "[77,     1] train loss: 0.474 train acc: 0.738\n",
      "[77,     2] train loss: 0.437 train acc: 0.789\n",
      "[77,     3] train loss: 0.512 train acc: 0.734\n",
      "[77,     4] train loss: 0.467 train acc: 0.793\n",
      "[77,     5] train loss: 0.464 train acc: 0.785\n",
      "[77,     6] train loss: 0.474 train acc: 0.754\n",
      "[77,     7] train loss: 0.443 train acc: 0.758\n",
      "[77,     8] train loss: 0.479 train acc: 0.777\n",
      "[77] val loss: 0.368 val acc: 0.951\n",
      "[78,     1] train loss: 0.509 train acc: 0.754\n",
      "[78,     2] train loss: 0.578 train acc: 0.723\n",
      "[78,     3] train loss: 0.475 train acc: 0.766\n",
      "[78,     4] train loss: 0.478 train acc: 0.785\n",
      "[78,     5] train loss: 0.505 train acc: 0.738\n",
      "[78,     6] train loss: 0.467 train acc: 0.762\n",
      "[78,     7] train loss: 0.496 train acc: 0.754\n",
      "[78,     8] train loss: 0.496 train acc: 0.731\n",
      "[78] val loss: 0.401 val acc: 0.958\n",
      "[79,     1] train loss: 0.493 train acc: 0.762\n",
      "[79,     2] train loss: 0.507 train acc: 0.746\n",
      "[79,     3] train loss: 0.442 train acc: 0.785\n",
      "[79,     4] train loss: 0.448 train acc: 0.770\n",
      "[79,     5] train loss: 0.488 train acc: 0.781\n",
      "[79,     6] train loss: 0.463 train acc: 0.801\n",
      "[79,     7] train loss: 0.460 train acc: 0.762\n",
      "[79,     8] train loss: 0.517 train acc: 0.719\n",
      "[79] val loss: 0.376 val acc: 0.942\n",
      "[80,     1] train loss: 0.521 train acc: 0.730\n",
      "[80,     2] train loss: 0.494 train acc: 0.746\n",
      "[80,     3] train loss: 0.533 train acc: 0.715\n",
      "[80,     4] train loss: 0.510 train acc: 0.758\n",
      "[80,     5] train loss: 0.462 train acc: 0.789\n",
      "[80,     6] train loss: 0.487 train acc: 0.758\n",
      "[80,     7] train loss: 0.454 train acc: 0.781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80,     8] train loss: 0.490 train acc: 0.748\n",
      "[80] val loss: 0.389 val acc: 0.960\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MALDataset(Dataset):\n",
    "    def __init__(self, r_test_x, r_test_y):\n",
    "        self.r_test_x = r_test_x\n",
    "        self.r_test_y = r_test_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.r_test_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.from_numpy(np.array(r_test_x[idx]))\n",
    "        label = [[1,0],[0,1]]\n",
    "        label = torch.from_numpy(np.array(label[r_test_y[idx]], dtype='float32'))\n",
    "        return data, label\n",
    "\n",
    "ratio = 0.7\n",
    "\n",
    "trainset = MALDataset(r_test_x[:int(len(r_test_x)*ratio)], r_test_y[:int(len(r_test_y)*ratio)])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "valset = MALDataset(r_test_x[int(len(r_test_x)*ratio):], r_test_y[int(len(r_test_y)*ratio):])\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "# Define a Loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "#Train the network\n",
    "for epoch in range(80):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_tot_acc = 0\n",
    "    train_tot_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        outputs = outputs[:,1] > outputs[:,0]\n",
    "        labels = labels[:,1] > labels[:,0]\n",
    "        acc = np.sum(outputs.astype(\"int32\") == labels.astype(\"int32\"))/len(labels)\n",
    "#         print(outputs.astype('int32'), labels.astype(\"int32\"))\n",
    "        train_acc += acc\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        train_tot_loss += loss.item()\n",
    "        train_tot_acc += acc\n",
    "        if i % 1 == 0: \n",
    "            print('[%d, %5d] train loss: %.3f train acc: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1, train_acc/ 1))\n",
    "            running_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "    \n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    for i, data in enumerate(valloader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        outputs = outputs[:,1] > outputs[:,0]\n",
    "        labels = labels[:,1] > labels[:,0]\n",
    "        acc = np.sum(outputs.astype(\"int32\") == labels.astype(\"int32\"))/len(labels)\n",
    "        val_acc += acc\n",
    "    print('[%d] val loss: %.3f val acc: %.3f' %\n",
    "              (epoch + 1, val_loss / len(valloader), val_acc/len(valloader)))\n",
    "    train_accs.append(train_tot_acc/len(trainloader))\n",
    "    train_losses.append(train_tot_loss/len(trainloader))\n",
    "    val_accs.append(val_acc / len(valloader))\n",
    "    val_losses.append(val_loss / len(valloader))\n",
    "\n",
    "PATH = './Path/dlmal_e_net.pth'\n",
    "torch.save(net, PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94891e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABIRUlEQVR4nO3dd3iUVfbA8e9JJyEkEFqAhIRepIdepFgAFWxIEZUVRV0UsO3a17a7+tPVteKq2BVEulgQpAjSS+gt1CS0JKSSntzfH+8QkpDAAJlMkjmf58mTmbfNySSZ87733vdcMcaglFLKdbk5OwCllFLOpYlAKaVcnCYCpZRycZoIlFLKxWkiUEopF6eJQCmlXJwmAuVSROQLEXnVzm0Pi8g1jo5JKWfTRKCUUi5OE4FSlZCIeDg7BlV1aCJQFY6tSeZJEdkmImdEZJqI1BORX0QkVUSWiEjNQtsPE5GdIpIkIstFpHWhdZ1EZLNtv+8Bn2KvdaOIRNr2XS0i7e2M8QYR2SIiKSISLSIvFlvfx3a8JNv6cbbl1UTkPyJyRESSRWSVbVl/EYkp4X24xvb4RRGZJSLfiEgKME5EuonIGttrHBeR90XEq9D+bUVksYicFpGTIvKMiNQXkXQRCSq0XWcRiRMRT3t+dlX1aCJQFdVtwLVAC+Am4BfgGaAO1t/tJAARaQFMB6bY1v0M/CgiXrYPxXnA10At4AfbcbHt2wn4DHgACAL+BywQEW874jsD3A0EAjcAD4nIzbbjNrbF+54tpo5ApG2/N4EuQC9bTH8D8u18T4YDs2yv+S2QBzwK1AZ6AoOAv9pi8AeWAL8CDYBmwO/GmBPAcuCOQse9C5hhjMmxMw5VxWgiUBXVe8aYk8aYWGAlsM4Ys8UYkwnMBTrZthsJ/GSMWWz7IHsTqIb1QdsD8AT+a4zJMcbMAjYUeo0JwP+MMeuMMXnGmC+BLNt+F2SMWW6M2W6MyTfGbMNKRlfbVo8BlhhjptteN8EYEykibsC9wGRjTKztNVcbY7LsfE/WGGPm2V4zwxizyRiz1hiTa4w5jJXIzsZwI3DCGPMfY0ymMSbVGLPOtu5LYCyAiLgDo7GSpXJRmghURXWy0OOMEp5Xtz1uABw5u8IYkw9EAw1t62JN0cqKRwo9bgw8bmtaSRKRJCDEtt8FiUh3EVlma1JJBh7EOjPHdowDJexWG6tpqqR19oguFkMLEVkoIidszUX/siMGgPlAGxEJx7rqSjbGrL/MmFQVoIlAVXbHsD7QARARwfoQjAWOAw1ty84KLfQ4GvinMSaw0JevMWa6Ha/7HbAACDHGBAAfAWdfJxpoWsI+8UBmKevOAL6Ffg53rGalwoqXCp4K7AGaG2NqYDWdFY6hSUmB266qZmJdFdyFXg24PE0EqrKbCdwgIoNsnZ2PYzXvrAbWALnAJBHxFJFbgW6F9v0EeNB2di8i4mfrBPa343X9gdPGmEwR6YbVHHTWt8A1InKHiHiISJCIdLRdrXwGvCUiDUTEXUR62vok9gE+ttf3BJ4DLtZX4Q+kAGki0gp4qNC6hUCwiEwREW8R8ReR7oXWfwWMA4ahicDlaSJQlZoxZi/Wme17WGfcNwE3GWOyjTHZwK1YH3insfoT5hTadyNwP/A+kAhE2ba1x1+Bl0UkFXgBKyGdPe5RYChWUjqN1VHcwbb6CWA7Vl/FaeB1wM0Yk2w75qdYVzNngCKjiErwBFYCSsVKat8XiiEVq9nnJuAEsB8YUGj9n1id1JuNMYWby5QLEp2YRinXJCJLge+MMZ86OxblXJoIlHJBItIVWIzVx5Hq7HiUc2nTkFIuRkS+xLrHYIomAQV6RaCUUi5PrwiUUsrFVbrCVbVr1zZhYWHODkMppSqVTZs2xRtjit+bAlTCRBAWFsbGjRudHYZSSlUqIlLqMGFtGlJKKReniUAppVycJgKllHJxla6PoCQ5OTnExMSQmZnp7FAqLR8fHxo1aoSnp85NopSrqRKJICYmBn9/f8LCwihaaFLZwxhDQkICMTExhIeHOzscpVQ5qxJNQ5mZmQQFBWkSuEwiQlBQkF5RKeWiqkQiADQJXCF9/5RyXVUmESilVJVlDCx6Fk7tdsjhNREopZSz5efDxs9h328lr9/9I6x5H45FOuTlNRGUgaSkJD788MNL3m/o0KEkJSWVfUBKqcoj/TRMHwULp8Ds8ZB2quj6/DxY9i8Iag7t73BICJoIykBpiSA3N/eC+/38888EBgY6KCqlVIUXsxH+1w8OLoN+f4OcDPj9paLb7JgDcbthwNPg5u6QMKrE8NHCXvpxJ7uOpZTpMds0qME/bmpb6vqnnnqKAwcO0LFjRzw9PfHx8aFmzZrs2bOHffv2cfPNNxMdHU1mZiaTJ09mwoQJwLm6SWlpaQwZMoQ+ffqwevVqGjZsyPz586lWrVqJr/fJJ5/w8ccfk52dTbNmzfj666/x9fXl5MmTPPjggxw8eBCAqVOn0qtXL7766ivefPNNRIT27dvz9dc6Ra1SThf5HSyYBDWC4d5F0LAz5GbC6nehy73QqAvk5cLyf0PdttDmFoeFolcEZeC1116jadOmREZG8sYbb7B582beeecd9u3bB8Bnn33Gpk2b2LhxI++++y4JCQnnHWP//v1MnDiRnTt3EhgYyOzZs0t9vVtvvZUNGzawdetWWrduzbRp0wCYNGkSV199NVu3bmXz5s20bduWnTt38uqrr7J06VK2bt3KO++845g3QSllv5iNVhJo3BMe+MNKAgD9noTq9eCXv1n9BttmwOkDMPBZcHPcx3WVuyK40Jl7eenWrVuRG7Peffdd5s6dC0B0dDT79+8nKCioyD7h4eF07NgRgC5dunD48OFSj79jxw6ee+45kpKSSEtL4/rrrwdg6dKlfPXVVwC4u7sTEBDAV199xYgRI6hduzYAtWrVKqsfUyl1OdJPww/jwD8YRnwJ1WqeW+dTA655CeY9CFu+hj/ehAadoOVQh4ZU5RJBReDn51fwePny5SxZsoQ1a9bg6+tL//79S7xxy9vbu+Cxu7s7GRkZpR5/3LhxzJs3jw4dOvDFF1+wfPnyMo1fKeUg+fkw9wFIPWE1B/mWcGLWfiRs+BQWPgomD258Gxx8n482DZUBf39/UlNLnvo1OTmZmjVr4uvry549e1i7du0Vv15qairBwcHk5OTw7bffFiwfNGgQU6dOBSAvL4/k5GQGDhzIDz/8UNAcdfr06St+faWUHdJPw6q3rSGhGYnWsj//C/t/g+v/ZfUBlMTNDYb+n5UEQnpAs0EOD1WvCMpAUFAQvXv35qqrrqJatWrUq1evYN3gwYP56KOPaN26NS1btqRHjx5X/HqvvPIK3bt3p06dOnTv3r0gCb3zzjtMmDCBadOm4e7uztSpU+nZsyfPPvssV199Ne7u7nTq1IkvvvjiimNQSl3Eyv9YY//PqtMa4vdC21ug2/0X3rdhFxg7B+q2dvjVAFTCyesjIiJM8RnKdu/eTevWrZ0UUdWh76NSZSQrFd5qA036Wx/6R9dB9FrIy4GR31h9AeVMRDYZYyJKWqdXBEopVdYiv4OsFOg9xWoCCu/n7IguSBNBBTZx4kT+/PPPIssmT57MX/7yFydFpJS6qPw8WDsVGnUrvR+ggtFEUIF98MEHzg5BKVWaY5Hw8xMQMR46jj63fN8iSDwE1/zDaaFdKk0ESil1qbZ8Awsfg/xciN0EXr7QZri1bu2HEBACrW5yboyXQIePKqWUvXKz4McpMH8ihPaAyZHQqCvMvg8OLIXj2+DwSug2Adwrz3l25YlUKaWcKfEIzPqLdQXQezIMfMH6sB/zPXxxI8wYC8EdwNMPOt/t7GgviV4RKKXUxexaAB/1hfj9cMdXcO3L5874q9W0xvxXrwtHV0OnO6FaoFPDvVSaCJygevXqzg5BKWWPnEz46QmYeRcENbUKxJ3tCyjMvx7cPR86jrWGjFYy2jSklFIlycmEr2+Go2ug58Mw6B/g4VX69jUbw82Vc6Rf1UsEvzwFJ7aX7THrt4Mhr5W6+qmnniIkJISJEycC8OKLL+Lh4cGyZctITEwkJyeHV199leHDSziTKCYtLY3hw4eXuF9J8wqUNgeBUuoK5OfDvIesJHDbNGh3u7MjcqiqlwicYOTIkUyZMqUgEcycOZNFixYxadIkatSoQXx8PD169GDYsGHIReqG+Pj4MHfu3PP227VrF6+++iqrV6+mdu3aBcXjzs5BMHfuXPLy8khLS3P4z6tUlbfsn7BzjlUSuoonAaiKieACZ+6O0qlTJ06dOsWxY8eIi4ujZs2a1K9fn0cffZQ//vgDNzc3YmNjOXnyJPXr17/gsYwxPPPMM+ftt3Tp0hLnFShpDgKl1BXY8i2sfNMa+dN7srOjKRcOTQQiMhh4B3AHPjXGvFZsfWPgM6AOcBoYa4yJcWRMjjJixAhmzZrFiRMnGDlyJN9++y1xcXFs2rQJT09PwsLCSpyHoLjL3U8pdRkiv4PFL1izgtVoYI382TrDKhZ3w1vlUvmzInDYqCERcQc+AIYAbYDRItKm2GZvAl8ZY9oDLwP/dlQ8jjZy5EhmzJjBrFmzGDFiBMnJydStWxdPT0+WLVvGkSNH7DpOafuVNq9ASXMQKKXskH4aFj0D1WpZdwKnnoC9v0LDCGuIqLunsyMsN468IugGRBljDgKIyAxgOLCr0DZtgMdsj5cB8xwYj0O1bduW1NRUGjZsSHBwMHfeeSc33XQT7dq1IyIiglatWtl1nNL2a9u2bYnzCpQ2B4FS6iKWvwaZyTDuJ6jn/Clunclh8xGIyO3AYGPMfbbndwHdjTEPF9rmO2CdMeYdEbkVmA3UNsYkFDvWBGACQGhoaJfiZ9daR79s6PuoXMap3TC1N0T8BW74j7OjKRcXmo/A2TeUPQFcLSJbgKuBWCCv+EbGmI+NMRHGmIg6deqUd4xKqarEGPj1afD2hwHPOjuaCsGRTUOxQEih541sywoYY44BtwKISHXgNmNMkgNjqjC2b9/OXXfdVWSZt7c369atc1JESrmIvb/AwWUw+PWSJ493QY5MBBuA5iISjpUARgFjCm8gIrWB08aYfOBprBFEl8UYc9Ex+hVJu3btiIyMdHYYBSrblKVKXZbcLPjtWajdErqOd3Y0FYbDmoaMMbnAw8AiYDcw0xizU0ReFpFhts36A3tFZB9QD/jn5byWj48PCQkJ+mF2mYwxJCQk4OPj4+xQlLpy2emlr/vzXTh9EAb/y6VGBV1MlZi8Picnh5iYGB1vfwV8fHxo1KgRnp76z6EqsYQD8HF/6DAKhr5RdF3cPvioN7S6EUZ87pTwnKnKT17v6elJeHi4s8NQSjlTfj4smGRNGr/+Y2tugE5jz637cRJ4+sKQ150bZwXk7FFDSilVNjZ/CUdWwY1vQ3g/ayrJY1usdZs+twrIXf8v6+5hVYQmAqVU5Zcca5WKCO8HXf4Ct39ufeB/f5dVjXjJixB+NXQcc9FDuSJNBEqpys0Y+OlxyMuBm96x6gP51bbKRKSdgk8G2tb912VqB12qKtFHoJRyEXk5MP9hiNlgzRMS3AFMHuz7Ba57FWo1Obdtw85w41vWRPPXvlJ0nSpCE4FSqnLIz4M5E6x5ApoOtNr/d82z1jXoDN0fOn+fTmOtJqGARuUaamWjiUApVfEZAwunWEng2pfPzROQkQgnd0KdVucmky8uMKTk5aqAJgKlVMVmDPz2HGz+Cvo+XnSymGo1IayP82KrIrSzWClVsW34FNa8D90mwMDnnR1NlaSJQClVceXnwZ/vQGgvq0icjvpxCE0ESqmKK2oJJEdD9wfATT+uHEXfWaWUc+XlwO6F1tl/cRs/B7+60OqG8o/LhWgiUEo516758P2dsO6josuTY2D/ImsIqFYKdShNBEopx8rPh6WvwrHIktdH2yZjWvpPSIo+t3zzV9aIoS73ODxEV6eJQCnlWJs+hz/egHX/K3l99HqoY5sr++cnrA//vFwrETQbBDXDyi1UV6X3ESilHCflmFXwDazKoMVlp8PJHdBrEvgGWbOH7ZpvNQWlHoehb5ZruK5KrwiUUo7z85OQlw3dH4Sko0WbfsAqE5GfCyHdrG2CO8Avf4c1H4B/MLQY7Jy4XYwmAqWUY+z+EfYshP5PnZsg5sifRbeJWW99b9TVKhFx0ztw5pS1Xae7Si8bocqUJgKlVNnLTIafnoB67aDnw1C3LfgEwuFizUPRG6yqoH61recNOkGPv4K7F3S+u9zDdlWaCJRSZW/Ji9aZ/bB3rfZ+Nzdo3LtoIjDGKifdqFvRfa99GSZt0WJx5UgTgVKqbO1fAhs/s87sG3Y+tzysNyQesjqQAZKOWMmiUbH51N3ctWx0OdNEoJQqO2cSYP5freGgxQvENe5tfT9s6yeI3mB9Dyl2RaDKnSYCpVTZMAZ+nGTNEXDbJ+DpU3R9/XbgHXBuGGnMevD0s/oPlFNpIlBKlY3Ib61RQgOfsz70i3Nzh9Aeha4I1ltNRzoyyOk0ESilrtzpg9b4/7C+1iih0oT1gYT9cPqQdSNZo67lF6MqlSYCpdSVyUyBmfeAuMPNU60z/9KE2foJ1nxw7kYy5XR6TaaUuny5WVbl0JM7YfSMiw/5rN8BvPxhy9fW84YRF95elQu9IlBKXZ78PJgzAQ79AcM/gBbXXXwfdw8I7Q65mVAzHKrXcXyc6qI0ESilLp0xVp/Arnlw7SvQcbT9+54dRqrNQhWGJgKl1KVb+yFs+AR6PQK9J13avuH9rO+aCCoMTQRKKWtI59yHIGbjxbeN3w9LXoKWQ+Galy/9tRp2gTu+ho5jL31f5RDaWayUqzIGon6HlW/C0TXWsl3zYdS30HRAyfvk58OCR6ybxW58+/ImlBeBNsMuP25V5vSKQKmqIiu19OkgizMGvrkVvr3NmiNgyBsweas1G9h3d1glpEuycZqVNK7/N/jXL6vIlZM5NBGIyGAR2SsiUSLyVAnrQ0VkmYhsEZFtIjLUkfEoVaUtegY+vtqa+zc//8LbntgGB5ZCn0etSp/dJ1hJYNxCa3KYmXfDlm+L7pN01Koq2nQgdBzjqJ9COYHDEoGIuAMfAEOANsBoEWlTbLPngJnGmE7AKOBDR8WjVJWWnQ475oJvbfjj/2D2vZCTUfr2e38BBHpMBA+vc8t9a8Fd86wO3fl/hfe7wW/PW+Wjf5xiXUnc+F+reUdVGY7sI+gGRBljDgKIyAxgOLCr0DYGqGF7HAAcc2A8SlVde3+G7FQY/Z01/ePif1hn8KO+K7kJZ89P1qidksbxe1eHMTNh0xfWcddOhdXvWuuGvAE1Gzv0R1Hlz5GJoCFQeILSGKB7sW1eBH4TkUcAP+Cakg4kIhOACQChoaFlHqhSld7WGVCjETTuY53N12oKc+6Hb2+HB1YWPYNPjrGahq55qfTjeXhD9wesr6xUOLAMUmKh632O/1lUuXN2Z/Fo4AtjTCNgKPC1iJwXkzHmY2NMhDEmok4dvRNRqSJST1rt/e3vODeKp/WNMPQNOLEdDi4vuv3eX6zvLe3skvP2t0b59Hjo8kYJqQrPkb/VWKBw4ZFGtmWFjQdmAhhj1gA+QG0HxqRU1bNjFpg86DCq6PKrbge/OrDuo6LL9/4MQc2gTovyi1FVaI5MBBuA5iISLiJeWJ3BC4ptcxQYBCAirbESQZwDY1Kq6tk6w5r0vU7Loss9fSBiPOz7FRIOWMsyU+DQSmg5pPzjVBWWwxKBMSYXeBhYBOzGGh20U0ReFpGzd5M8DtwvIluB6cA4Y4xxVExKVTknd1rt/R1KqfUTcS+4eZ67KjjwO+Tn2N8spFyCQ+8sNsb8DPxcbNkLhR7vAno7MgalqrStM8DNA666reT1/vWg3e3WPQEDnoU9P0O1WhBSfNyGcmXa86NUZZWfB9t/gGbXgt8Futa6Pwg5Z6zhoPsXQYvBF548RrkcrTWkVGWReBg+G2I17XhWs64EUo/D4H9feL8GHa3Sz8tfg9wMaKXNQqoovSJQqrKIWgKpx6wrgNBe1gTxHcfa197f/UErCbh7Q5NSCsopl6VXBEpVFtEbwK8u3PzhpZd4aHUD1GoCddtYdw4rVYgmAqUqi5j1VlmIy6nz4+YO45eAu2fZx6XsYowhMyefal4Vr39Gm4aUqgzS4uD0QWjU9fKP4RcEPjUuvp1yiJd+3EXHl3/jzUV7OZOV6+xwirArEYjIHBG5oaTyD0qpchCzwfquwz4rpV93nOCL1YdpHOTL+8uiGPDmcn7YGE1+fsW4bcreD/YPgTHAfhF5TURaXmwHpVQZillvjRJq0NHZkahLdCwpg7/P3ka7hgEsfKQvsx/qRYPAajw5axv3fL6+xGSQn294+cddLNl1slxitCsRGGOWGGPuBDoDh4ElIrJaRP4iItroqJSjRW+A+u2tYaOq0sjNy2fKjEhy8/J5b3QnvDzc6NK4JnMe6sVzN7Rm5f54vll35Lz9vlxzmM/+PMQj07cQdSrV4XHa3dQjIkHAOOA+YAvwDlZiWOyQyJRyBUv/CXMftKqEliYvB2I3WR3FqlJ5f1kU6w+f5tVbriKstl/Bcjc3YXyfcPo2r83rv+zhWNK5SYSOJJzh/37dS48mtajm5c7Eb7eQmZPn0DjtGjUkInOBlsDXwE3GmOO2Vd+LyEZHBadUlRa3F/54AzCwdTo0HQS9J1vzCRQeGXRyh3UPgCaCCm3fyVSenbud1MzcIstu7dSQWzo1Om97EeFft7Tjurf/4Pl5O/j0ngiMgadmb8fDTXh7ZEf2nkhl3OcbeOnHXfz71nYOi93e4aPvGmOWlbTCGBNRhvEo5TpWvW019Ty4CnbNg7UfwVfDoN+TMPC5c9tF2zqKG2kiqKgOx5/hzk/XYYyhS+OaBcu7NK7J00Nbl7pfSC1fHr+uBa/+tJuF246TnJHDmoMJvHZrO4IDqhEcUI0Hr27KRysO0LNpEMM6NHBI/PYmgjYissUYkwQgIjWB0cYYnWNYqcuReBi2zbRmAAtqCn0ft+YPnvsA/PkudPkLBDS0to1eB/7BEHD+WaVyvmNJGdz56Tpy8/KZ+UBPmtfzv6T9/9I7nB+3HuPFBTvJzMmjT7PajOx6biqXx69rwfpDCTwzZzvtGwYUaWIqK/b2Edx/NgkAGGMSgfvLPBqlXMWf71g3efV65NwyTx+49mUw+bDi9XPLY9Zb9w/ohPEVTlxqFmM/XUdKRg5fj+9+yUkAwN1N+Pet7UnOyMEA/761HVLod+3p7sa7to7m9YdOl2H059h7ReAuInJ2rgARcQe8HBKRUlVdynHY8g10HAM1il3q12wMEX+BDdOg1yRrmsiko9DtAefE6kKMMXz252ES0rL42+BWF90+Pi2Lu6at43hyJt/c142rGgZc9mu3aVCDD+7sjL+3ByG1fM9b36imL8uf7E8NH8cM0rQ3EfyK1TH8P9vzB2zLlFKlSTlundmf2gU9J0LrYdZZ/Zr3rRLSvaeUvF+/J61EseyfcNWt1jLtKHaovHzDiwt28vVaayjniIgQwi/QBBOTmM5d09ZzIjmTT++JoEvjWlccw/Vt619wvaOSANifCP6O9eH/kO35YuBTh0SkVGWXkWQ1/aydCvm5Vlv/zLut5p2+j8PGz6zJYmqFl7x/9brWRPEr/wMZieDuBcEdyvVHcCWZOXk8NjOSn7efYHS3EL7fEM2sTdE8eX3JVwX7T6Zy17T1pGfn8s193cokCTibXYnAGJMPTLV9KaVKYgxs+RoWv2B9gLe7AwY8A4GhEPktLPsXTLdNMN/nsQsfq9ckq3no4DIrgXh4Oz5+F5SamcP9X21k7cHTPHdDa+7r24QTyZnM3hTLY9e2xN2taL/M1ugkxn2+Hg93N75/oCetg6tG7SZ77yNoDvwbaIM1wTwAxpgmDopLqcolOQYWTLLmBG7cGwa/BsHtz63vfDdcdTus/59VKqLuRdqgqwVCnymw5EWtL+RAz8/bwcbDifx3ZEdu7mSN0rojIoSHvt3Myv1x9G9Zt2DbxDPZ3PP5evx9PPhmfHcaB5X96B1nsbdp6HPgH8DbwADgL2jlUqUgOx22fW9dBeTnwtA3IWI8uJXw7+HlC30etf/Y3R6Ak7ug/ciyi1cVWH0gnnmRx5g0sFlBEgAY1Loetfy8+GFjTJFE8MZve0nNzGXmAz2rVBIA+xNBNWPM77aRQ0eAF0VkE/DCxXZUqsqJj4J9v1gzhh1ZA3lZ0LgPDH+/9Hb/y+HlC7d9UnbHUwWyc/N5ft4OQmv58tcBzYqs8/JwY3jHBnyz9ginz2RTy8+LbTFJTF9/lHt7h9PiMoaIVnT2ntVn2UpQ7xeRh0XkFkCnOVKuZ8ds+KAb/PYcpJ6ErvfB2Dlwz49lmwRUgVmbYnhsZiS20euXrKT9Pll5kANxZ3hpeFt8PM+fKGZElxBy8gzzI2PJzze8MH8nQX7eTL6m+WXFUNHZe0UwGfAFJgGvYDUP3eOooJSqkLbPgjn3Q0gP60xd7/R1uMycPF77ZTfxadkMbluf64oNsczLNzwyfTNtGwQwsdiZPcCmI6cZ99kGBrauy2PXtqBxkB/Rp9N5b+l+Bretz4BCTT+FtWlQg3YNA/hhYwx+3h5ERifxnxEdHDqE05kumghsN4+NNMY8AaRh9Q8o5Vq2zbTKP4T2hDEzdd7fcjJrUwzxadkE+nryn9/2Mah1vSIjeWZsOMrP20/w8/YTtKjnz7Vt6hWsS07PYdL0SLw93flt50l+2nackV1DiE7MwE2EF25qc8HXHhHRiBfm7+SlBTvp0rgmtxTqR6hqLto0ZIzJA/qUQyxKVUxbv7eSQOPecOcPmgTKSV6+4ZOVB+kQEsgrw69i78lUftx6rGB94pls3li0l27htWjXMIDHZ0YSfTodsJqD/jZ7KydTMpl2TwQrnuzP6G6hfL8hmj/2xTHlmuY0CLzw3A7DOjTAy8ON9Jw8XhrWFje3qlviw96moS0isgD4AThzdqExZo5DolKqokg9AT9OspLAmJlWB64qF7/sOM6RhHSeHtKK69rU58PlB3hr8T5uaB+Mp7sb/7fIGsXzyvCrqObpzg3vreTh7zbzw4O9+H7DURbtPMmzQ1vTISQQgFduvor7+zZhVVQ8IyIu3qwX6OvF5EHNcRO5ovIRlYG9icAHSAAGFlpmAE0Eqmpb/R7kZcNN72gSKEfGGD5acYAmtf24tk193NyEJ69vwb1fbOSHjTFc1bAGMzZYo3ha1rdG8bxxewce/GYTj0zfzLK9cfRvWYfxfYp24IcG+TImKNTuOErqd6iK7L2zWPsFlOs5E28rBzHCKhWtys2fUQnsiE3htVvbFfQJDGhZl86hgbz7+37q1fCmdnVvphQaxTP4qvrc2zucz/48RF1/b/4zokOVbs4pS/beWfw51hVAEcaYe8s8IqUqijXvQ04G9H3C2ZG4nI9WHKCuvze3dD7XQSsiPHl9K0Z/spYTKZm8PbID/sVG8Tw1pBUe7sLQdsEEVdeyHPayt2loYaHHPsAtwLFStlWqcjAGjkfCzrng5mlV/fS0VVBJPw3rP4G2t0CdFk4N09Vsj0lmVVQ8Tw1phbdH0TH+PZsGcUP7YLJy8ri54/mjeLw83HjmAjOCqZLZ2zQ0u/BzEZkOrHJIREo5WkYirH4fds6B0wet2j/5uXBoBYz8FvzrwbqPIDsN+unVwOXIzs3H3U3OK9pmj09WHqS6twdjupfclv/+6E4ARSZvUVfmcusFNQdKvhNDqYpu/sOw6i2rKuiw9+CJ/XDHV3ByJ3wyEA6vsuYPbnUj1Gvr7GgrnWNJGQx4czlP/LD1kvc9npzBz9ut8f6l3bwlIpoEyphdiUBEUkUk5ewX8CPWHAUX22+wiOwVkSgReaqE9W+LSKTta5+IJF3yT6DUpdi/GPYstCaHv3u+VRXUtxa0GQ73/goY+OIGyEqGq//m7GgrneSMHMZ9vp7YpAzmRcZyIC7tkvb/cvUR8o1hXK8wxwSoSmRXIjDG+BtjahT6alG8uag42x3JHwBDsMpXjxaRIrfyGWMeNcZ0NMZ0BN5Dh6MqR8rNgl/+BkHNoOfD568P7gD3L4WwvtBhjE4GcwEfLIui/xvLmB8ZW1DLJys3jwlfbeRQ/Blrjl13Nz5ecdDuY57JyuW7dUcYfFX9EqdrVI5j7xXBLSISUOh5oIjcfJHdugFRxpiDxphsYAYw/ALbjwam2xOPUpdl9btWn8CQ/yt9ohf/+jBuIdyiczCVJupUGv9dso/4tGwmz4jk1qmr2XQkkcdmbmXdodO8OaIDwzo04I6IEOZsieFEcqZdx529OYaUzNzzxv4rx7O3j+Afxpjks0+MMUlY8xNcSEMgutDzGNuy84hIYyAcWFrK+gkislFENsbFxdkZslKFJB2FP/5jNQE1G+TsaCotY6y5fX083Vn6xNW8cXt7YhIzuG3qan7adpynhrRiuG00z4R+Tcg3MG1V0auCY0kZPPzdZlbsO/e/nJ9v+PzPw3QICaRzaM1y/ZmU/cNHS0oY9u5rj1HALFtdo/MYYz4GPgaIiIi4vFq0ynXkZsP+36yzfv/64N8Afn3amjj++n85O7pK7aftx1kVFc/Lw9tS19+HEREhDGkXzCd/HMTb040H+p2btDCkli83tg/mu3VHmTigGYG+XhxLymDUx2s5ejqdn7Yf5+EBzZhyTQuW7TlV0KSkHcHlz94P840i8hZWmz/ARGDTRfaJBUIKPW9kW1aSUbZjKnX5jIFd863pHRMPnb9+0D+0dLSNMYZ//rSbvi3qcHWLOnbtk5aVyysLd9G2QQ3u7N64YHl1bw8evbbkey0evLop8yOP8fWaI9zapRGjP15L4plspt/fg7lbYnhvaRQbDyeSmZtHcIAPQ66qX+JxlGPZmwgeAZ4Hvse6w3gxF//g3gA0F5FwrAQwChhTfCMRaQXUBNbYGYtS54vZCIuegeh1ULcNjJoOfrUh9bhVOC43C7o/6Owoy9Xh+DOkZeWWWDBt9/FUPl11iJkbo/l1Sr+LVuIEePf3/ZxMyWLq2C523x/QOrgGA1vV5fPVh/lhUwyJ6dl8fV93OoYE0rNpEF3DavH8/B1k5uTz9JBWeLrrDLjOYO8NZWeA84Z/XmSfXBF5GFgEuAOfGWN2isjLwEZjzALbpqOAGeZypx9S6sBS+OZ264P/pneh01hwO3/WKVczecYWjp5OZ83Tg86bhWv+1lg83IS8fMOU7yOZfn+PC3647z2RymerDjEyIuSS2/Af6t+UER+tIScvn2/Gdy+oBgowIiKE9o0CmbUpmjt7NC79IMqhxJ7PXxFZDIywdRIjIjWxPryvd2x454uIiDAbN24s75dVFdXJXfDZ9RAQAvf+Aj5Vu1ywvfadTOW6t/8A4J1RHQs6cMHqmO3z+lJaBdfghnbBPP7DVp68vmWplTbPZOUy/IM/STyTzeLHrqaWn9clx/PduqN0Cg2kdXCNy/uB1BUTkU3GmIiS1tl7HVb7bBIAMMYkoncWK2dLPQnfjQRPX7hzpiaBQmZtisHDTQgO8GH6+qNF1m08ksix5EyGdWjArZ0bclOHBry1eB9bjiaedxxjDE/P2c7BuDTeHd3pspIAwJjuoZoEKjB7E0G+iBQU/hCRMEqoRqpUuclOh+mjID0exszQTuBCcvPymbM5lgGt6jK2R2PWHjzNwUJ3+C7YGouPpxvXtqmHiPDqzVdRv4YPk2dEkpCWVeRYX605woKtx3j8upb0bla7vH8UVU7s7Sx+FlglIisAAfoCExwWlVKFZabA9h/g5A7IPmN9nT4Ip3bDqG+hQSdnR+gUx5MzmLM5lvv6hhep0vnH/jji07IY0aURHUMDeXvxPmZsiOaZoa3Jycvnp23HubZNffy8rX//gGqe/HdUR0Z/vJbery/ljogQ7uvThPgzWbz60y4GtarLQ1frfAxVmb2dxb+KSATWh/8WYB6Q4cC4lIJjkdbEMNtnQc4ZqFYLvP3Bqzp4+cHwD6DVDc6O0imMMTw1ezsr9sWRlJ7Nszecq94ya1MMQX5eDGhVF093N65pXY9Zm2J4/LoWrI5KIDE9h+EdGhQ5XtewWvwyuS8f/3GQ6euP8s3aI/h5eVCvhg9v3dFRJ3ip4uydmOY+YDLWvQCRQA+s4Z4DL7CbUpfvz3dg8QvgUQ3a3QYR90LDLs6OqsJYuucUK/bFERbkyycrD9G/ZV16N6tN4plsluw6xdgejQuGYo7uHsqvO0/w286T/L77JAHVPOlXwr0Dzev588aIDjxxfUu+WH2YZXtO8eaIDgT4llwFVFUd9vYRTAa6AkeMMQOATkCSo4JSLs4Y2PAphPaEx3dbZ/6aBApk5ebx8sJdNK3jx4JH+tCkjh+Pz9xKUno2C7YeIzsvn9u7nOsz6dusNg0Dq/HF6sP8tuskQ9vVx8uj9H/9ejV8+PvgVvw6pV+Vn7RdWexNBJnGmEwAEfE2xuwBWjouLOXSjm22agN1GgvVtO5McdNWHeJIQjr/uKktNXw8eWdkJ+LTsnh6znZmbYqhTXAN2jQ4N0LHzU0Y3S2ETUcSSc/OY1iHEkt+KRdmbyKIEZFArL6BxSIyHzjiqKCUi9s5z5o1rOVQZ0dS4ZxMyeT9pVFc26ZeQfNOu0YBPHZdC37ZcYLtscmMiDh/BNWIiBDc3YT6NXzoFl6rvMNWFZy9ncW32B6+KCLLgADgV4dFpVyXMbBrHjTpb00Y48Li07L450+78XQX2jYIoG2DGny15gi5eYbnbig6L+8D/ZqyfG8ckUeTGFasIxis5p7Hr2tB/Ro+lzV9pKraLrmCqDFmhSMCUQqAY1usZqF+rj072KmUTMZ8uo7o0+n4eXswc2NMwbqJA5rSOMivyPbubsKn90RwNCGdoOolz7Xw1/4l3zmsVFmWklbqyu2aZzULueiwUIATyZmM+WQtJ1Iy+fLebnQPr8WJlEx2xqYQm5TByK4hJe5Xw8dTO3fVZdFEoCoOY2DnXJduFopNymDMJ2tJSMvmq3u7ERFmvQ/BAdUIDrh4hVClLofWfFWOE7cXMpNLXpeTAbt/tCaROetss1Cbm8slPGc4fSabw/FnSlyXkZ3HnZ+s5fSZbL4efy4JKOVomgiUY2Qkwkd94ZNBkBRddF1milU2+vux8MVQSLbNV1TFm4XSs3O5/aPV3PjeKmKTzr8xf+qKAxxOSOd/Y7vQSadrVOVIE4FyjKjfIS/LOsOfdh2c2mMtPxMPX94E0Wuh1yNWvaD/9YUDy6xho+FXV9lmoVcW7uZQ/Bly8/N5avY2CpeAP5Jwho9WHGBYhwb00uJuqpxpIlCOsf838A2C+5aAybfmDNi1AD4fAnF7rBnErnsV7l8GvrXh65sh6Qi0veWih66Mft1xgunrjzKhXxOeHdqalfvjmbnx3JXSKwt34ekmPFtsWKhS5UETgSp7+XkQtQSaXQPB7WH8Iussf+Zd1rSRd82FFtdZ29ZpAfcvhatutxJCJW4WSsnM4cUFO+n92lLe+m0vKZk5gDUK6Kk522jXMIDHr23Jnd0b06NJLV5duJvjyRks3XOSJbtPMWlQc+rV8HHyT6FckV0zlFUkOkNZJRC9AaZdA7dNg3a3W8vSTsHyf0OXcRDcoeT98vMq5RSTxhgWbjvOKwt3EZeWRefQmmw6kkigryd/7W/d6LXlaBI/TepDkzrVATiakM71//2DiLCaHD2djoeb8MvkfhesAaTUlbjQDGU6fFSVvf2LQNygaaHitNXrwo1vX3i/CpoEMnPy8PZwQ+T8O3JTMnOY+O1mVu6Pp13DAD65O4IOIYHsiE3m/xbt5V8/W30jr9/WriAJAIQG+fL3wS158cddAHw9vpsmAeU0mghU2du3CEK6V4lO36hTaYz6eC1dGgcy9c4u59Xlf37eDlYfSODFm9pwV8+wgvINVzUM4Kt7u7H2YAKH4s9wR8T5N4Hd3TOMtQdPE1Tdi77Nzy8LrVR50USgylbKcTixDQb9w9mRXLHo0+mM/XQdaVk5LNp5kg+WRfHIoOYF6+duiWF+5DEeu7YF43qHl3iMHk2C6NEkqMR1bm7CR3dpeW3lfHotqsrW/t+s7y2ud24cV+hUSiZjp60jPTuXOQ/15uaODXhryT6W7T0FWEni+Xk76RpWk4kDtIaPqtw0EajLt+VbWPwPyMs9t2z/b1CjIdRtU/p+TpSWlcvrv+5h/BcbyMnLL3GbxDPZ3DVtPXGpWXxxbzfaNKjBv29tT6v6NZgyI5KDcWlMnrEFEXh7ZEet5qkqPW0aUpfHGFj+GiQfhYQouP0za/nB5dBuBJTQsepM+fmGOVtief3XPcSlZgGw5WhSibX5/zZ7G4cSzvDFuK50tt3hW83Lnf+N7cKN761k2Pt/kpaVy7ujO9Gopm+5/hxKOYImAnV5Eg5YSSCsL+xZCN+NhG4TIDvN6c1CefmGeVtiiU5MJz07j/TsXLbFJLMtJpkOIYG8fUdHxn2+nmV7T52XCJIzcli25xTj+4Sfd4dvaJAv74zuxL1fbODWzg1LrPuvVGWkiUBdngNLre/D3oMjq2HBw9Z3d28I7+fU0L5bd4Tn5+8EwMfTDT8vD2r5efGfER24pVND3NyErmG1WLbnFH8f3KrIvsv3niI333Bd23olHntAy7qseGIADQL1xi9VdWgiUJfnwO9QMxxq2b68q8Os8da9A15+F9/fQRLSsnhj0V56NQ3i6/HdS22/H9CqDv/6eQ/HkzOKlHdevOsktat70TGk9KJvoUHaHKSqFu0sVpcuNxsOrSx6w1ib4fDXNTD8fefFBbz5217Ss/N4aVjbC3biDmhZF4Dle+MKlmXn5rNibxyDWtXTDmDlUjQRqEsXsx5yzhRNBAC1m1t3EDuYMYbHZ27lxQU7yczJK1i+NTqJGRuiGdcrjOb1/C94jGZ1q9MwsBrL9pwqWLbuUAKpWblc06bkZiGlqiptGlKX7sBSEHen9QWsOZDA7M3WHL7rD53mwzs7E1rLlxcW7CTIz5vJ1zS/yBFAROjfsg7ztsSSlZuHt4c7i3edxMfTjT5aBlq5GL0iUJcu6ncI6QY+NZzy8u8vi6KuvzdT7+xMbFIGN763iqfmbGNrdBLPDG2Fv4+nXccZ0LIuZ7Lz2Hg4EWMMS3adpG/zOlTzqpg1j5RyFE0E6tKciYfjW89vFionm48msvpAAhP6NWFIu2B+mtSHZnWrM3NjDBGNa3JLp4Z2H6tXsyC83N1YtucUO4+lcCw5k2u1WUi5IG0aUpfm4HLAOC0RfLA0ikBfT0Z3CwWgUU1fZj7Qk+83HGVAq7olVggtja+XB92b1GLZ3lP4eXsgAgNbOb6PQ6mKxqFXBCIyWET2ikiUiDxVyjZ3iMguEdkpIt85Mh5VBg4sA59AaNCp3F9617EUft9zint7h+Pnfe4cxsvDjbt6hl3WXb4DWtblQNwZvt8QTZfQmtSu7l2WIStVKTgsEYiIO/ABMARoA4wWkTbFtmkOPA30Nsa0BaY4Kh5VBoyx7h9o0t8pcwd8uDyK6t4e3NMzrMyOOcB2BXAiRZuFlOty5BVBNyDKGHPQGJMNzACGF9vmfuADY0wigDHmFKriitsDqced0ix0MC6Nn7YfZ2yPxgT42tcZbI/w2n6E2W4Q00SgXJUj+wgaAtGFnscA3Ytt0wJARP4E3IEXjTG/Fj+QiEwAJgCEhoY6JFhlh13zre9lkAgio5O469N1XNUwgEGt6zKodT3Ca5d+R/KHyw/g5e7G+D4l1/2/End0DWHNgYQiM4gp5UqcPWrIA2gO9AdGA5+ISGDxjYwxHxtjIowxEXXq6ExOZSbpKPy3nTUc9GIip1vVRptfD4Hnz7Z1qb5bd4TcfMPpM9m8+tNuBry5nJveW1Uw4XthUadSmbM5hju7N6aOf9m34f+1fzO+Hl/8HEUp1+HIRBALFP7EaGRbVlgMsMAYk2OMOQTsw0oMqjwsetZKBjvnXHi7yOkw7yHrBrIRX1zxy2bm5PHL9hMMbRfMokf7sfJvA3juhtbsPJbMfxbtPW/7NxbtxdfLg4kDml7xayulzufIRLABaC4i4SLiBYwCFhTbZh7W1QAiUhurqeigA2NSZx1YCrsXgKcvHFxhdQSXpHASGD0DvK684NqS3SdJzcrl1s7WmP+QWr7c17cJd/cM46u1R9gWk1Sw7ZajiSzaeZL7+zYhSEf0KOUQDksExphc4GFgEbAbmGmM2SkiL4vIMNtmi4AEEdkFLAOeNMYkOComZZObDb/8HWo1gYHPQXI0nC4h/x5cXuZJAGDu5ljq1fA+by7fx65rQZ3q3jw7dwd5+QZjDK//uocgPy/G9y37vgGllMWhfQTGmJ+NMS2MMU2NMf+0LXvBGLPA9tgYYx4zxrQxxrQzxsxwZDzKZt1HEL8PBr8Oza+zlh1acf52m74A36AyTQIJaVms2BfHzR0bnlfhs4aPJ8/f2Ibtscl8s/YIf+yPZ+3B0zwysBnVvfXeR6UcRf+7XE3KcVjxOrQYAi2us5qEajS0zv4j7j23XXY67FsEHUaVWRIAWLjtOLn5hls6l1wK4sb2wczcGM0bi/YSHOBDo5rVGN1dR4op5UiaCFzN4hcgLwcG/8t6LgLhV8O+XyE/H9xsF4n7f4OcdGh7i92H/nrtEbZGJxFQzZOAap4E+npyfdv61KtxbjavOVtiaVXfn1b1Sy5YJyK8MvwqrvvvH+w/lcbbIzvg7aFF4JRyJE0EriQ3C3bMhm73W/0DZzXpD1u/g5PbIbiDtWznXPCrA41723Xo5IwcXvlxF94ebuQbw5lsa56Ad5bs5/0xnenZNIiDcWkFFUIvJKy2Hy/e1JY/D8QzrIP9ReSUUpdHE4ErOX0ITB407FJ0+dl5BQ4utxJB9hmrWajjGLtLSSzaeYLsvHxmPtiTjiGB5OTlE3UqjYe/28zYaet4ekgrUjJyEMGuD/cx3UMZo01CSpULZ99QpspTQpT1PajYePwawVCnlTWMFKwkkJtxSc1CP249RmgtXzo0CgDA092N1sE1mP9wH65tXY9Xf9rNh8sP0LtpbeoH6MTvSlUkmghcScJ+63tQs/PXhV8NR1ZbzUe75oFfXWjcy67DxqdlsfpAAjd1CD6vDHR1bw+mju3M3wa3JN8YPctXqgLSpiFXkhBlfcD7BJy/rkl/WP8/q3lo32/Q6U67m4V+3n6cvHxTapOPiPDX/s0Y1ysMXy/9k1OqotErAlcSH2VNMF+SsN4gbtaoostoFmpRrzot6194wnhNAkpVTJoIXEnC/vP7B87yCbA6keP2QPV6ENrTrkPGJmWw4XAiwzo0KMNAlVLlSROBq0g/DekJEHSBmn5N+lvfWw+zu1nop23HALixvSYCpSorTQSuIuGA9b20piGAlkPAzQM6jLb7sAu2HqNDowDCLjCXgFKqYtNE4CoKho6WMGLorIZd4KloaNSl9G0KORiXxo7YFG7SZiGlKjVNBK4iYT+IO9QMu/B2xeoKmdLKUwPztsQios1CSlV2mghcRfx+Kwm42z/f7++7T9L5lcXM21J8PiGYsf4o7y+LYmDLunqDmFKVnCYCV5Fw4ML9AyWYuyWWxPQcpnwfyfPzdpCVm4cxhv8u2cdTc7bTr0Ud3h3dyUEBK6XKiw7sdgX5+XD6ADQdYPcuefmGlfvjubljA+rW8OHjPw6yLTaZ5nWrM2tTDLd3acS/b22Hp7ueSyhV2WkicAUpMZCbeeGO4mK2xiSRnJHDoNb1uKlDAzqHBvLkD9vYGp3EwwOa8fh1Lc4rJ6GUqpw0EbiCeFuNoUtoGlqxNw43gT7NagMw+Kpg2jYI4GD8Ga5uUccRUSqlnEQTgSuwZ+hoMSv2xdEhJJCafl4Fy0Jq+RJSq+xmK1NKVQzawOsKEqLAy98qHWGHxDPZbI1J0jN/pVyEJgJXEG+rMWRnm/7KqHiMQROBUi5CE4EruMShoyv2xhHo60n7RoGOi0kpVWFoIqjqcjIgOdru/oH8fMOKfXH0bV4HdzcdFaSUK9BEUNUlHACM3Ylg94kU4tOytFlIKReiiaCqOztiyM6moRX74gDo17y2oyJSSlUwmgiquP27t1gPap0/IU1aVi7PzN3OtFWHOJWSCVj9A22Ca1C3htYPUspV6H0Elysvp+QCbsZA9DqI3QQBIVArHGqGg3f1cg9x2qpDBG7dhL9bLU6cyqVjyLl1efmGKTO2sGT3KQBe/WkXPcKD2HQkkQn9mpR7rEop53HdRJAUDUf+hMDG1od19XpWGYZjkRCz3vogz88D3yDwqw3VakLKMTi1G+L2QuoxqN8eWg6FVkOtM+7tP8CGT+HkjvNfr1ot6zh+daxjYuBMvPWVnmDNF+ztDz41rDH/nj7g7g0eXuDuBXnZkJsNeVnW85ZDoc1wa/vijOH7n34lfe0MRnpFspfGPPDlRuZN7EWjmtYNYW/+tpclu0/x8vC29G5WmwWRx1iw9Ri5+Ybr2tZ36FuvlKpY5EL15iuiiIgIs3Hjxis/0Iw7Yc/Cc889fa0P2/xc63nNMGvZ2Q9qkwce1aBOC6jTGmo0gCOrrbN/jFXr3+RBvaug633WB3XqcTh9EBIPQXIspMef+/AXAd/aVnI4mxiyUq2vzBQrKeVlQ24W5OeAmyd4eFtJID0eko6Ch481q1j41ZCVUnDsxH2rqJlxlHzcILwvx9tPZPB8CA7wYdZDvfh990ke/X4rd3YP5dWbryqoGWSMISk9p8jdxEqpqkFENhljIkpc55KJIDcb/q8JtLjempYx8ZD1ge3hA426Wl/VC42ayc+HrGTwDgC3Yt0qaXGwf5F1pdB6GIR0s/vGrctmDMRshG3fw47ZkHHaWuzuTbJbANsz63Ki0fXcOuZB3P3rAvBnVDz3fLaetg0D2H08hc6hgXw9vrtWD1XKRWgiKO7QSvjyRhj1HbS6oWwCu4Cs3DzSMnOp5edV9hU7c7PJSznG7N0ZvPZ7NIkZOdzbO5xnh7bGrdh9ADM3RPO32dsIreXL/Im99cxfKRdyoUTgmn0EUUusppbwfgWL3ly0l5Mpmbw4rC1+3lf+tsQmZbB87ymW7Ylj9YF40rPz8PZwo0FgNRoE+lDX34dAX09q+npR08+LrmE1aVW/aHt/cnoO7y/bz+zNsfRsGsQ9PcPoGlazIJkkpWezfG8cn6yMZuexFLqF1eIfw9rQtkFAiTHd0TWE2v5etKxfQ5OAUqqAQxOBiAwG3gHcgU+NMa8VWz8OeAM4Oxfi+8aYTx0ZE2AlgtAeVucscCI5k49WHCA337A9NplP7o4oscpmXr4hJy+f7Lx8MrPziEnKIPp0OkcT0olJzOB4SiYnkjM4npxJaqbV19CoZjVu69yI8Np+nEjJJDYpg9jEDDYknCYpPYe0rNyC43cKDWR011Cub1uf2ZtjeHfpfpIzcujfog4r98Xx07bjtKrvz6DWddlwKJGNR06Tb6BhYDXeG92JG9sHX/SKY2Ar+wrPKaVch8OahkTEHdgHXAvEABuA0caYXYW2GQdEGGMetve4V9w0lHIM3moN174MvScD8NZve3lvWRT/vLkdr/2yGw93Nz68szMdQwJZuT+eX7YfZ8nuk6Rk5pZ62Dr+3jQI8KFeDR+CA3xoHORHvxa1aVqn+gU/nLNz84lPy+Ln7ceZsSGaqFNpiFjdAL2bBfHM0Na0bRBARnYe8yNj+XLNEXYfT6FNcA0Gta7LwFZ16dAo8LxmIKWUKsxZTUPdgChjzEFbEDOA4cCuC+7lQMnpOQRE/W49aXYNYLXff7f+KANb1mVM91B6Ng3ivi83MPbTdfh4upOWlUtANU+ua1uf0Fq+eLq74ekueHu60yDAh9BavjSq6Us1L/fLisnL1lx0X98mjO8TzsYjify28wS9mtamf8s6BUmkmpc7o7qFMrJrCGey86heBs1XSikFjk0EDYHoQs9jgO4lbHebiPTDunp41BgTXXwDEZkATAAIDQ29rGCmrTrE1OUHWBW+CB//BlC3DQC/bD9BfFo29/QKAyC8th9zJ/bmtV/2kJdnGNo+mF5Ng8pldI2I0DWsFl3Dal1wG00CSqmy5Oyxgz8CYcaY9sBi4MuSNjLGfGyMiTDGRNSpc3nF0Po2r01GViZ5UUvJb3ZNwRDPL1Yfpkltv4IpGQFq+Hjyr1va8frt7bm6RR0dYqmUqtIc+QkXCxQqakAjznUKA2CMSTDGZNmefgp0cVQwLer5817fPPzMGX7OaAvA1ugkIqOTuLtnY21jV0q5LEcmgg1AcxEJFxEvYBSwoPAGIhJc6OkwYLcD42GA21bycOeZrUGsjornyzWH8fNy57YujRz5skopVaE5rLHZGJMrIg8Di7CGj35mjNkpIi8DG40xC4BJIjIMyAVOA+McFQ+ARC2GkG7USarDpBmRpGTkMKpbCP4+JRSPU0opF+HQXkdjzM/Az8WWvVDo8dPA046MoUDqSTixDfdBL/BB884Mf/9PsvPyubtn43J5eaWUqqhcZ/jJgbPDRq+lVf0aTB3bmf0n02hW19+5cSmllJO5TiLwCYCWN0D9doB1h63eZauUUq6UCFrdUC4F5pRSqrLRAfJKKeXiNBEopZSL00SglFIuThOBUkq5OE0ESinl4jQRKKWUi9NEoJRSLk4TgVJKuTiHTVXpKCISBxy5zN1rA/FlGE5ZqqixVdS4oOLGVlHjgoobW0WNC6pObI2NMSVO6FLpEsGVEJGNpc3Z6WwVNbaKGhdU3NgqalxQcWOrqHGBa8SmTUNKKeXiNBEopZSLc7VE8LGzA7iAihpbRY0LKm5sFTUuqLixVdS4wAVic6k+AqWUUudztSsCpZRSxWgiUEopF+cyiUBEBovIXhGJEpGnnBzLZyJySkR2FFpWS0QWi8h+2/eaTogrRESWicguEdkpIpMrQmwi4iMi60Vkqy2ul2zLw0Vkne13+r2IeJVnXMVidBeRLSKysKLEJiKHRWS7iESKyEbbMqf/ndniCBSRWSKyR0R2i0hPZ8cmIi1t79XZrxQRmeLsuArF96jt73+HiEy3/V+Uyd+ZSyQCEXEHPgCGAG2A0SLSxokhfQEMLrbsKeB3Y0xz4Hfb8/KWCzxujGkD9AAm2t4nZ8eWBQw0xnQAOgKDRaQH8DrwtjGmGZAIjC/nuAqbDOwu9LyixDbAGNOx0FhzZ/8uz3oH+NUY0wrogPXeOTU2Y8xe23vVEegCpANznR0XgIg0BCYBEcaYqwB3YBRl9XdmjKnyX0BPYFGh508DTzs5pjBgR6Hne4Fg2+NgYG8FeN/mA9dWpNgAX2Az0B3rjkqPkn7H5RxTI6wPiIHAQkAqQmzAYaB2sWVO/10CAcAhbINVKlJshWK5DvizosQFNASigVpYUwwvBK4vq78zl7gi4NybeFaMbVlFUs8Yc9z2+ARQz5nBiEgY0AlYRwWIzdb0EgmcAhYDB4AkY0yubRNn/k7/C/wNyLc9D6JixGaA30Rkk4hMsC1z+u8SCAfigM9tzWmfiohfBYntrFHAdNtjp8dljIkF3gSOAseBZGATZfR35iqJoFIxVnp32rheEakOzAamGGNSCq9zVmzGmDxjXbI3AroBrco7hpKIyI3AKWPMJmfHUoI+xpjOWE2iE0WkX+GVTvw78wA6A1ONMZ2AMxRrbnHm/4CtnX0Y8EPxdc6Ky9YvMRwriTYA/Di/efmyuUoiiAVCCj1vZFtWkZwUkWAA2/dTzghCRDyxksC3xpg5FSk2AGNMErAM6zI4UEQ8bKuc9TvtDQwTkcPADKzmoXcqQmy2s0iMMaew2rq7UTF+lzFAjDFmne35LKzEUBFiAytxbjbGnLQ9rwhxXQMcMsbEGWNygDlYf3tl8nfmKolgA9Dc1sPuhXXZt8DJMRW3ALjH9vgerPb5ciUiAkwDdhtj3qoosYlIHREJtD2uhtVvsRsrIdzurLgAjDFPG2MaGWPCsP6ulhpj7nR2bCLiJyL+Zx9jtXnvoAL8nRljTgDRItLStmgQsKsixGYzmnPNQlAx4joK9BARX9v/6dn3rGz+zpzVGeOEzpahwD6stuVnnRzLdKx2vhyss6PxWO3KvwP7gSVALSfE1QfrsncbEGn7Gurs2ID2wBZbXDuAF2zLmwDrgSisy3hvJ/9e+wMLK0JsttffavvaefZv3tm/y0LxdQQ22n6n84CaFSE2rCaXBCCg0DKnx2WL4yVgj+1/4GvAu6z+zrTEhFJKuThXaRpSSilVCk0ESinl4jQRKKWUi9NEoJRSLk4TgVJKuThNBEqVIxHpf7ZCqVIVhSYCpZRycZoIlCqBiIy1zYEQKSL/sxW9SxORt2014X8XkTq2bTuKyFoR2SYic8/WqxeRZiKyxDaPwmYRaWo7fPVCtfi/td0pqpTTaCJQqhgRaQ2MBHobq9BdHnAn1l2nG40xbYEVwD9su3wF/N0Y0x7YXmj5t8AHxppHoRfW3eRgVXWdgjU3RhOsmjFKOY3HxTdRyuUMwpqYZIPtZL0aVqGxfOB72zbfAHNEJAAINMassC3/EvjBVuenoTFmLoAxJhPAdrz1xpgY2/NIrLkpVjn8p1KqFJoIlDqfAF8aY54uslDk+WLbXW59lqxCj/PQ/0PlZNo0pNT5fgduF5G6UDDPb2Os/5ezlR7HAKuMMclAooj0tS2/C1hhjEkFYkTkZtsxvEXEtzx/CKXspWciShVjjNklIs9hze7lhlUldiLWBCrdbOtOYfUjgFX+9yPbB/1B4C+25XcB/xORl23HGFGOP4ZSdtPqo0rZSUTSjDHVnR2HUmVNm4aUUsrF6RWBUkq5OL0iUEopF6eJQCmlXJwmAqWUcnGaCJRSysVpIlBKKRf3/z8zMj+u7osHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(train_accs)\n",
    "plt.plot(val_accs)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05ad20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LYTKYEKDDSIRKQRVKAVELFSLM\n",
      "GIGVINFAYYLAKHGKRYSDGSANN\n",
      "HMQIINEINTRFKTLVEKTWPGDEK\n",
      "ILEANLLPPPEPKESWRRIMDELSV\n",
      "TYFGELSRMTQFKDKSARYAENINA\n",
      "KEILGDEADQYVKVPDTLDVWFDSG\n",
      "AIQFAWELLTSEKWFALPKERLWVT\n",
      "AALASVKGWVSAKLQOOOOOOOOOO\n",
      "PIQETITFDDFAKVDLRVALIENAE\n",
      "ERGKVIADYEERKAKIKADAEEAAR\n",
      "RAALGVLRIIVEKNLNLDLQTLTEE\n",
      "IRQIIDEDLASGKHTTVHTRFPPEP\n",
      "KVISVQEMHAQIKOOOOOOOOOOOO\n",
      "OOOOOOOOOMKIKTRFAPSPTGYLH\n",
      "WDAAQSLLATYIKLNVARHQQGQPL\n",
      "CGTAMDSYLIDPKRKLHVCGNNPTC\n",
      "DAREAFLNITVTKDSRTRYSEAGHP\n",
      "MATGSYPWIPPIKGSDTQDCFVYRT\n",
      "EIYKRLIVSEDNKTLLGAVLVGDTS\n",
      "TFIATSPMHIATKLRSTLDEVIERA\n",
      "ISGLYERVPNIDKAIISVHTHDDLG\n",
      "RNGLRPARYVITKDKLITCASEVGI\n",
      "ADVIQIKVAQGAKPGEGGQLPGDKV\n",
      "PAPCDYAITAIQKFLETDIPVFGIC\n",
      "LPEQVRTNADLEKMVDTSDEWIVTR\n",
      "LVDDERWARFNEKLENIERERQRLK\n",
      "AAGDEVTVVRDEKKAREVALYRQGK\n",
      "QLGIVSLREALEKAEEAGVDLVEIS\n",
      "QLGIVSLREALEKAEEAGVDLVEIS Q 127\n",
      "VGEDWISLDMHGKRPKAVNVRTAPH\n",
      "OOOMSLLNVPAGKDLPEDIYVVIEI\n",
      "OOOOOOOOOOOMKPYQRQFIEFALS\n",
      "FGRPQRVAQEMQKEIALILQREIKD\n",
      "AAALGQIEKQFGKGSIMRLGEDRSM\n",
      "HTTIGKVDFDADKLKENLEALLVAL\n",
      "KQYDINEAIALLKELATAKFVESVD\n",
      "VRQHVIYKEAKIKOOOOOOOOOOOO\n",
      "RAKYQRQLARAIKRARYLSLLPYTD\n",
      "AFAVIVKAAEAAKQAOOOOOOOOOO\n",
      "FQVNQLLDILRAKLLKRGIEGSSLD\n",
      "TIFRELEVFVRSKLKEYQYQEVKGP\n",
      "LKAMGEMKNGEAKOOOOOOOOOOOO\n",
      "YLAVKRRIQPGDKMAGRHGNKGVIS\n",
      "OOMKTLGEFIVEKQHEFSHATGELT\n",
      "LEKKLQYVNEALKDEHWICGQRFTI\n",
      "IAELGRQITERYKDSGSDMVLVGLL\n",
      "KILKDLDEDIRGKDVLIVEDIIDSG\n",
      "SNGKSASAKSLFKLQTLGLTQGTVV\n",
      "APSQEEAVIAFGKFKLNLGTREMFR\n",
      "ANNDMQELEARLKEAREAGARHVLI\n",
      "AERGYLADVELSKIGSFEAALLAYV\n",
      "MAVAHFSPVNDLKHLNIMITAGPTR\n",
      "NNALHLFWQDGDKVLPLERKELLGQ\n",
      "QIDLYAVDGDEYKFLCIAKGGGSAN\n",
      "QIDLYAVDGDEYKFLCIAKGGGSAN Q 152\n",
      "VGDLQRSIDFYTKVLGMKLLRTSEN\n",
      "MEEVKQSNRLVIKTROOOOOOOOOO\n",
      "NDRRCLHLQLTEKGHEFLREVLPPQ\n",
      "KTLLPVLDTMLYKFDDNEIITSAID\n",
      "RWVNALVSELNDKEQHGSQWKFDVH\n",
      "FKDKVKGEWDKIKKDMOOOOOOOOO\n",
      "TIAIAINLFNPQKIVIAGEITEADK\n",
      "LPQELRQAIEHIKAHVTAETPKGKH\n",
      "EEVVEIRGGQRRKSERKFFPGYVLV\n",
      "FLGDGEMDEPESKGAITIATREKLD\n",
      "MQSMQFPAELIEKVCGTIOOOOOOO\n",
      "PGDPIIAHVSPGKGLVIHHESCRNI\n",
      "DTLHLEGKELEFKVIKLDQKRNNVV\n",
      "GGSAEEEAAAYIKEHVTKPVVGYIA\n",
      "VTEIAKKLNRSIKTISSQKKSAMMK\n",
      "TVALIAGGHTLGKTHGAGPTSNVGP\n",
      "NADWVIDGEQQPKSLFKMIKNTFET\n",
      "GDPETQPIMLRMKSDLVELCLAACE\n",
      "EAIGVLEQQSDLKGLLLRSNKAAFI\n",
      "VLFDMAREVNRLKAEDMAAANAMAS\n",
      "GGLDSSIISAITKKYAARRVEDQER\n",
      "SGSRDKGLHGKLKAGVCYSMLDTIN\n",
      "EILQWQRERLVAKLEDAQVQLENNR\n",
      "TTWRKLDETTRNKITDAASAAALMT\n",
      "DTALVHIAAAYHKPTLAFYPNSRTP\n",
      "QFTDDLIARNLLKDVTRVVVDVYGS\n",
      "QFTDDLIARNLLKDVTRVVVDVYGS Q 178\n",
      "SQIEIALDQGEVKAGEFAEPICELE\n",
      "EAMAMAKRVSKLKNANRFFVASDVH\n",
      "TEMMRYMHSLERKDLALNQAMIPLG\n",
      "SHLNTGRPYNADKPNKYTSRYFDEA\n",
      "PLGEEEVALARQKLGWHHPPFEIPK\n",
      "TPSHNPPEDGGIKYNPPNGGPADTN\n",
      "IGPDWYGTDVHHKTLGIVGMGRIGM\n",
      "QLETILNYIDIGKKEGADVLTGGRR\n",
      "QLETILNYIDIGKKEGADVLTGGRR Q 186\n",
      "QEIVDSMTIETYKQISENTKIISQK\n",
      "QEIVDSMTIETYKQISENTKIISQK Q 187\n",
      "LAQEEVWIRQGIKARRTRNEGRVRA\n",
      "GYDPIFFVPSEGKTAAELTREEKSA\n",
      "OOOOOOOOOOMQKVVLATGNVGKVR\n",
      "RSACTYVGASRLKELTKRTTFIRVQ\n",
      "KVRFFKSNSETIKOOOOOOOOOOOO\n",
      "MKVITLTGKDGGKMAGTADIEIRVP\n",
      "MAGVVIHAAFVYKLGDWFARDTRNF\n",
      "PHKSPEVFNLIMKRRAIAGSMIGGI\n",
      "GFGARHNASNSLKDIAELVPFAHRY\n",
      "NIFQSDHPVAMMKAVQAVVHHNETA\n",
      "KVALAQAQGQLAKDKATLANARRDL\n",
      "FEIVNNESDPRFKEYWTEYFQIMKR\n",
      "(200, 1, 25)\n",
      "[[[11  7 13 ...  9 10 13]]\n",
      "\n",
      " [[10  4 11 ... 12 16 11]]\n",
      "\n",
      " [[10  1  8 ...  6  6 20]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3 10 14 ...  7 17  1]]\n",
      "\n",
      " [[12 20  1 ...  2  4 11]]\n",
      "\n",
      " [[14  7 10 ... 13 12  2]]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "r_test_x = []\n",
    "r_test_y = []\n",
    "posit_1 = 1;\n",
    "negat_0 = 0; #dinh nghĩa label\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'OARNDCQEGHILKMFPSTWYV'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "i = 0\n",
    "#-------------------------TEST DATASET----------------------------------------\n",
    "#for positive sequence\n",
    "def innertest1():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    #rint(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    r_test_x.append(integer_encoded)\n",
    "    r_test_y.append(posit_1)\n",
    "for seq_record in SeqIO.parse(\"./Datasets/independent_data/ecoli_test.fasta\", \"fasta\"):\n",
    "\n",
    "    innertest1()\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#for negative sequence\n",
    "def innertest2():\n",
    "    #Input\n",
    "    data = seq_record.seq\n",
    "    print(data) \n",
    "    # integer encode input data\n",
    "    for char in data:\n",
    "        if char not in alphabet:\n",
    "#             print(data, i)\n",
    "            return\n",
    "    integer_encoded = [char_to_int[char] for char in data]\n",
    "    if integer_encoded[0] == 6: print(data, int_to_char[6], len(r_test_x))\n",
    "    r_test_x.append(integer_encoded) \n",
    "    r_test_y.append(negat_0)\n",
    "\n",
    "for seq_record in SeqIO.parse(\"./Datasets/independent_data/ecoli_test_neg.fasta\", \"fasta\"):\n",
    "    innertest2()\n",
    "# Changing to array (matrix)    \n",
    "r_test_x = np.array(r_test_x)\n",
    "r_test_y = np.array(r_test_y)\n",
    "\n",
    "# Balancing test dataset\n",
    "# Testing Data Balancing by undersampling####################################\n",
    "# rus = RandomUnderSampler(random_state=7)\n",
    "# x_res3, y_res3 = rus.fit_resample(r_test_x, r_test_y)\n",
    "# #Shuffling\n",
    "# r_test_x, r_test_y = shuffle(x_res3, y_res3, random_state=7)\n",
    "# r_test_x = np.array(r_test_x)\n",
    "# r_test_y = np.array(r_test_y)\n",
    "\n",
    "r_test_x = np.expand_dims(r_test_x, 1)\n",
    "print(r_test_x.shape)\n",
    "print(r_test_x)\n",
    "print(r_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f316a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  7 13  1  6  3  1  1  2 11  8 18 12  1  7 12 20  4  1  2 11  9  9 10\n",
      "  13]]\n",
      "['L', 'E', 'M', 'A', 'Q', 'N', 'A', 'A', 'R', 'L', 'G', 'W', 'K', 'A', 'E', 'K', 'V', 'D', 'A', 'R', 'L', 'H', 'H', 'I', 'M']\n",
      "tensor([[0.4831, 0.5169]], grad_fn=<SoftmaxBackward>)\n",
      "[[14  7 10 20  3  3  7 16  4 15  2 14 12  7 19 18 17  7 19 14  6 10 13 12\n",
      "   2]]\n",
      "['F', 'E', 'I', 'V', 'N', 'N', 'E', 'S', 'D', 'P', 'R', 'F', 'K', 'E', 'Y', 'W', 'T', 'E', 'Y', 'F', 'Q', 'I', 'M', 'K', 'R']\n",
      "tensor([[0.7532, 0.2468]], grad_fn=<SoftmaxBackward>)\n",
      "['K', 'Y', 'V', 'L', 'A', 'R', 'T', 'D', 'T', 'A', 'T', 'D', 'K', 'D', 'Y', 'L', 'D', 'I', 'E', 'R', 'V', 'I', 'G', 'H', 'E'] [[0.76885164 0.23114839]]\n",
      "['D', 'E', 'F', 'A', 'D', 'G', 'A', 'S', 'Y', 'L', 'Q', 'G', 'K', 'K', 'V', 'V', 'I', 'V', 'G', 'C', 'G', 'A', 'Q', 'G', 'L'] [[0.65274066 0.34725934]]\n",
      "['O', 'O', 'O', 'M', 'T', 'D', 'K', 'T', 'S', 'L', 'S', 'Y', 'K', 'D', 'A', 'G', 'V', 'D', 'I', 'D', 'A', 'G', 'N', 'A', 'L'] [[0.5597522  0.44024774]]\n",
      "['F', 'D', 'V', 'Y', 'T', 'P', 'D', 'I', 'L', 'R', 'C', 'R', 'K', 'S', 'G', 'V', 'L', 'T', 'G', 'L', 'P', 'D', 'A', 'Y', 'G'] [[0.6008571  0.39914286]]\n",
      "['T', 'E', 'R', 'I', 'L', 'F', 'Y', 'T', 'G', 'V', 'N', 'H', 'K', 'I', 'G', 'E', 'V', 'H', 'D', 'G', 'A', 'A', 'T', 'M', 'D'] [[0.66489214 0.3351079 ]]\n",
      "['P', 'A', 'P', 'N', 'M', 'L', 'M', 'M', 'D', 'R', 'V', 'V', 'K', 'M', 'T', 'E', 'T', 'G', 'G', 'N', 'F', 'D', 'K', 'G', 'Y'] [[0.55577123 0.4442287 ]]\n",
      "['A', 'A', 'I', 'A', 'E', 'A', 'R', 'E', 'H', 'G', 'D', 'L', 'K', 'E', 'N', 'A', 'E', 'Y', 'H', 'A', 'A', 'R', 'E', 'Q', 'Q'] [[0.6481208  0.35187915]]\n",
      "['F', 'A', 'G', 'F', 'V', 'K', 'A', 'A', 'S', 'E', 'F', 'Q', 'K', 'R', 'Q', 'A', 'K', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'] [[0.8623488  0.13765126]]\n",
      "['E', 'K', 'G', 'M', 'K', 'L', 'R', 'Q', 'V', 'R', 'T', 'A', 'K', 'D', 'V', 'V', 'I', 'S', 'D', 'A', 'L', 'T', 'F', 'M', 'A'] [[0.51082116 0.4891788 ]]\n",
      "['C', 'K', 'A', 'F', 'N', 'A', 'K', 'T', 'D', 'S', 'I', 'E', 'K', 'G', 'L', 'P', 'I', 'P', 'V', 'V', 'I', 'T', 'V', 'Y', 'A'] [[0.70401955 0.29598054]]\n",
      "['V', 'T', 'Y', 'P', 'Y', 'V', 'T', 'I', 'D', 'V', 'S', 'S', 'K', 'S', 'H', 'P', 'F', 'Y', 'T', 'G', 'K', 'L', 'R', 'T', 'V'] [[0.7707537  0.22924633]]\n",
      "['F', 'L', 'V', 'P', 'Q', 'G', 'K', 'A', 'V', 'P', 'A', 'T', 'K', 'K', 'N', 'I', 'E', 'F', 'F', 'E', 'A', 'R', 'R', 'A', 'E'] [[0.5117055  0.48829448]]\n",
      "['I', 'T', 'V', 'N', 'K', 'N', 'S', 'V', 'P', 'N', 'D', 'P', 'K', 'S', 'P', 'F', 'V', 'T', 'S', 'G', 'I', 'R', 'V', 'G', 'T'] [[0.53004587 0.4699541 ]]\n",
      "['A', 'N', 'I', 'K', 'G', 'L', 'T', 'F', 'T', 'Y', 'E', 'P', 'K', 'V', 'L', 'R', 'H', 'F', 'T', 'A', 'K', 'L', 'K', 'E', 'V'] [[0.64102244 0.3589776 ]]\n",
      "['M', 'L', 'Q', 'R', 'I', 'Y', 'G', 'T', 'A', 'W', 'A', 'D', 'K', 'K', 'A', 'L', 'N', 'A', 'Y', 'L', 'Q', 'R', 'L', 'E', 'E'] [[0.6780146 0.3219854]]\n",
      "['K', 'D', 'N', 'L', 'F', 'V', 'R', 'I', 'D', 'R', 'R', 'R', 'K', 'L', 'P', 'A', 'T', 'I', 'I', 'L', 'R', 'A', 'L', 'N', 'Y'] [[0.51606256 0.4839375 ]]\n",
      "['E', 'G', 'R', 'S', 'G', 'I', 'T', 'F', 'S', 'Q', 'E', 'L', 'K', 'D', 'S', 'G', 'M', 'R', 'S', 'H', 'V', 'W', 'G', 'N', 'V'] [[0.5978052  0.40219483]]\n",
      "['S', 'F', 'R', 'L', 'M', 'G', 'F', 'G', 'H', 'R', 'V', 'Y', 'K', 'N', 'Y', 'D', 'P', 'R', 'A', 'T', 'V', 'M', 'R', 'E', 'T'] [[0.5439624 0.4560376]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'A', 'E', 'M', 'K', 'N', 'L', 'K', 'I', 'E', 'V', 'V', 'R', 'Y', 'N', 'P', 'E'] [[0.5639271 0.4360729]]\n",
      "['G', 'M', 'M', 'V', 'N', 'N', 'V', 'P', 'K', 'L', 'A', 'C', 'K', 'T', 'F', 'L', 'R', 'D', 'Y', 'T', 'D', 'G', 'M', 'K', 'V'] [[0.6355127  0.36448732]]\n",
      "['E', 'I', 'T', 'S', 'T', 'D', 'D', 'F', 'Y', 'R', 'L', 'G', 'K', 'E', 'L', 'A', 'L', 'Q', 'S', 'G', 'L', 'A', 'H', 'K', 'G'] [[0.7045625 0.2954375]]\n",
      "['G', 'Q', 'R', 'I', 'Q', 'N', 'L', 'R', 'N', 'V', 'M', 'S', 'K', 'T', 'G', 'K', 'T', 'A', 'A', 'I', 'L', 'L', 'D', 'T', 'K'] [[0.50541013 0.4945899 ]]\n",
      "['I', 'Q', 'E', 'S', 'H', 'V', 'H', 'D', 'V', 'T', 'I', 'T', 'K', 'E', 'S', 'P', 'N', 'Y', 'R', 'L', 'G', 'S', 'O', 'O', 'O'] [[0.5542287  0.44577128]]\n",
      "['A', 'L', 'A', 'Q', 'E', 'G', 'G', 'I', 'G', 'F', 'I', 'H', 'K', 'N', 'M', 'S', 'I', 'E', 'R', 'Q', 'A', 'E', 'E', 'V', 'R'] [[0.7255632 0.2744368]]\n",
      "['T', 'Q', 'E', 'S', 'L', 'Y', 'L', 'A', 'L', 'R', 'M', 'V', 'K', 'P', 'G', 'I', 'N', 'L', 'R', 'E', 'I', 'G', 'A', 'A', 'I'] [[0.503096   0.49690402]]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'M', 'A', 'Y', 'K', 'H', 'I', 'L', 'I', 'A', 'V', 'D', 'L', 'S', 'P', 'E', 'S'] [[0.5474809  0.45251912]]\n",
      "['K', 'G', 'I', 'Y', 'K', 'L', 'E', 'T', 'I', 'E', 'G', 'S', 'K', 'G', 'K', 'V', 'Q', 'L', 'L', 'G', 'S', 'G', 'S', 'I', 'L'] [[0.5224442  0.47755584]]\n",
      "['Q', 'R', 'G', 'Y', 'Q', 'A', 'G', 'I', 'A', 'G', 'R', 'S', 'K', 'E', 'M', 'C', 'P', 'Y', 'Q', 'T', 'L', 'N', 'Q', 'R', 'S'] [[0.6174117 0.3825884]]\n",
      "['D', 'S', 'Y', 'I', 'P', 'E', 'P', 'E', 'R', 'A', 'I', 'D', 'K', 'P', 'F', 'L', 'L', 'P', 'I', 'E', 'D', 'V', 'F', 'S', 'I'] [[0.5755609 0.4244391]]\n",
      "['L', 'S', 'T', 'G', 'G', 'T', 'A', 'R', 'L', 'L', 'A', 'E', 'K', 'G', 'L', 'P', 'V', 'T', 'E', 'V', 'S', 'D', 'Y', 'T', 'G'] [[0.6772466  0.32275346]]\n",
      "['Q', 'R', 'Y', 'W', 'L', 'V', 'D', 'P', 'L', 'D', 'G', 'T', 'K', 'E', 'F', 'I', 'K', 'R', 'N', 'G', 'E', 'F', 'T', 'V', 'N'] [[0.50006086 0.49993917]]\n",
      "['I', 'G', 'T', 'V', 'I', 'D', 'N', 'D', 'N', 'C', 'T', 'S', 'K', 'F', 'S', 'R', 'F', 'F', 'A', 'T', 'R', 'E', 'E', 'A', 'E'] [[0.51201934 0.48798063]]\n",
      "['Q', 'L', 'T', 'E', 'E', 'G', 'Y', 'Y', 'S', 'V', 'F', 'G', 'K', 'S', 'G', 'A', 'R', 'I', 'E', 'I', 'P', 'G', 'C', 'S', 'L'] [[0.66321874 0.33678123]]\n",
      "['I', 'T', 'Q', 'S', 'R', 'L', 'R', 'I', 'D', 'A', 'N', 'F', 'K', 'R', 'F', 'V', 'D', 'E', 'E', 'V', 'L', 'P', 'G', 'T', 'G'] [[0.6801786  0.31982142]]\n",
      "['N', 'F', 'I', 'A', 'T', 'I', 'E', 'E', 'R', 'Q', 'G', 'L', 'K', 'V', 'S', 'C', 'P', 'E', 'E', 'I', 'A', 'F', 'R', 'K', 'G'] [[0.6113256  0.38867438]]\n",
      "['T', 'E', 'G', 'L', 'T', 'A', 'E', 'Q', 'I', 'R', 'R', 'G', 'K', 'T', 'V', 'V', 'V', 'E', 'G', 'C', 'E', 'E', 'K', 'L', 'A'] [[0.5088525  0.49114758]]\n",
      "['K', 'G', 'N', 'Q', 'V', 'R', 'I', 'G', 'V', 'N', 'A', 'P', 'K', 'E', 'V', 'S', 'V', 'H', 'R', 'E', 'E', 'I', 'Y', 'Q', 'R'] [[0.50587285 0.49412712]]\n",
      "['H', 'M', 'Q', 'I', 'I', 'N', 'E', 'I', 'N', 'T', 'R', 'F', 'K', 'T', 'L', 'V', 'E', 'K', 'T', 'W', 'P', 'G', 'D', 'E', 'K'] [[0.49619544 0.50380456]]\n",
      "['T', 'Y', 'F', 'G', 'E', 'L', 'S', 'R', 'M', 'T', 'Q', 'F', 'K', 'D', 'K', 'S', 'A', 'R', 'Y', 'A', 'E', 'N', 'I', 'N', 'A'] [[0.42054075 0.5794592 ]]\n",
      "['W', 'D', 'A', 'A', 'Q', 'S', 'L', 'L', 'A', 'T', 'Y', 'I', 'K', 'L', 'N', 'V', 'A', 'R', 'H', 'Q', 'Q', 'G', 'Q', 'P', 'L'] [[0.3998274  0.60017264]]\n",
      "['D', 'A', 'R', 'E', 'A', 'F', 'L', 'N', 'I', 'T', 'V', 'T', 'K', 'D', 'S', 'R', 'T', 'R', 'Y', 'S', 'E', 'A', 'G', 'H', 'P'] [[0.43921703 0.56078297]]\n",
      "['R', 'N', 'G', 'L', 'R', 'P', 'A', 'R', 'Y', 'V', 'I', 'T', 'K', 'D', 'K', 'L', 'I', 'T', 'C', 'A', 'S', 'E', 'V', 'G', 'I'] [[0.37593877 0.6240613 ]]\n",
      "['A', 'D', 'V', 'I', 'Q', 'I', 'K', 'V', 'A', 'Q', 'G', 'A', 'K', 'P', 'G', 'E', 'G', 'G', 'Q', 'L', 'P', 'G', 'D', 'K', 'V'] [[0.4389108  0.56108916]]\n",
      "['L', 'P', 'E', 'Q', 'V', 'R', 'T', 'N', 'A', 'D', 'L', 'E', 'K', 'M', 'V', 'D', 'T', 'S', 'D', 'E', 'W', 'I', 'V', 'T', 'R'] [[0.3511316 0.6488684]]\n",
      "['L', 'V', 'D', 'D', 'E', 'R', 'W', 'A', 'R', 'F', 'N', 'E', 'K', 'L', 'E', 'N', 'I', 'E', 'R', 'E', 'R', 'Q', 'R', 'L', 'K'] [[0.45958945 0.5404105 ]]\n",
      "['A', 'A', 'G', 'D', 'E', 'V', 'T', 'V', 'V', 'R', 'D', 'E', 'K', 'K', 'A', 'R', 'E', 'V', 'A', 'L', 'Y', 'R', 'Q', 'G', 'K'] [[0.4892304 0.5107696]]\n",
      "['F', 'G', 'R', 'P', 'Q', 'R', 'V', 'A', 'Q', 'E', 'M', 'Q', 'K', 'E', 'I', 'A', 'L', 'I', 'L', 'Q', 'R', 'E', 'I', 'K', 'D'] [[0.44439378 0.5556062 ]]\n",
      "['A', 'A', 'A', 'L', 'G', 'Q', 'I', 'E', 'K', 'Q', 'F', 'G', 'K', 'G', 'S', 'I', 'M', 'R', 'L', 'G', 'E', 'D', 'R', 'S', 'M'] [[0.35494956 0.6450504 ]]\n",
      "['H', 'T', 'T', 'I', 'G', 'K', 'V', 'D', 'F', 'D', 'A', 'D', 'K', 'L', 'K', 'E', 'N', 'L', 'E', 'A', 'L', 'L', 'V', 'A', 'L'] [[0.48312232 0.51687765]]\n",
      "['R', 'A', 'K', 'Y', 'Q', 'R', 'Q', 'L', 'A', 'R', 'A', 'I', 'K', 'R', 'A', 'R', 'Y', 'L', 'S', 'L', 'L', 'P', 'Y', 'T', 'D'] [[0.42241934 0.57758063]]\n",
      "['Y', 'L', 'A', 'V', 'K', 'R', 'R', 'I', 'Q', 'P', 'G', 'D', 'K', 'M', 'A', 'G', 'R', 'H', 'G', 'N', 'K', 'G', 'V', 'I', 'S'] [[0.20278183 0.7972181 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'N', 'G', 'K', 'S', 'A', 'S', 'A', 'K', 'S', 'L', 'F', 'K', 'L', 'Q', 'T', 'L', 'G', 'L', 'T', 'Q', 'G', 'T', 'V', 'V'] [[0.37942448 0.62057555]]\n",
      "['A', 'P', 'S', 'Q', 'E', 'E', 'A', 'V', 'I', 'A', 'F', 'G', 'K', 'F', 'K', 'L', 'N', 'L', 'G', 'T', 'R', 'E', 'M', 'F', 'R'] [[0.41035813 0.5896418 ]]\n",
      "['N', 'N', 'A', 'L', 'H', 'L', 'F', 'W', 'Q', 'D', 'G', 'D', 'K', 'V', 'L', 'P', 'L', 'E', 'R', 'K', 'E', 'L', 'L', 'G', 'Q'] [[0.43200082 0.56799924]]\n",
      "['N', 'D', 'R', 'R', 'C', 'L', 'H', 'L', 'Q', 'L', 'T', 'E', 'K', 'G', 'H', 'E', 'F', 'L', 'R', 'E', 'V', 'L', 'P', 'P', 'Q'] [[0.41514918 0.5848508 ]]\n",
      "['R', 'W', 'V', 'N', 'A', 'L', 'V', 'S', 'E', 'L', 'N', 'D', 'K', 'E', 'Q', 'H', 'G', 'S', 'Q', 'W', 'K', 'F', 'D', 'V', 'H'] [[0.3528062  0.64719385]]\n",
      "['T', 'I', 'A', 'I', 'A', 'I', 'N', 'L', 'F', 'N', 'P', 'Q', 'K', 'I', 'V', 'I', 'A', 'G', 'E', 'I', 'T', 'E', 'A', 'D', 'K'] [[0.44749504 0.55250496]]\n",
      "['D', 'T', 'L', 'H', 'L', 'E', 'G', 'K', 'E', 'L', 'E', 'F', 'K', 'V', 'I', 'K', 'L', 'D', 'Q', 'K', 'R', 'N', 'N', 'V', 'V'] [[0.19784532 0.80215466]]\n",
      "['G', 'G', 'S', 'A', 'E', 'E', 'E', 'A', 'A', 'A', 'Y', 'I', 'K', 'E', 'H', 'V', 'T', 'K', 'P', 'V', 'V', 'G', 'Y', 'I', 'A'] [[0.44738925 0.55261075]]\n",
      "['V', 'T', 'E', 'I', 'A', 'K', 'K', 'L', 'N', 'R', 'S', 'I', 'K', 'T', 'I', 'S', 'S', 'Q', 'K', 'K', 'S', 'A', 'M', 'M', 'K'] [[0.19625975 0.80374026]]\n",
      "['T', 'V', 'A', 'L', 'I', 'A', 'G', 'G', 'H', 'T', 'L', 'G', 'K', 'T', 'H', 'G', 'A', 'G', 'P', 'T', 'S', 'N', 'V', 'G', 'P'] [[0.38406432 0.6159357 ]]\n",
      "['E', 'A', 'I', 'G', 'V', 'L', 'E', 'Q', 'Q', 'S', 'D', 'L', 'K', 'G', 'L', 'L', 'L', 'R', 'S', 'N', 'K', 'A', 'A', 'F', 'I'] [[0.42019072 0.57980925]]\n",
      "['V', 'L', 'F', 'D', 'M', 'A', 'R', 'E', 'V', 'N', 'R', 'L', 'K', 'A', 'E', 'D', 'M', 'A', 'A', 'A', 'N', 'A', 'M', 'A', 'S'] [[0.4385361  0.56146395]]\n",
      "['G', 'G', 'L', 'D', 'S', 'S', 'I', 'I', 'S', 'A', 'I', 'T', 'K', 'K', 'Y', 'A', 'A', 'R', 'R', 'V', 'E', 'D', 'Q', 'E', 'R'] [[0.4216906 0.5783094]]\n",
      "['E', 'I', 'L', 'Q', 'W', 'Q', 'R', 'E', 'R', 'L', 'V', 'A', 'K', 'L', 'E', 'D', 'A', 'Q', 'V', 'Q', 'L', 'E', 'N', 'N', 'R'] [[0.46173295 0.5382671 ]]\n",
      "['T', 'T', 'W', 'R', 'K', 'L', 'D', 'E', 'T', 'T', 'R', 'N', 'K', 'I', 'T', 'D', 'A', 'A', 'S', 'A', 'A', 'A', 'L', 'M', 'T'] [[0.28393736 0.71606266]]\n",
      "['D', 'T', 'A', 'L', 'V', 'H', 'I', 'A', 'A', 'A', 'Y', 'H', 'K', 'P', 'T', 'L', 'A', 'F', 'Y', 'P', 'N', 'S', 'R', 'T', 'P'] [[0.43844277 0.56155723]]\n",
      "['E', 'A', 'M', 'A', 'M', 'A', 'K', 'R', 'V', 'S', 'K', 'L', 'K', 'N', 'A', 'N', 'R', 'F', 'F', 'V', 'A', 'S', 'D', 'V', 'H'] [[0.4469873 0.5530127]]\n",
      "['S', 'H', 'L', 'N', 'T', 'G', 'R', 'P', 'Y', 'N', 'A', 'D', 'K', 'P', 'N', 'K', 'Y', 'T', 'S', 'R', 'Y', 'F', 'D', 'E', 'A'] [[0.3199616 0.6800384]]\n",
      "['L', 'A', 'Q', 'E', 'E', 'V', 'W', 'I', 'R', 'Q', 'G', 'I', 'K', 'A', 'R', 'R', 'T', 'R', 'N', 'E', 'G', 'R', 'V', 'R', 'A'] [[0.43713745 0.5628625 ]]\n",
      "['G', 'Y', 'D', 'P', 'I', 'F', 'F', 'V', 'P', 'S', 'E', 'G', 'K', 'T', 'A', 'A', 'E', 'L', 'T', 'R', 'E', 'E', 'K', 'S', 'A'] [[0.4036415  0.59635854]]\n",
      "['R', 'S', 'A', 'C', 'T', 'Y', 'V', 'G', 'A', 'S', 'R', 'L', 'K', 'E', 'L', 'T', 'K', 'R', 'T', 'T', 'F', 'I', 'R', 'V', 'Q'] [[0.3689149  0.63108504]]\n",
      "['M', 'A', 'G', 'V', 'V', 'I', 'H', 'A', 'A', 'F', 'V', 'Y', 'K', 'L', 'G', 'D', 'W', 'F', 'A', 'R', 'D', 'T', 'R', 'N', 'F'] [[0.4799414 0.5200586]]\n",
      "['P', 'H', 'K', 'S', 'P', 'E', 'V', 'F', 'N', 'L', 'I', 'M', 'K', 'R', 'R', 'A', 'I', 'A', 'G', 'S', 'M', 'I', 'G', 'G', 'I'] [[0.4798579  0.52014214]]\n",
      "['G', 'F', 'G', 'A', 'R', 'H', 'N', 'A', 'S', 'N', 'S', 'L', 'K', 'D', 'I', 'A', 'E', 'L', 'V', 'P', 'F', 'A', 'H', 'R', 'Y'] [[0.4060031  0.59399694]]\n",
      "['N', 'I', 'F', 'Q', 'S', 'D', 'H', 'P', 'V', 'A', 'M', 'M', 'K', 'A', 'V', 'Q', 'A', 'V', 'V', 'H', 'H', 'N', 'E', 'T', 'A'] [[0.30343068 0.6965693 ]]\n",
      "acc: 62.0\n",
      "sn: 61.76470588235294\n",
      "sp: 62.244897959183675\n",
      "mcc: 0.2400480144048017\n"
     ]
    }
   ],
   "source": [
    "PATH = './Path/dlmal_e_net.pth'\n",
    "net = torch.load(PATH, map_location=\"cpu\")\n",
    "net.eval()\n",
    "\n",
    "print(r_test_x[0])\n",
    "print([int_to_char[i] for i in r_test_x[0][0]])\n",
    "y = net(torch.from_numpy(np.array([r_test_x[0]])))\n",
    "print(F.softmax(y, dim=1))\n",
    "\n",
    "print(r_test_x[-1])\n",
    "print([int_to_char[i] for i in r_test_x[-1][0]])\n",
    "y = net(torch.from_numpy(np.array([r_test_x[-1]])))\n",
    "print(F.softmax(y, dim=1))\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for i in range(len(r_test_x)):\n",
    "    y = net(torch.from_numpy(np.array([r_test_x[i]])))\n",
    "    y = F.softmax(y, dim=1)\n",
    "    y = y.detach().cpu().numpy()\n",
    "    if (y[0][0] > y[0][1] and r_test_y[i] == 0):\n",
    "        TN += 1\n",
    "    elif (y[0][0] < y[0][1] and r_test_y[i] == 1):\n",
    "        TP += 1\n",
    "    elif r_test_y[i] == 0:\n",
    "        FN += 1\n",
    "        print([int_to_char[i] for i in r_test_x[i][0]], y)\n",
    "    else:\n",
    "        FP +=1\n",
    "        print([int_to_char[i] for i in r_test_x[i][0]], y)\n",
    "       \n",
    "print(\"acc:\", (TP+TN)/(TP+TN+FP+FN)*100)\n",
    "print(\"sn:\", (TP)/(TP+FN)*100)\n",
    "print(\"sp:\", TN/(TN+FP)*100)\n",
    "print(\"mcc:\", (TP*TN-FP*FN)/((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15f81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
